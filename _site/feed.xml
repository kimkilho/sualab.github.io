<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://github.sualab.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://github.sualab.io/" rel="alternate" type="text/html" /><updated>2020-11-23T14:11:54+09:00</updated><id>http://github.sualab.io/feed.xml</id><title type="html">Cognex Deep Learning Lab-KOR Research Blog</title><subtitle>Cognex Deep Learning Lab-KOR research blog: covers subjects regarding machine learning, computer vision, high-performance computing, and so on.
</subtitle><entry><title type="html">ECCV 2020 Virtual Conference 참석 후기 및 프로그램 소개</title><link href="http://github.sualab.io/review/2020/09/03/ECCV-2020-review.html" rel="alternate" type="text/html" title="ECCV 2020 Virtual Conference 참석 후기 및 프로그램 소개" /><published>2020-09-03T18:00:00+09:00</published><updated>2020-09-03T18:00:00+09:00</updated><id>http://github.sualab.io/review/2020/09/03/ECCV-2020-review</id><content type="html" xml:base="http://github.sualab.io/review/2020/09/03/ECCV-2020-review.html">&lt;p&gt;안녕하세요, 오늘은 지난 8월 23일(일) ~ 8월 28일(금) 6일간 진행된 European Conference on Computer Vision(이하 ECCV) 2020 학회를 Virtual로 참석하며 느낀 점들을 공유 드리고, 주요 프로그램들을 소개 드리겠습니다.&lt;/p&gt;

&lt;p&gt;ECCV는 2년마다 열리며 짝수 년도에 열리는 Computer Vision 학회입니다. 비슷한 학회로 International Conference on Computer Vision(ICCV)가 있죠. ICCV도 2년마다 열리며 홀수 년도에 열립니다. 저는 원래 올해 영국 Glasgow에서 열리는 ECCV를 현장에서 참석하려고 8월달만 기다리고 있었는데 전세계적으로 퍼진 COVID-19로 인해 올해 모든 Machine Learning, Computer Vision 관련 학회들이 Virtual로 진행되었습니다.&lt;/p&gt;

&lt;p&gt;저는 Virtual로 학회에 참석한 적이 처음이라 집에서 듣는 학회가 과연 얼마나 효율적일지 궁금했는데요, 직접 일주일간 집(혹은 회사)에서 학회를 참석한 후기를 들려드리겠습니다.&lt;/p&gt;

&lt;blockquote&gt; ECCV 2020 주요 통계 &lt;/blockquote&gt;
&lt;p&gt;이번 &lt;a href=&quot;https://eccv2020.eu/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; ECCV 2020&lt;/b&gt;&lt;/a&gt;은 올해로 16번째 열렸고 학회가 개최된 첫날 Opening 행사에서 주요 통계치들을 공개하였습니다. 사진 자료는 &lt;a href=&quot;https://twitter.com/CSProfKGD/status/1297892662687797255&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Kosta Derpanis 라는 분의 트위터의 게시물&lt;/b&gt;&lt;/a&gt;에서 인용하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/stat1.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/stat1.jpg&quot; alt=&quot;Submitted and accepted papers&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Submitted and accepted papers&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;우선 총 5150편의 submission이 있었고, 그 중 1360편이 accept 되었으며 accepted rate는 약 26%를 보였습니다. 2014년이 28%, 2016년이 27%, 2018년이 32%였고 올해가 26%로 대체로 비슷한 비율을 유지하고 있습니다. 전체 Accepted paper 중 Oral paper는 7.5%, Spotlight paper는 11.8%의 비율을 보이고 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/stat2.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/stat2.jpg&quot; alt=&quot;Area of submitted papers&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Area of submitted papers&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;다음은 전체 제출된 논문들의 연구 분야를 분류한 자료입니다. Deep Learning의 응용, 방법론, 이론을 다룬 논문이 주를 이뤘고, 아무래도 Computer Vision 학회이다 보니 인식 쪽 논문도 많은 비율을 보이고 있습니다. Unsupervised Learning 논문이 352편이나 되는 점도 인상깊네요.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/stat3.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/stat3.jpg&quot; alt=&quot;Institutions of submitted papers&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Institutions of submitted papers&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;다음은 Accepted paper들의 저자들이 속한 기관이 대한 자료입니다. 구글이 1위를 차지하였고 페이스북이 5위를, 마이크로소프트가 7위를, 화웨이가 11위를 차지하였습니다. 대체로 학계에 계신 분들의 비중이 높았으며 Computer Vision은 역시 중국이 강세임을 보여주고 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/stat4.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/stat4.jpg&quot; alt=&quot;Growth of ECCV&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Growth of ECCV&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 2년전과 비교했을 때 학회에 제출된 논문의 편수가 얼마나 증가했는지를 보여주고 있습니다. 우선 ECCV 2018에 비해 제출된 논문 편수가 2.1배 늘었으며, CVPR 2020과 비교하였을 때 2년전보다 제출된 논문의 수가 비슷해졌음을 보여주고 있습니다.&lt;/p&gt;

&lt;blockquote&gt; ECCV 주요 프로그램 소개 및 Virtual 참석 후기 &lt;/blockquote&gt;

&lt;p&gt;대부분의 학회는 크게 6가지 프로그램으로 분류할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Oral, Spotlight 논문들의 구두 발표가 진행되는 &lt;strong&gt;Main Conference&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;대부분의 논문들을 정해진 시간에 Poster와 함께 발표하고 실시간으로 질의 응답하는 &lt;strong&gt;Poster Session&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;특정한 주제를 정해 놓고 그 주제와 관련된 연구들을 소개하고 경우에 따라 Challenge도 개최하는 &lt;strong&gt;Workshop&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Workshop과 비슷하게 특정한 주제를 정해두지만 교육의 목적이 크고, 종종 실습(Hands-on Training)도 같이 진행하는 &lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;본인들의 연구를 Live Demo로 보여주는 &lt;strong&gt;Demo Session&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;학회에 일정 금액의 돈을 내고 스폰서(등급이 나뉘어져 있음)로 참여하여 기업을 홍보하는 &lt;strong&gt;Exhibition&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 모든 프로그램이 Virtual로 진행이 되었으며, 학회에 등록한 사람들은 별도의 페이지에 접속할 수 있게 됩니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/lobby.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/lobby.PNG&quot; alt=&quot;ECCV2020 Lobby&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Lobby&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;사이트에 접속을 하면 이런 화면에서 시작합니다. Lobby에서 원하는 프로그램을 클릭하여 듣는 방식입니다. 어떻게 보면 더운 여름 발표 들으러 이곳 저곳 걸어 다니지 않고 컴퓨터 앞에서 클릭 몇 번 해주면 된다는 점이 장점이라고 생각합니다. 다만 아쉬운 점도 있었습니다. 이제 각 프로그램마다 Virtual로 참석해보고 느낀 점들을 공유 드리겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;main-conference&quot;&gt;Main Conference&lt;/h3&gt;
&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/oral_session.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/oral_session.PNG&quot; alt=&quot;ECCV2020 Oral Session&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Oral Session&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Main Conference는 월요일부터 금요일까지 5일간 진행이 되었고, Oral Paper와 Spotlight Paper에 선정된 저자들은 정해진 시간에 실시간으로 1번 발표를 하는 방식으로 진행이 되었습니다. 오프라인 학회에서도 Main Conference Room에 자리가 없으면, 빈 Conference Room에서 화면을 띄워준 채로 실시간 송출을 해주는데, 이걸 집에서 보는 느낌이라고 생각하시면 좋을 것 같습니다.&lt;/p&gt;

&lt;p&gt;요일마다 주제가 바뀌었고, 중간 중간 Network Break 타임에는 Industry Session도 진행이 되었습니다. 각 요일마다 어떤 발표들이 진행되었는지 궁금하실 분들을 위해 링크를 첨부 드리겠습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://eccv2020.eu/wp-content/uploads/2020/08/ECCV-Programme-Monday.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 8/24 월요일 &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://eccv2020.eu/wp-content/uploads/2020/08/ECCV-Programme-Tuesday.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 8/25 화요일 &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://eccv2020.eu/wp-content/uploads/2020/08/ECCV-Programme-Wednesday.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 8/26 수요일 &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://eccv2020.eu/wp-content/uploads/2020/08/ECCV-Programme-Thursday.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 8/27 목요일 &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://eccv2020.eu/wp-content/uploads/2020/08/ECCV-Programme-Friday-2.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 8/28 금요일 &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;poster-session&quot;&gt;Poster Session&lt;/h3&gt;
&lt;p&gt;저는 주로 오프라인 학회에 참석할 때, 아무래도 요즘 학회의 규모가 너무 커지다 보니 모든 논문을 다 살펴보기엔 무리가 커서, 주요 논문들을 미리 추려서 간단히 읽어보고 참석을 합니다. 그리고 Poster Session을 빠르게 돌아다니면서 재미있어 보이는 Poster를 찾으면 일단 사진을 찍고, 서있는 저자에게 설명을 해달라고 부탁드리고, 궁금한 점을 질문하면서 돌아다니는 재미가 있었는데요. 개인적으로 Virtual Conference로 바뀌면서 가장 아쉬웠던 프로그램이 바로 Poster Session이었습니다. 
&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/papers.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/papers.PNG&quot; alt=&quot;ECCV2020 Submitted Papers&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Submitted Papers&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;우선 Lobby의 Papers and Presentations를 들어가면 모든 논문들의 제목과 논문(.pdf), 10분 분량의 전체 발표 Video, 1분 분량의 Short Video를 확인할 수 있고, 주요 연구 주제들끼리 모여 있는 점은 좋았습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/poster_session.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/poster_session.PNG&quot; alt=&quot;ECCV2020 Poster Session&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Poster Session&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;다만, 제가 원했던 Poster Session은 오프라인에서 넓은 공간에 포스터가 배치 되어있는 것처럼, 온라인으로도 빠르게 포스터 하나 하나씩 넘기며 볼 수 있는 것을 기대했는데 그렇진 않았습니다. 정해진 시간에 Poster Session에 들어가면 위와 같이 각 논문 마다 별도의 탭이 존재하였고, Launch 버튼을 누르면 zoom으로 연결되는 방식이었습니다. 들어가면 저자와 소통을 할 수 있긴 하지만 zoom 특성 상 접속하는데 딜레이가 있다 보니 빠르게 훑는 것은 불가능했습니다.&lt;/p&gt;

&lt;h3 id=&quot;workshop&quot;&gt;Workshop&lt;/h3&gt;
&lt;p&gt;Workshop과 Tutorial은 보통 같은 날 같은 시간에 동시 다발적으로 진행이 되다 보니 듣고 싶은 세션들을 미리 추려서 스케쥴 표를 만들어서 다녔습니다. Workshop은 하나의 주제가 하루에 2번씩 zoom을 통해 진행이 되었고, 학회의 첫날인 일요일과, 마지막 날인 금요일에 진행되었습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/sunday_workshop.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/sunday_workshop.PNG&quot; alt=&quot;ECCV2020 Workshop&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Workshop&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;저는 이번 ECCV 2020에서 &lt;a href=&quot;https://vipriors.github.io/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Visual Inductive Priors for Data-Efficient Deep Learning” &lt;/b&gt;&lt;/a&gt; 의 Action Recognition 과 Semantic Segmentation Challenge에 참가를 하였고, 그 중 &lt;a href=&quot;http://mvp.yonsei.ac.kr/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 연세대학교 MVP Lab &lt;/b&gt;&lt;/a&gt; 박사과정분들, 저희 Cognex Deep Learning Lab 조동헌 연구원과 함께 참여한 Action Recognition에서 4위의 성적을 거두었습니다. 또한 Challenge에서 시도했던 방법들을 논문으로 제출하여 Oral Paper에 선정이 되었습니다. 좋은 팀원 분들을 만난 덕에 좋은 성과를 얻을 수 있었던 것 같습니다. 다시 한번 감사드립니다! 혹시나 궁금해하실 분들을 위해 저희 연구 성과를 담은 논문 링크를 남깁니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/2008.05721.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Learning Temporally Invariant and Localizable Features via Data Augmentation for Video Recognition &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;저는 위의 “Visual Inductive Priors for Data-Efficient Deep Learning“ Workshop 외에도 &lt;a href=&quot;https://sites.google.com/view/ipcv2020/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Imbalance Problems in Computer Vision (IPCV)” &lt;/b&gt;&lt;/a&gt;, &lt;a href=&quot;http://www.robustvision.net/index.php&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Robust Vision Challenge 2020” &lt;/b&gt;&lt;/a&gt; 도 재미있게 들었습니다.&lt;/p&gt;

&lt;p&gt;전체 Workshop의 리스트와 홈페이지들은 &lt;a href=&quot;https://eccv2020.eu/workshops/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; ECCV 2020 공식 홈페이지&lt;/b&gt;&lt;/a&gt; 에서 확인하실 수 있고, 일부 Workshop의 경우 학회를 등록하지 않은 사람들도 발표 자료와 발표 Video를 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;tutorial&quot;&gt;Tutorial&lt;/h3&gt;
&lt;p&gt;Tutorial도 Workshop과 마찬가지로 하나의 주제가 하루에 2번씩 zoom을 통해 진행이 되었고, 실용적인 내용이 많아서 개인적으론 많은 도움이 되었습니다. 이번 학회에서 가장 만족스러웠던 프로그램을 꼽으라면 저는 바로 Tutorial을 꼽을 수 있을 것 같네요.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/sunday_tutorial.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/sunday_tutorial.PNG&quot; alt=&quot;ECCV2020 Tutorial&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Tutorial&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;제가 들었던 Tutorial 중에 유익하면서도 누구나 발표 자료와 발표 Video를 확인할 수 있는 프로그램들을 소개 드리겠습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Accelerating Computer Vision with Mixed Precision”&lt;/b&gt;&lt;/a&gt; : TensorFlow와 PyTorch의 학습을 가속시키기 위한 Mixed Precision Training 기법을 다룬 Tutorial. 저는 개인적으로 Code Optimization Tricks 발표 “PyTorch Performance Tuning Guide” 가 가장 큰 도움이 되었습니다.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hangzhang.org/ECCV2020/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “From HPO to NAS: Automated Deep Learning”&lt;/b&gt;&lt;/a&gt; : AutoML을 이용한 Hyper Parameter Optimization부터 Neural Architecture Search 까지 최근 급성장한 연구들을 아주 자세히 다루고 있습니다.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hbilen.github.io/wsl-eccv20.github.io/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Weakly-Supervised Learning in Computer Vision”&lt;/b&gt;&lt;/a&gt; : Computer Vision에서 다뤄지는 Weakly Supervised Learning, 사람이 annotation pipeline에 개입하여 효율성을 높이는 Human-in-the-Loop에 대한 내용, Weakly Supervised Learning을 올바르게 평가하기 위한 방법 등을 굉장히 자세히 다루고 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;demo-session&quot;&gt;Demo Session&lt;/h3&gt;
&lt;p&gt;다음으로 데모는 월요일부터 목요일까지 진행되었고, 총 41가지 주제의 데모가 존재합니다. 하나의 주제 당 하루에 2번, 각각 2시간씩 진행이 되었고 lobby의 하단에 Networking Lounge를 눌러서 들어가면 모든 데모들의 설명이 담긴 워드 파일과 5분 내외의 Demo Video를 볼 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/demo.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/demo.PNG&quot; alt=&quot;ECCV2020 Demo&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Demo&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;다만 Poster와 마찬가지로 Demo도 실시간으로 동작하는 것을 보여주는 것이 의미가 있다고 생각하는데 단순히 문서와 Video로만 제공이 되니 아무래도 몰입도가 떨어졌고, 모든 컨텐츠가 Video 형태로 제공되다 보니 Demo만의 매력이 사라진 느낌을 받았습니다.&lt;/p&gt;

&lt;h3 id=&quot;exhibition&quot;&gt;Exhibition&lt;/h3&gt;
&lt;p&gt;마지막은 기업들이 돈을 지불하고, 지불한 금액에 따라 다이아몬드, 플래티넘, 골드, 실버, 스타트업 등급으로 나뉘어서 본인들의 부스를 운영하는 Exhibition입니다. 각 등급별 가격과 혜택은 &lt;a href=&quot;https://eccv2020.eu/wp-content/uploads/2020/05/ECCV20-Partnership-Package-Overview-dig.jpg&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; ECCV 2020 Partnership Package Overview 자료 &lt;/b&gt;&lt;/a&gt;에서 확인하실 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/exhibition.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/exhibition.PNG&quot; alt=&quot;ECCV2020 Exhibition&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Exhibition&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Lobby에서 Exhibition에 들어가면 다음과 같이 등급에 따라 부스가 나뉘어져 있으며, 각 부스를 클릭하면 본인들의 연구와 제품들을 Video 형태로 보여주고, 채용 관련 탭도 별도로 운영하고 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/facebook_booth.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/facebook_booth.PNG&quot; alt=&quot;ECCV2020 Facebook Booth&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Facebook Booth&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 다이아몬트 파트너 Facebook의 부스이며 저는 &lt;a href=&quot;https://cdn-akamai.6connex.eu//53/75//PyTorch_Video_Resources_15980345972578817.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; PyTorch의 주요 기능들을 다룬 자료&lt;/b&gt;&lt;/a&gt; 가 큰 도움이 되었습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/apple_booth.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/ECCV-2020-review/apple_booth.PNG&quot; alt=&quot;ECCV2020 Apple Booth&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ECCV2020 Apple Booth&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;다음은 골드 파트너 Apple의 부스이며 굉장히 심플하게 부스를 꾸몄으며, 주로 연구 내용 소개와 채용 관련 내용이 주를 이루고 있었습니다.&lt;/p&gt;

&lt;p&gt;오프라인에서는 스폰서의 등급에 따라 부스의 크기, 면적 등이 달랐던 것 같은데 온라인에서는 등급에 무관하게 동일한 형태로 운영이 되는 것 같았습니다. 그리고 무엇보다, 오프라인에서 부스를 돌아다니면 각종 스티커와 기념품, 간식을 얻을 수 있었는데 온라인은 그렇지 못한 점이 아쉬웠습니다. 껄껄껄&lt;/p&gt;

&lt;blockquote&gt; 결론 &lt;/blockquote&gt;
&lt;p&gt;이번 포스팅에서는 COVID-19로 인해 Virtual Conference로 진행된 ECCV 2020의 주요 프로그램들을 소개 드리고, 각 프로그램마다 느낀 점들을 공유 드렸습니다. 가격이 낮아졌다는 점과 집에서 들을 수 있다는 점이 장점이 될 수도 있지만, 저에게는 단점이 더 크게 느껴졌던 것 같습니다. 아무래도 Poster Session과 Demo는 현장에서 생생하게 체험을 하고, 서로 마주보며 열띤 토론을 해야 재밌는데 모니터 화면 너머로 소통을 하다 보니 몰입도가 떨어졌습니다. 하지만 Workshop, Tutorials은 좋은 내용이 많이 다뤄졌고, 자료도 대부분 공개가 되어있어서 학회에 참석하지 않아도 양질의 자료들을 접할 수 있다는 점은 참 좋은 것 같습니다. 전세계적으로 안정을 찾아서 다시 오프라인 학회가 개최될 그 날을 기다리며 오늘의 글 마치겠습니다.&lt;/p&gt;

&lt;blockquote&gt; Reference &lt;/blockquote&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/CSProfKGD/status/1297892662687797255&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; ECCV 2020 통계치 자료 &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lixin4ever/Conference-Acceptance-Rate&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 주요 AI 학회 acceptance rate 자료 &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>이호성</name><email>Hoseong.Lee@cognex.com</email></author><category term="Review" /><category term="ECCV2020" /><category term="conference" /><summary type="html">안녕하세요, 오늘은 지난 8월 23일(일) ~ 8월 28일(금) 6일간 진행된 European Conference on Computer Vision(이하 ECCV) 2020 학회를 Virtual로 참석하며 느낀 점들을 공유 드리고, 주요 프로그램들을 소개 드리겠습니다. ECCV는 2년마다 열리며 짝수 년도에 열리는 Computer Vision 학회입니다. 비슷한 학회로 International Conference on Computer Vision(ICCV)가 있죠. ICCV도 2년마다 열리며 홀수 년도에 열립니다. 저는 원래 올해 영국 Glasgow에서 열리는 ECCV를 현장에서 참석하려고 8월달만 기다리고 있었는데 전세계적으로 퍼진 COVID-19로 인해 올해 모든 Machine Learning, Computer Vision 관련 학회들이 Virtual로 진행되었습니다. 저는 Virtual로 학회에 참석한 적이 처음이라 집에서 듣는 학회가 과연 얼마나 효율적일지 궁금했는데요, 직접 일주일간 집(혹은 회사)에서 학회를 참석한 후기를 들려드리겠습니다. ECCV 2020 주요 통계 이번 ECCV 2020은 올해로 16번째 열렸고 학회가 개최된 첫날 Opening 행사에서 주요 통계치들을 공개하였습니다. 사진 자료는 Kosta Derpanis 라는 분의 트위터의 게시물에서 인용하였습니다. Submitted and accepted papers 우선 총 5150편의 submission이 있었고, 그 중 1360편이 accept 되었으며 accepted rate는 약 26%를 보였습니다. 2014년이 28%, 2016년이 27%, 2018년이 32%였고 올해가 26%로 대체로 비슷한 비율을 유지하고 있습니다. 전체 Accepted paper 중 Oral paper는 7.5%, Spotlight paper는 11.8%의 비율을 보이고 있습니다. Area of submitted papers 다음은 전체 제출된 논문들의 연구 분야를 분류한 자료입니다. Deep Learning의 응용, 방법론, 이론을 다룬 논문이 주를 이뤘고, 아무래도 Computer Vision 학회이다 보니 인식 쪽 논문도 많은 비율을 보이고 있습니다. Unsupervised Learning 논문이 352편이나 되는 점도 인상깊네요. Institutions of submitted papers 다음은 Accepted paper들의 저자들이 속한 기관이 대한 자료입니다. 구글이 1위를 차지하였고 페이스북이 5위를, 마이크로소프트가 7위를, 화웨이가 11위를 차지하였습니다. 대체로 학계에 계신 분들의 비중이 높았으며 Computer Vision은 역시 중국이 강세임을 보여주고 있습니다. Growth of ECCV 마지막으로 2년전과 비교했을 때 학회에 제출된 논문의 편수가 얼마나 증가했는지를 보여주고 있습니다. 우선 ECCV 2018에 비해 제출된 논문 편수가 2.1배 늘었으며, CVPR 2020과 비교하였을 때 2년전보다 제출된 논문의 수가 비슷해졌음을 보여주고 있습니다. ECCV 주요 프로그램 소개 및 Virtual 참석 후기 대부분의 학회는 크게 6가지 프로그램으로 분류할 수 있습니다. Oral, Spotlight 논문들의 구두 발표가 진행되는 Main Conference 대부분의 논문들을 정해진 시간에 Poster와 함께 발표하고 실시간으로 질의 응답하는 Poster Session 특정한 주제를 정해 놓고 그 주제와 관련된 연구들을 소개하고 경우에 따라 Challenge도 개최하는 Workshop Workshop과 비슷하게 특정한 주제를 정해두지만 교육의 목적이 크고, 종종 실습(Hands-on Training)도 같이 진행하는 Tutorial 본인들의 연구를 Live Demo로 보여주는 Demo Session 학회에 일정 금액의 돈을 내고 스폰서(등급이 나뉘어져 있음)로 참여하여 기업을 홍보하는 Exhibition 이 모든 프로그램이 Virtual로 진행이 되었으며, 학회에 등록한 사람들은 별도의 페이지에 접속할 수 있게 됩니다. ECCV2020 Lobby 사이트에 접속을 하면 이런 화면에서 시작합니다. Lobby에서 원하는 프로그램을 클릭하여 듣는 방식입니다. 어떻게 보면 더운 여름 발표 들으러 이곳 저곳 걸어 다니지 않고 컴퓨터 앞에서 클릭 몇 번 해주면 된다는 점이 장점이라고 생각합니다. 다만 아쉬운 점도 있었습니다. 이제 각 프로그램마다 Virtual로 참석해보고 느낀 점들을 공유 드리겠습니다. Main Conference ECCV2020 Oral Session Main Conference는 월요일부터 금요일까지 5일간 진행이 되었고, Oral Paper와 Spotlight Paper에 선정된 저자들은 정해진 시간에 실시간으로 1번 발표를 하는 방식으로 진행이 되었습니다. 오프라인 학회에서도 Main Conference Room에 자리가 없으면, 빈 Conference Room에서 화면을 띄워준 채로 실시간 송출을 해주는데, 이걸 집에서 보는 느낌이라고 생각하시면 좋을 것 같습니다. 요일마다 주제가 바뀌었고, 중간 중간 Network Break 타임에는 Industry Session도 진행이 되었습니다. 각 요일마다 어떤 발표들이 진행되었는지 궁금하실 분들을 위해 링크를 첨부 드리겠습니다. 8/24 월요일 8/25 화요일 8/26 수요일 8/27 목요일 8/28 금요일 Poster Session 저는 주로 오프라인 학회에 참석할 때, 아무래도 요즘 학회의 규모가 너무 커지다 보니 모든 논문을 다 살펴보기엔 무리가 커서, 주요 논문들을 미리 추려서 간단히 읽어보고 참석을 합니다. 그리고 Poster Session을 빠르게 돌아다니면서 재미있어 보이는 Poster를 찾으면 일단 사진을 찍고, 서있는 저자에게 설명을 해달라고 부탁드리고, 궁금한 점을 질문하면서 돌아다니는 재미가 있었는데요. 개인적으로 Virtual Conference로 바뀌면서 가장 아쉬웠던 프로그램이 바로 Poster Session이었습니다. ECCV2020 Submitted Papers 우선 Lobby의 Papers and Presentations를 들어가면 모든 논문들의 제목과 논문(.pdf), 10분 분량의 전체 발표 Video, 1분 분량의 Short Video를 확인할 수 있고, 주요 연구 주제들끼리 모여 있는 점은 좋았습니다. ECCV2020 Poster Session 다만, 제가 원했던 Poster Session은 오프라인에서 넓은 공간에 포스터가 배치 되어있는 것처럼, 온라인으로도 빠르게 포스터 하나 하나씩 넘기며 볼 수 있는 것을 기대했는데 그렇진 않았습니다. 정해진 시간에 Poster Session에 들어가면 위와 같이 각 논문 마다 별도의 탭이 존재하였고, Launch 버튼을 누르면 zoom으로 연결되는 방식이었습니다. 들어가면 저자와 소통을 할 수 있긴 하지만 zoom 특성 상 접속하는데 딜레이가 있다 보니 빠르게 훑는 것은 불가능했습니다. Workshop Workshop과 Tutorial은 보통 같은 날 같은 시간에 동시 다발적으로 진행이 되다 보니 듣고 싶은 세션들을 미리 추려서 스케쥴 표를 만들어서 다녔습니다. Workshop은 하나의 주제가 하루에 2번씩 zoom을 통해 진행이 되었고, 학회의 첫날인 일요일과, 마지막 날인 금요일에 진행되었습니다. ECCV2020 Workshop 저는 이번 ECCV 2020에서 “Visual Inductive Priors for Data-Efficient Deep Learning” 의 Action Recognition 과 Semantic Segmentation Challenge에 참가를 하였고, 그 중 연세대학교 MVP Lab 박사과정분들, 저희 Cognex Deep Learning Lab 조동헌 연구원과 함께 참여한 Action Recognition에서 4위의 성적을 거두었습니다. 또한 Challenge에서 시도했던 방법들을 논문으로 제출하여 Oral Paper에 선정이 되었습니다. 좋은 팀원 분들을 만난 덕에 좋은 성과를 얻을 수 있었던 것 같습니다. 다시 한번 감사드립니다! 혹시나 궁금해하실 분들을 위해 저희 연구 성과를 담은 논문 링크를 남깁니다. Paper: Learning Temporally Invariant and Localizable Features via Data Augmentation for Video Recognition 저는 위의 “Visual Inductive Priors for Data-Efficient Deep Learning“ Workshop 외에도 “Imbalance Problems in Computer Vision (IPCV)” , “Robust Vision Challenge 2020” 도 재미있게 들었습니다. 전체 Workshop의 리스트와 홈페이지들은 ECCV 2020 공식 홈페이지 에서 확인하실 수 있고, 일부 Workshop의 경우 학회를 등록하지 않은 사람들도 발표 자료와 발표 Video를 볼 수 있습니다. Tutorial Tutorial도 Workshop과 마찬가지로 하나의 주제가 하루에 2번씩 zoom을 통해 진행이 되었고, 실용적인 내용이 많아서 개인적으론 많은 도움이 되었습니다. 이번 학회에서 가장 만족스러웠던 프로그램을 꼽으라면 저는 바로 Tutorial을 꼽을 수 있을 것 같네요. ECCV2020 Tutorial 제가 들었던 Tutorial 중에 유익하면서도 누구나 발표 자료와 발표 Video를 확인할 수 있는 프로그램들을 소개 드리겠습니다. “Accelerating Computer Vision with Mixed Precision” : TensorFlow와 PyTorch의 학습을 가속시키기 위한 Mixed Precision Training 기법을 다룬 Tutorial. 저는 개인적으로 Code Optimization Tricks 발표 “PyTorch Performance Tuning Guide” 가 가장 큰 도움이 되었습니다. “From HPO to NAS: Automated Deep Learning” : AutoML을 이용한 Hyper Parameter Optimization부터 Neural Architecture Search 까지 최근 급성장한 연구들을 아주 자세히 다루고 있습니다. “Weakly-Supervised Learning in Computer Vision” : Computer Vision에서 다뤄지는 Weakly Supervised Learning, 사람이 annotation pipeline에 개입하여 효율성을 높이는 Human-in-the-Loop에 대한 내용, Weakly Supervised Learning을 올바르게 평가하기 위한 방법 등을 굉장히 자세히 다루고 있습니다. Demo Session 다음으로 데모는 월요일부터 목요일까지 진행되었고, 총 41가지 주제의 데모가 존재합니다. 하나의 주제 당 하루에 2번, 각각 2시간씩 진행이 되었고 lobby의 하단에 Networking Lounge를 눌러서 들어가면 모든 데모들의 설명이 담긴 워드 파일과 5분 내외의 Demo Video를 볼 수 있습니다. ECCV2020 Demo 다만 Poster와 마찬가지로 Demo도 실시간으로 동작하는 것을 보여주는 것이 의미가 있다고 생각하는데 단순히 문서와 Video로만 제공이 되니 아무래도 몰입도가 떨어졌고, 모든 컨텐츠가 Video 형태로 제공되다 보니 Demo만의 매력이 사라진 느낌을 받았습니다. Exhibition 마지막은 기업들이 돈을 지불하고, 지불한 금액에 따라 다이아몬드, 플래티넘, 골드, 실버, 스타트업 등급으로 나뉘어서 본인들의 부스를 운영하는 Exhibition입니다. 각 등급별 가격과 혜택은 ECCV 2020 Partnership Package Overview 자료 에서 확인하실 수 있습니다. ECCV2020 Exhibition Lobby에서 Exhibition에 들어가면 다음과 같이 등급에 따라 부스가 나뉘어져 있으며, 각 부스를 클릭하면 본인들의 연구와 제품들을 Video 형태로 보여주고, 채용 관련 탭도 별도로 운영하고 있습니다. ECCV2020 Facebook Booth 위의 그림은 다이아몬트 파트너 Facebook의 부스이며 저는 PyTorch의 주요 기능들을 다룬 자료 가 큰 도움이 되었습니다. ECCV2020 Apple Booth 다음은 골드 파트너 Apple의 부스이며 굉장히 심플하게 부스를 꾸몄으며, 주로 연구 내용 소개와 채용 관련 내용이 주를 이루고 있었습니다. 오프라인에서는 스폰서의 등급에 따라 부스의 크기, 면적 등이 달랐던 것 같은데 온라인에서는 등급에 무관하게 동일한 형태로 운영이 되는 것 같았습니다. 그리고 무엇보다, 오프라인에서 부스를 돌아다니면 각종 스티커와 기념품, 간식을 얻을 수 있었는데 온라인은 그렇지 못한 점이 아쉬웠습니다. 껄껄껄 결론 이번 포스팅에서는 COVID-19로 인해 Virtual Conference로 진행된 ECCV 2020의 주요 프로그램들을 소개 드리고, 각 프로그램마다 느낀 점들을 공유 드렸습니다. 가격이 낮아졌다는 점과 집에서 들을 수 있다는 점이 장점이 될 수도 있지만, 저에게는 단점이 더 크게 느껴졌던 것 같습니다. 아무래도 Poster Session과 Demo는 현장에서 생생하게 체험을 하고, 서로 마주보며 열띤 토론을 해야 재밌는데 모니터 화면 너머로 소통을 하다 보니 몰입도가 떨어졌습니다. 하지만 Workshop, Tutorials은 좋은 내용이 많이 다뤄졌고, 자료도 대부분 공개가 되어있어서 학회에 참석하지 않아도 양질의 자료들을 접할 수 있다는 점은 참 좋은 것 같습니다. 전세계적으로 안정을 찾아서 다시 오프라인 학회가 개최될 그 날을 기다리며 오늘의 글 마치겠습니다. Reference ECCV 2020 통계치 자료 주요 AI 학회 acceptance rate 자료</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/eccv2020_thumbnail.png" /><media:content medium="image" url="http://github.sualab.io/eccv2020_thumbnail.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">안전한 C++ 코드 만들기</title><link href="http://github.sualab.io/development/2020/08/06/secure-coding.html" rel="alternate" type="text/html" title="안전한 C++ 코드 만들기" /><published>2020-08-06T20:00:00+09:00</published><updated>2020-08-06T20:00:00+09:00</updated><id>http://github.sualab.io/development/2020/08/06/secure-coding</id><content type="html" xml:base="http://github.sualab.io/development/2020/08/06/secure-coding.html">&lt;p&gt;C++만큼 개발자에게 다양한 선택지를 주는 언어는 거의 없습니다. C++의 언어적 특징은 제품의 형태에 최적화된 형태로 코드를 만드는 데 많은 도움을 주지만, 개발자가 신경 써야 할 영역이 많기 때문에 개발자의 역량에 따라 제품의 완성도 편차가 심하며 안전하지 않은 코드를 작성하기 쉬운 환경에 노출된다는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;안전한 코딩은 단순히 잘 동작한다는 것 이상을 뜻합니다. 가독성이 좋아 유지보수가 쉽고, 잘 사용하기는 쉽지만 잘못 쓰기는 어려운 코드를 만드는 것이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;안전한 코딩(Secure coding)&lt;/code&gt;의 본질이라 생각합니다.&lt;/p&gt;

&lt;p&gt;그럼 코그넥스 개발팀에서 사용 중인 C++ 문법과 디자인 패턴에는 어떤 것들이 있고, 이러한 것들이 어떻게 안전한 코드를 만드는지 살펴보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;스마트-포인터-stdshared_ptr과-stdunique_ptr&quot;&gt;스마트 포인터 (std::shared_ptr과 std::unique_ptr)&lt;/h2&gt;

&lt;p&gt;저희 제품에서는 특정 핵심 로직을 제외한 모든 곳에서 new 할당자 대신 스마트 포인터를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::shared_ptr&amp;lt;T&amp;gt;&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::unique_ptr&amp;lt;T&amp;gt;&lt;/code&gt;는 객체가 해제되는 시점에서 자동으로 할당된 객체를 파괴하는 기능을 제공하며 객체 복사가 가능한 std::shared_ptr와 복사가 불가능하고 이동만 가능한 std::unique_ptr 모두 사용하고 있습니다.&lt;/p&gt;

&lt;p&gt;두 객체 모두 함수 안에서 임시로 사용되는 객체나 변수가 아닌, 여러 메서드나 클래스에서 오랫동안 유지해야 하는 객체나 변수에 사용하는 것이 좋습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;객체를 반드시 복사해야 할 필요가 없다면 항상 unique_ptr을 선호하는 것이 안전합니다. 그 이유는 shared_ptr 객체는 복사된 객체를 가지고 있는 모든 곳에서 해제하기 전까지 객체 해제가 되지 않아 메모리 누수(memory leak)을 만들기 상대적으로 더 쉽기 때문입니다. unique_ptr은 객체 복사가 불가능하기 때문에 안전하게 필요한 곳에서만 사용하고 필요한 시기에 해제하기가 쉽습니다.&lt;/p&gt;

  &lt;p&gt;반대로 깊은 복사(deep-copy)가 잦을 때는 포인터 자체를 사용하지 않는 것도 좋은 방법입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;스마트 포인터를 사용할 때는 반드시 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_shared / make_unique&lt;/code&gt; 템플릿을 이용하는 것이 좋은데, 그 이유는 크게 두 가지가 있으며 &lt;a href=&quot;https://herbsutter.com/2013/05/29/gotw-89-solution-smart-pointers/&quot;&gt;허브 서터의 이야기&lt;/a&gt;를 인용하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;먼저 new 할당자를 사용할 때 어떤 일이 벌어지는지 살펴봅시다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shared_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;widget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;widget&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;[그림 1] sp1과 sp2가 할당되는 과정&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/secure-coding/p1.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/secure-coding/p1.png&quot; alt=&quot;sp1과 sp2가 할당되는 과정&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;sp1과 sp2가 할당되는 과정&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;처음 Widget이 new 할당자를 통해 생성되면, shared_ptr 객체가 한번더 생성되면서 Widget을 가리킵니다. 총 2번의 객체 할당이 발생하게 됩니다. 그렇다면 make_shared는 어떨까요?&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_shared&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;widget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;[그림 2] sp1과 sp2가 할당되는 과정&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/secure-coding/p2.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/secure-coding/p2.png&quot; alt=&quot;sp1과 sp2가 할당되는 과정&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;sp1과 sp2가 할당되는 과정&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;make_shared 템플릿을 사용하면, Widget을 클래스 인자로 받는 shared_ptr 생성 1회로 끝납니다. new 할당자와 다르게 총 1번의 객체 할당만 발생하게 됩니다.&lt;/p&gt;

&lt;p&gt;그러나 make_shared를 사용해야만 하는 더 큰 이유는, new 할당자를 사용할 때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;메모리 누수(memory leak)&lt;/code&gt;가 발생할 수 있기 때문입니다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;widget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gadget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;widget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;widget&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}},&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;unique_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gadget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gadget&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 코드에서 new gadget{} 안에서 예외가 발생할 경우 new widget{}은 해제되지 않아 메모리 누수가 발생할 수 있습니다. 또한 함수를 호출할 때 인자의 평가(Evaluation)는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;명시되지 않은 행동(Unspecified Behavior)&lt;/code&gt;으로 컴파일러마다 정의되는 행동이 다르기 때문에, new widget{} 안에서 예외가 발생할 때 new gadget{}으로 생성된 객체가 해제되지 않는 반대의 상황 또한 발생할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 이유로 항상 make_shared 또는 make_unique를 사용하는 것이 안전합니다.&lt;/p&gt;

&lt;h2 id=&quot;static_cast-dynamic_cast-그리고-reinterpret_cast&quot;&gt;static_cast&lt;T&gt;, dynamic_cast&lt;T&gt;, 그리고 reinterpret_cast&lt;T&gt;&lt;/T&gt;&lt;/T&gt;&lt;/T&gt;&lt;/h2&gt;

&lt;p&gt;캐스팅 연산은 보통 3가지(const_cast 제외) 중 1개를 선택하게 되는 데, static_cast와 dynamic_cast, 그리고 reinterpret_cast 캐스팅이 있습니다.&lt;/p&gt;

&lt;p&gt;static_cast 연산은 컴파일 시점에서 형 변환을 하며 대부분의 int, double과 같은 원시 타입(primitive type) 값을 변환할 때 사용하거나, 안전하다고 확신할 수 있는 때에(null이 아닌 경우)만 부모 클래스로 형 변환 등을 할 때 사용합니다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dynamic_cast 연산은 컴파일 시점이 아닌 런타임 시점에서 형 변환을 합니다. 형변환이 실패한 경우 null을 반환하기 때문에 안전하다고 확신할 수 없는 모든 객체의 형 변환에 사용합니다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Class&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Class&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;dynamic_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// a는 null이 아닙니다. 형변환이 실패하면 null 여부를 검사해 확인할 수 있습니다.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;마지막으로 reinterpret_cast 연산은 가장 위험한 연산으로, C 스타일의 캐스팅과 동일한 형 변환을 수행합니다. 포인터를 정수로 저장하거나 정수를 포인터 주소로 사용하는 경우, 또는 void * 포인터 값을 임의의 클래스 포인터 주소로 사용하는 경우를 예로 들 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;reinterpret_cast 연산은 형 변환에 사용되는 두 객체 값 제어가 가능한 경우가 아니면 사용하지 않는 게 좋습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;assert-와-static_assert-그리고-enable_if&quot;&gt;assert 와 static_assert, 그리고 enable_if&lt;/h2&gt;

&lt;p&gt;다음으로 볼 내용은 assert와 static_assert 입니다. 그리고 가능한 경우 함께 사용할 수 있는 std::enable_if도 소개하려 합니다.&lt;/p&gt;

&lt;p&gt;먼저 assert(eval)는 디버그 모드에서만 동작하는 메서드로 eval 식이 참이 아닌 경우 즉시 프로그램을 강제 종료하는 기능을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;assert는 로직이 런타임에서 의도한대로 동작하는 지 확인하고 싶을 때 사용하면 좋습니다. 검사 식 자체가 로직의 방향을 이해하는 데 큰 도움이 되기 때문에 복잡한 코드의 가독성을 크게 끌어올리는 장점을 가지고 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// 값을 찾았을 때는 level 값이 반드시 100 이상이고, 그렇지 않으면 100 미만이어야 함을 보장하는 assert&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// 이 식을 가정하에 코드가 작성됐다는 걸 알 수 있기 때문에 더 빠르게 코드를 이해할 수 있습니다.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;found&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;level&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;found&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;level&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;static_assert는 런타임이 아닌 컴파일 시점에서 의도한대로 동작하는 지 확인할 때 사용하며, 전처리 매크로가 의도한 대로 설정됐는지, 템플릿 인자에 의도한 타입이 들어왔는지 확인할 때 유용하게 사용할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// 템플릿 인자 T의 타입이 bool이 아닌 경우 컴파일 에러가 발생합니다.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;static_assert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_same&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;decltype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;T must be bool&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그러나 템플릿 인자 타입을 검사할 때는 std::enable_if를 사용하는 게 조금 더 좋습니다. 그 이유는 static_assert의 경우 생성된 코드 안에서 에러가 출력되고 에러가 발생한 위치를 알려주지 않는 반면, std::enable_if는 해당하는 템플릿 함수가 없다는 에러가 출력되고 템플릿 함수를 호출한 쪽에서 에러가 발생하기 때문에 원인을 쉽게 찾을 수 있기 때문입니다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// double 또는 int를 인자로 사용하는 CheckValue() 템플릿 함수만 사용 가능합니다.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enable_if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_same&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_same&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;nullptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;CheckValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;const-reference-키워드&quot;&gt;const reference 키워드&lt;/h2&gt;

&lt;p&gt;const reference 는 const &amp;amp; 를 뜻하며 변경이 불가능하고 참조만 가능한 변수나 인자를 뜻합니다. 이 키워드는 함수의 인자, 스택 로컬 변수 등을 선언할 때 사용하는 게 좋습니다.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// 사용 예 1&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// 사용 예 2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GetStringValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;const reference는 일반적인 reference (&amp;amp;)와는 조금 다른 특징을 가지고 있습니다. 첫 번째는 const 키워드로 인해 변경이 불가능하다는 것이고, 두 번째는 reference임에도 객체의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;생명 주기(lifetime)&lt;/code&gt;가 더 길다는 것입니다.&lt;/p&gt;

&lt;p&gt;위 예제의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;사용 예 2&lt;/code&gt;를 보면 마치 GetStringValue() 가 반환하는 std::string 객체를 복사하지 않고 참조만 하는 것처럼 보이지만, 실제로는 const reference에 의해 생명 주기가 연장되어 name 변수가 사라질 때까지 유지됩니다(복사되는 것이 아니며 RVO와도 아무런 관계가 없다는 점에 주의하세요).&lt;/p&gt;

&lt;p&gt;const reference는 불필요한 복사를 막아주고 실수로 변수를 수정하는 일을 쉽게 제한할 수 있다는 점에서 안전한 코드를 만드는 데 큰 도움을 줄 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;요약&quot;&gt;요약&lt;/h2&gt;

&lt;p&gt;이번 글을 통해 소개드린 내용 말고도 많은 장치들을 활용하여 안전하게 코드를 만들 수 있습니다. 특히 템플릿이나 람다 식은 디버깅을 어렵게 만들 수 있기 때문에 가능한 적게 사용한다거나, constexpr if 등을 이용해 중복 코드를 최대한 제거하여 실수할 여지를 줄이는 것 등이 있을 것입니다. 때로는 스크립트로 생성되는 인터페이스로만 코드 사용이 가능하게 하는 것도 좋은 방법입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;물론 때로는 최적화를 위해 이해하기 어려운 코드를 만들어야 할 때도 있습니다. 이 때는 핵심적인 부분은 최대한 숨기고 주석으로 대체하되, 성능에 별 지장이 없는 논리적인 부분을 잘못 사용하기 어렵게 만드는 것이 매우 중요합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;핵심은 최대한 일관성 있는 코드를 만들고, 실수하기 어려운 환경을 만드는 것입니다. 저는 그것이 안전한 코드를 만드는 데 필요한 가장 중요한 원칙이라 생각합니다.&lt;/p&gt;</content><author><name>이기곤</name><email>Gigone.Lee@cognex.com</email></author><category term="Development" /><category term="c++" /><category term="modern c++" /><category term="convention" /><summary type="html">C++만큼 개발자에게 다양한 선택지를 주는 언어는 거의 없습니다. C++의 언어적 특징은 제품의 형태에 최적화된 형태로 코드를 만드는 데 많은 도움을 주지만, 개발자가 신경 써야 할 영역이 많기 때문에 개발자의 역량에 따라 제품의 완성도 편차가 심하며 안전하지 않은 코드를 작성하기 쉬운 환경에 노출된다는 단점이 있습니다. 안전한 코딩은 단순히 잘 동작한다는 것 이상을 뜻합니다. 가독성이 좋아 유지보수가 쉽고, 잘 사용하기는 쉽지만 잘못 쓰기는 어려운 코드를 만드는 것이 안전한 코딩(Secure coding)의 본질이라 생각합니다. 그럼 코그넥스 개발팀에서 사용 중인 C++ 문법과 디자인 패턴에는 어떤 것들이 있고, 이러한 것들이 어떻게 안전한 코드를 만드는지 살펴보겠습니다. 스마트 포인터 (std::shared_ptr과 std::unique_ptr) 저희 제품에서는 특정 핵심 로직을 제외한 모든 곳에서 new 할당자 대신 스마트 포인터를 사용합니다. std::shared_ptr&amp;lt;T&amp;gt;와 std::unique_ptr&amp;lt;T&amp;gt;는 객체가 해제되는 시점에서 자동으로 할당된 객체를 파괴하는 기능을 제공하며 객체 복사가 가능한 std::shared_ptr와 복사가 불가능하고 이동만 가능한 std::unique_ptr 모두 사용하고 있습니다. 두 객체 모두 함수 안에서 임시로 사용되는 객체나 변수가 아닌, 여러 메서드나 클래스에서 오랫동안 유지해야 하는 객체나 변수에 사용하는 것이 좋습니다. 객체를 반드시 복사해야 할 필요가 없다면 항상 unique_ptr을 선호하는 것이 안전합니다. 그 이유는 shared_ptr 객체는 복사된 객체를 가지고 있는 모든 곳에서 해제하기 전까지 객체 해제가 되지 않아 메모리 누수(memory leak)을 만들기 상대적으로 더 쉽기 때문입니다. unique_ptr은 객체 복사가 불가능하기 때문에 안전하게 필요한 곳에서만 사용하고 필요한 시기에 해제하기가 쉽습니다. 반대로 깊은 복사(deep-copy)가 잦을 때는 포인터 자체를 사용하지 않는 것도 좋은 방법입니다. 스마트 포인터를 사용할 때는 반드시 make_shared / make_unique 템플릿을 이용하는 것이 좋은데, 그 이유는 크게 두 가지가 있으며 허브 서터의 이야기를 인용하면 다음과 같습니다. 먼저 new 할당자를 사용할 때 어떤 일이 벌어지는지 살펴봅시다. auto sp1 = shared_ptr&amp;lt;widget&amp;gt;{ new widget{} }; auto sp2 = sp1; [그림 1] sp1과 sp2가 할당되는 과정 sp1과 sp2가 할당되는 과정 처음 Widget이 new 할당자를 통해 생성되면, shared_ptr 객체가 한번더 생성되면서 Widget을 가리킵니다. 총 2번의 객체 할당이 발생하게 됩니다. 그렇다면 make_shared는 어떨까요? auto sp1 = make_shared&amp;lt;widget&amp;gt;(); auto sp2 = sp1; [그림 2] sp1과 sp2가 할당되는 과정 sp1과 sp2가 할당되는 과정 make_shared 템플릿을 사용하면, Widget을 클래스 인자로 받는 shared_ptr 생성 1회로 끝납니다. new 할당자와 다르게 총 1번의 객체 할당만 발생하게 됩니다. 그러나 make_shared를 사용해야만 하는 더 큰 이유는, new 할당자를 사용할 때 메모리 누수(memory leak)가 발생할 수 있기 때문입니다. void sink( unique_ptr&amp;lt;widget&amp;gt;, unique_ptr&amp;lt;gadget&amp;gt; ); sink( unique_ptr&amp;lt;widget&amp;gt;{new widget{}}, unique_ptr&amp;lt;gadget&amp;gt;{new gadget{}} ); 위 코드에서 new gadget{} 안에서 예외가 발생할 경우 new widget{}은 해제되지 않아 메모리 누수가 발생할 수 있습니다. 또한 함수를 호출할 때 인자의 평가(Evaluation)는 명시되지 않은 행동(Unspecified Behavior)으로 컴파일러마다 정의되는 행동이 다르기 때문에, new widget{} 안에서 예외가 발생할 때 new gadget{}으로 생성된 객체가 해제되지 않는 반대의 상황 또한 발생할 수 있습니다. 이러한 이유로 항상 make_shared 또는 make_unique를 사용하는 것이 안전합니다. static_cast, dynamic_cast, 그리고 reinterpret_cast 캐스팅 연산은 보통 3가지(const_cast 제외) 중 1개를 선택하게 되는 데, static_cast와 dynamic_cast, 그리고 reinterpret_cast 캐스팅이 있습니다. static_cast 연산은 컴파일 시점에서 형 변환을 하며 대부분의 int, double과 같은 원시 타입(primitive type) 값을 변환할 때 사용하거나, 안전하다고 확신할 수 있는 때에(null이 아닌 경우)만 부모 클래스로 형 변환 등을 할 때 사용합니다. int a = 42; double b = static_cast&amp;lt;double&amp;gt;(a); dynamic_cast 연산은 컴파일 시점이 아닌 런타임 시점에서 형 변환을 합니다. 형변환이 실패한 경우 null을 반환하기 때문에 안전하다고 확신할 수 없는 모든 객체의 형 변환에 사용합니다. Class A {}; Class B : public A {}; B *b = new B(); A *a = dynamic_cast&amp;lt;A *&amp;gt;(b); assert(a); // a는 null이 아닙니다. 형변환이 실패하면 null 여부를 검사해 확인할 수 있습니다. 마지막으로 reinterpret_cast 연산은 가장 위험한 연산으로, C 스타일의 캐스팅과 동일한 형 변환을 수행합니다. 포인터를 정수로 저장하거나 정수를 포인터 주소로 사용하는 경우, 또는 void * 포인터 값을 임의의 클래스 포인터 주소로 사용하는 경우를 예로 들 수 있습니다. reinterpret_cast 연산은 형 변환에 사용되는 두 객체 값 제어가 가능한 경우가 아니면 사용하지 않는 게 좋습니다. assert 와 static_assert, 그리고 enable_if 다음으로 볼 내용은 assert와 static_assert 입니다. 그리고 가능한 경우 함께 사용할 수 있는 std::enable_if도 소개하려 합니다. 먼저 assert(eval)는 디버그 모드에서만 동작하는 메서드로 eval 식이 참이 아닌 경우 즉시 프로그램을 강제 종료하는 기능을 가지고 있습니다. assert는 로직이 런타임에서 의도한대로 동작하는 지 확인하고 싶을 때 사용하면 좋습니다. 검사 식 자체가 로직의 방향을 이해하는 데 큰 도움이 되기 때문에 복잡한 코드의 가독성을 크게 끌어올리는 장점을 가지고 있습니다. ... // 값을 찾았을 때는 level 값이 반드시 100 이상이고, 그렇지 않으면 100 미만이어야 함을 보장하는 assert // 이 식을 가정하에 코드가 작성됐다는 걸 알 수 있기 때문에 더 빠르게 코드를 이해할 수 있습니다. assert((found &amp;amp;&amp;amp; level &amp;gt;= 100) || (!found &amp;amp;&amp;amp; level &amp;lt; 100) ); static_assert는 런타임이 아닌 컴파일 시점에서 의도한대로 동작하는 지 확인할 때 사용하며, 전처리 매크로가 의도한 대로 설정됐는지, 템플릿 인자에 의도한 타입이 들어왔는지 확인할 때 유용하게 사용할 수 있습니다. // 템플릿 인자 T의 타입이 bool이 아닌 경우 컴파일 에러가 발생합니다. static_assert(std::is_same&amp;lt;decltype(T), bool&amp;gt;::value, &quot;T must be bool&quot;); 그러나 템플릿 인자 타입을 검사할 때는 std::enable_if를 사용하는 게 조금 더 좋습니다. 그 이유는 static_assert의 경우 생성된 코드 안에서 에러가 출력되고 에러가 발생한 위치를 알려주지 않는 반면, std::enable_if는 해당하는 템플릿 함수가 없다는 에러가 출력되고 템플릿 함수를 호출한 쪽에서 에러가 발생하기 때문에 원인을 쉽게 찾을 수 있기 때문입니다. // double 또는 int를 인자로 사용하는 CheckValue() 템플릿 함수만 사용 가능합니다. template &amp;lt;typename T, std::enable_if&amp;lt; std::is_same&amp;lt;T, std::double&amp;gt;::value || std::is_same&amp;lt;T, std::int&amp;gt;::value&amp;gt;::value&amp;gt; * = nullptr&amp;gt; void CheckValue(const T&amp;amp; value); const reference 키워드 const reference 는 const &amp;amp; 를 뜻하며 변경이 불가능하고 참조만 가능한 변수나 인자를 뜻합니다. 이 키워드는 함수의 인자, 스택 로컬 변수 등을 선언할 때 사용하는 게 좋습니다. // 사용 예 1 void func(const std::string &amp;amp;str) { // 사용 예 2 const std::string &amp;amp;name = GetStringValue() } const reference는 일반적인 reference (&amp;amp;)와는 조금 다른 특징을 가지고 있습니다. 첫 번째는 const 키워드로 인해 변경이 불가능하다는 것이고, 두 번째는 reference임에도 객체의 생명 주기(lifetime)가 더 길다는 것입니다. 위 예제의 사용 예 2를 보면 마치 GetStringValue() 가 반환하는 std::string 객체를 복사하지 않고 참조만 하는 것처럼 보이지만, 실제로는 const reference에 의해 생명 주기가 연장되어 name 변수가 사라질 때까지 유지됩니다(복사되는 것이 아니며 RVO와도 아무런 관계가 없다는 점에 주의하세요). const reference는 불필요한 복사를 막아주고 실수로 변수를 수정하는 일을 쉽게 제한할 수 있다는 점에서 안전한 코드를 만드는 데 큰 도움을 줄 수 있습니다. 요약 이번 글을 통해 소개드린 내용 말고도 많은 장치들을 활용하여 안전하게 코드를 만들 수 있습니다. 특히 템플릿이나 람다 식은 디버깅을 어렵게 만들 수 있기 때문에 가능한 적게 사용한다거나, constexpr if 등을 이용해 중복 코드를 최대한 제거하여 실수할 여지를 줄이는 것 등이 있을 것입니다. 때로는 스크립트로 생성되는 인터페이스로만 코드 사용이 가능하게 하는 것도 좋은 방법입니다. 물론 때로는 최적화를 위해 이해하기 어려운 코드를 만들어야 할 때도 있습니다. 이 때는 핵심적인 부분은 최대한 숨기고 주석으로 대체하되, 성능에 별 지장이 없는 논리적인 부분을 잘못 사용하기 어렵게 만드는 것이 매우 중요합니다. 핵심은 최대한 일관성 있는 코드를 만들고, 실수하기 어려운 환경을 만드는 것입니다. 저는 그것이 안전한 코드를 만드는 데 필요한 가장 중요한 원칙이라 생각합니다.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/thumbnail_secure_coding.jpg" /><media:content medium="image" url="http://github.sualab.io/thumbnail_secure_coding.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">파이토치에도 보일러플레이트가 스치운다</title><link href="http://github.sualab.io/development/2020/06/18/pytorch-boilerplate.html" rel="alternate" type="text/html" title="파이토치에도 보일러플레이트가 스치운다" /><published>2020-06-18T00:00:00+09:00</published><updated>2020-06-18T00:00:00+09:00</updated><id>http://github.sualab.io/development/2020/06/18/pytorch-boilerplate</id><content type="html" xml:base="http://github.sualab.io/development/2020/06/18/pytorch-boilerplate.html">&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/tldr.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/tldr.png&quot; alt=&quot;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;“OmegaConf”로 실험 설정을 관리하고 “Pytorch-Lightning”으로 실험 코드를 구성하고, “Microsoft NNI”+”Tensorboard”로 실험을 기록하는 과정을 “Docker” 환경을 구축해서 하자!&lt;/p&gt;

&lt;h2 id=&quot;딥러닝-실험이란&quot;&gt;딥러닝 “실험”이란?&lt;/h2&gt;

&lt;p&gt;연구는 관찰, 가설 설정, 실험 그리고 반복의 과정입니다. 관찰과 가설 설정은 연구자의 번뜩이는 아이디어를 갈고 닦으며 사고 과정에서 이루어질 수 있으나 결국 ‘실험’을 통해 여러 동료 연구자 및 통계 모델들에게 검증을 받아야 비로소 논문이 나옵니다. (혹은 제품화까지 갈 수 있겠네요!)&lt;/p&gt;

&lt;p&gt;그러면 딥러닝을 위한 실험은 어떻게 구성되어 있을까요? N명의 연구자가 있으면 N개의 연구 방법론들이 있겠습니다만, 필자는 딥러닝 실험을 “실험 도구”, “실험 수행” 그리고 “실험 환경”으로 나누어 접근하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/exp.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/exp.png&quot; alt=&quot;실험실의 삼요소&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;실험실의 삼요소&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;“실험 도구”는 우리의 가설을 현실에 구현할 매체입니다. 딥러닝에서는 실험을 수행할 Pytorch, Tensorflow, MXNet 등으로 생각해볼 수 있겠네요.&lt;/p&gt;

&lt;p&gt;“실험 수행”은 독립, 통제 변인과 그에 따른 종속 변인의 변화를 기록하며 가설을 수정하는 과정입니다. 코드에서 사용되는 여러 설정들을 관리하고 Loss/Metric들을 최적화시키는 프로세스라고 여겨집니다.&lt;/p&gt;

&lt;p&gt;마지막으로 “실험 환경”은 실험이 진행되는 환경입니다. 실험자는 패키지를 설치 후 고정된 환경에서 실험합니다만, 오픈 소스 기여자분들 덕분에 패키지는 지속해서 업데이트되어 인터페이스 혹은 작동 방법 등이 변경되곤 합니다.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-lightning--실험-도구&quot;&gt;&lt;a href=&quot;https://github.com/PyTorchLightning/pytorch-lightning&quot;&gt;Pytorch-Lightning&lt;/a&gt; : 실험 도구&lt;/h3&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/pl.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/pl.png&quot; alt=&quot;Pytorch 코드를 Pytorch Lightning으로 변환하는 도식&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Pytorch 코드를 Pytorch Lightning으로 변환하는 도식&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;“Pytorch Lightning”은 기존의 Pytorch 코드를 Research/Engineering/Non-essential 3가지로 구분하여 모델 정의 및 학습에 관련된 Research 코드 작성 외의 GPU 설정, 로깅, 실험 설정 등은 기본적으로 제공하여 적은 수정으로 사용할 수 있도록 제공합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Research Code == ‘LightningModule’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pytorch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lightning&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LightningModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'network'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# nn.Module
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hparams&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'network_option'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Dict Configs
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 네트워크 forward 정의
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;training_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_nb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1 train iteration 정의
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'loss'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'progress_bar'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train_loss'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;validation_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_nb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1 val iteration 정의
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;val_loss&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;validation_epoch_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1 val epoch 정의
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;avg_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;val_loss&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;val_loss&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;val_loss&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;log&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;progress_bar&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# train dataloader 정의
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;train&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;configure_optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# optimizer 정의
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Research 부분인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LightningModule&lt;/code&gt; 은 기존 pytorch의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; 에 데이터, 로스, 옵티마이저 설정을 추가한 모듈입니다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__init__&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; 에 네트워크 구조를 정의하고, 추가적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;training_step&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_dataloader&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configure_optimizers&lt;/code&gt;  등의 함수들을 오버 라이딩하여 학습 데이터 및 옵티마이저를 정의하여 줍니다.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;training_step&lt;/code&gt; , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;validation_step&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;validataion_epoch_end&lt;/code&gt;에서 “log” key로 리턴하는 내용은 Logger에 batch step 기준으로 로깅되고, “progress_bar” key로 리턴하는 내용은 terminal에서 progress bar 프로그램인 &lt;a href=&quot;https://github.com/tqdm/tqdm&quot;&gt;tqdm&lt;/a&gt;을 통해 출력됩니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/tqdm.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/tqdm.png&quot; alt=&quot;tqdm을 통해 출력되는 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;tqdm을 통해 출력되는 예시&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;LightningMoudule에 관해 더 자세한 내용은 &lt;a href=&quot;https://pytorch-lightning.readthedocs.io/en/0.7.6/lightning-module.html&quot;&gt;라이트닝 모듈 링크&lt;/a&gt;를 참고해주세요&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Engineering &amp;amp; Non-essential&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lightning&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Trainer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pytorch_lightning.logging&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorBoardLogger&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorBoardLogger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;./Logs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;exp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 실험 로거 정의
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;gpus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 사용할 gpu 개수 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;max_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 최대 epoch
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;log_save_interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# log 저장 간격
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# train, validation 실행
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# test 실행
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Engineering 관련 내용을 처리하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Trainer&lt;/code&gt;는 다양한 option들이 있습니다만, 기본적으로 logger와 gpu, epoch을 설정하여 기존의 코드에서 사용되었던 gpu 설정 및 for 반복문, metric 로깅 과정을 일부 생략할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이후 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trainer.fit&lt;/code&gt;을 통해 모델의 train/val 데이터를 이용한 학습 및 평가가 진행되고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trainer.test&lt;/code&gt;를 실행하여 test 데이터에 대한 최종 평가를 수행할 수 있습니다. 이 과정은 학습 중 tqdm을 통해 진행도가 출력되고 logger에 추가로 기록을 합니다. 위의 코드에서는 TensorBoardLogger를 사용하였으나 &lt;a href=&quot;https://www.comet.ml/&quot;&gt;Comet&lt;/a&gt;, &lt;a href=&quot;https://neptune.ai/&quot;&gt;Neptune&lt;/a&gt;, &lt;a href=&quot;https://www.wandb.com/&quot;&gt;WanDB&lt;/a&gt; 같은 다양한 로거들을 지원하고 있습니다&lt;/p&gt;

&lt;p&gt;gpu 분산 학습인 distributed training, TPU 사용 옵션, Nvidia에서 개발한 뉴럴넷 최적화 툴 &lt;a href=&quot;https://github.com/NVIDIA/apex&quot;&gt;APEX&lt;/a&gt;에 관련된 설정 등 여러 유용한 옵션들이 있으니 &lt;a href=&quot;https://pytorch-lightning.readthedocs.io/en/0.7.6/trainer.html#amp-level&quot;&gt;https://pytorch-lightning.readthedocs.io/en/0.7.6/trainer.html&lt;/a&gt;를 참고하시길 바랍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;앗! 잠깐만!&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch_lightning.seed_everything(seed)&lt;/code&gt; 를 코드 처음에 넣어두면 seed를 고정해 실험 reproducible를 확보할 수 있다는 사실~&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ms-nni-omegaconf--실험-수행&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/nni&quot;&gt;MS NNI&lt;/a&gt;, &lt;a href=&quot;https://github.com/omry/omegaconf&quot;&gt;OmegaConf&lt;/a&gt; : 실험 수행&lt;/h3&gt;

&lt;p&gt;위의 Pytorch Lightning 코드만 모두 작성하더라도, 실험 준비는 완료된 것이나 다름없습니다! 하지만, 실험은 한번 수행하고 끝나는 것이 아닌 무한 번의 실험을 통해 도출된 결과들을 분석하여 최적의 값을 찾는 것이 우리의 일입니다. 실험 수행 중 조금이나마 실험 관리 자동화를 도와줄 도구 Microsoft Neural Network Intelligence와 OmegaConf를 소개합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Microsoft Neural Network (aka NNI)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;NNI는 머신 러닝 과정 중 Hyper-parameter Tuning, feature engineering, neural architecture search 등의 과정을 도와주는 AutoML 오픈 소스 프로젝트입니다. 다양한 기능이 포함된 멋진 툴킷이지만, 오늘은 Hyper-parameter Tuning 정도만 소개를 드리려고 합니다.&lt;/p&gt;

&lt;p&gt;NNI는 3가지 과정을 통해 사용할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/nni.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/nni.png&quot; alt=&quot;MS NNI를 코드에 적용하는 3가지 단계&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;MS NNI를 코드에 적용하는 3가지 단계&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 과정으로는 Tuning을 적용할 Search Space 정의입니다. 작게는 learning rate, batch size부터 크게는 네트워크 종류 선택까지 적용해볼 수 있겠네요. Auto Searcher (Grid, GP, PPO 등)의 종류에 따라 다르지만, search 범위는 범주형 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;choice&lt;/code&gt;,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uniform&lt;/code&gt; , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;randint&lt;/code&gt; 정도의 타입으로 나눌 수 있겠습니다. 해당 설정은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.json&lt;/code&gt;타입으로 작성하시면 됩니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;train.batch_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;choice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]},&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;network.version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;choice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1_0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그다음에 search parameter를 코드에 연결하고 최적화할 목표 metric을 설정해야 합니다. 필수적으로는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;report_final_result&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_next_parameter&lt;/code&gt;를 사용하여야 하고, 선택적으로 학습 과정 중 관찰하고 싶은 metric을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;report_intermediate_result&lt;/code&gt;로 NNI 플랫폼에 기록해볼 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DefaultConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nni&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_next_parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 정의된 search space에서 next step의 config 호출
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;search_params_intp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OmegaConf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;structured&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OmegaConf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# nni search config와 기본 config 병합
&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ml&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MainPL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;final_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ml&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nni&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report_final_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# nni에서 추적할 log 기록
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;마지막으로는 AutoML을 할 때 사용할 Tuner와 탐색 시간 및 자원 설정입니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.yml&lt;/code&gt; 타입으로 작성하면 되고, 필수적으로는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;useAnnotation&lt;/code&gt; 을 false로 지정하고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;searchSpacePath&lt;/code&gt; 의 값으로 탐색할 값을 불러옵니다. 추가적인 옵션으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trialConcurrency&lt;/code&gt; 로 동시에 수행할 실험, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxExecDuration&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxTrialNum&lt;/code&gt; 으로 탐색 시간 및 횟수를 설정할 수 있습니다. 그리고 제일 중요한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tuner:builtinTuerName&lt;/code&gt; 을 통해 파라미터 탐색 시 사용할 Tuner를 고를 수 있습니다. Tuner는 기본적으로 모든 조합을 찾는 Grid Search, 강화학습 기반의 PPO Tuner, Random Search 등의 다양한 옵션들이 있습니다. 더욱 다양한 튜너에 대한 옵션과 사용법들은 &lt;a href=&quot;https://nni.readthedocs.io/en/latest/hyperparameter_tune.html&quot;&gt;https://nni.readthedocs.io/en/latest/hyperparameter_tune.html&lt;/a&gt;에서 확인해주시면 됩니다!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;authorName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;davinnovation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;experimentName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example_mnist&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trialConcurrency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;maxExecDuration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;maxTrialNum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;trainingServicePlatform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#choice: local, remote, pai
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;searchSpacePath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;useAnnotation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tuner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;builtinTunerName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GridSearch&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run_nni&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;codeDir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gpuNum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위의 단계를 마치고 NNI을 실행시킨다면 다음과 같은 결과를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:8080&lt;/code&gt; 에 접속하시면 현재 탐색 실행 상황 및 최적 값 등을 확인하실 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/nni2.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/nni2.png&quot; alt=&quot;NNI를 실행 후 parameter를 grid search하고 loss 결과에 매핑한 그래프&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;NNI를 실행 후 parameter를 grid search하고 loss 결과에 매핑한 그래프&lt;/span&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;OmegaConf&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;딥러닝 프로젝트를 진행하다 보면 초반에는 batch size, epoch 등의 적은 옵션들만 설정하여 하드 코딩으로 관리하기 괜찮지만, 설정의 종류가 시간이 지나며 늘어나거나 cli, config.json 등등 옵션들에 대한 entry point가 많아지면 관리하기도 힘든데요, 이를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OmegaConf&lt;/code&gt; 를 통해 관리하면 조금 더 편하게 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;여러 사용법이 있습니다만, 필자는 데이터를 관리하는데 기본적인 매니징 기능이 추가된 &lt;a href=&quot;https://docs.python.org/ko/3/library/dataclasses.html&quot;&gt;dataclass&lt;/a&gt; 를 기반으로 설정 관리하는 것을 선호합니다. 우선 config.py에 관리하고자 하는 설정을 입력합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dataclasses&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataclass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;field&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;omegaconf&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MISSING&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataclass&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TrainConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataclass&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;OptConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# flexible
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Adam&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataclass&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DefaultConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrainConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrainConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ValConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ValConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TestConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TestConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HWConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HWConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NetworkConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NetworkConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OptConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OptConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 위의 Config.py를 main에서 호출하여 기본 config로 사용하고, entry point를 다양화할 수 있도록 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merge_with_cli&lt;/code&gt; 등을 통해 수정할 수 있도록 합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DefaultConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OmegaConf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;structured&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cfg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge_with_cli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;OmegaConf의 장점은 계층적인 설정을 다양한 entry point에서 (dictionary, dataclass, cli args 변환, list etc) 입력받을 수 있고 통합할 수 있도록 지원합니다. 사용법을 추가로 확인하고 싶으시다면 &lt;a href=&quot;https://docs.google.com/presentation/d/e/2PACX-1vT_UIV7hCnquIbLUm4NnkUpXvPEh33IKiUEvPRF850WKA8opOlZOszjKdZ3tPmf8u7hGNP6HpqS-NT5/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000&amp;amp;slide=id.g84632f636b_10_519&quot;&gt;omegaconf 2.0 slide&lt;/a&gt;를 확인해주세요&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;앗! 잠깐만!&lt;/p&gt;

    &lt;p&gt;아쉽게도 OmegaConf는 argparse나 python-fire 등의 argument manager들이 기본적으로 지원하는 auto doc을 생성하지 않아 —help 명령어를 사용하는 것이 어렵습니다 ㅠㅠ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;anaconda-docker--실험-환경&quot;&gt;&lt;a href=&quot;https://www.anaconda.com/products/individual&quot;&gt;Anaconda&lt;/a&gt;, &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; : 실험 환경&lt;/h3&gt;

&lt;p&gt;딥러닝 실험 코드를 작성하고, 설정 관리 및 자동 탐색 툴까지 붙였는데 더 해야 하는 것이 있을까요? 이번 툴은 필수는 아니나 추후에 다시 코드 실행 환경을 복구하기 위한 Docker와 Anaconda를 통한 파이썬 라이브러리 관리를 소개드립니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/meme.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/meme.png&quot; alt=&quot;논문 코드가 바로 실행될 때.jpg&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;논문 코드가 바로 실행될 때.jpg&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;논문을 탐색하며 github 링크가 있을 때는 정말 기쁩니다만, repo를 pull 해서 실행 시 requirements가 없으면 당황스럽죠.  많은 연구원들이 실험할 때 여러 패키지를 사용하며 코드를 작성합니다만, 사용되었던 패키지들은 업데이트가 되며 몇 년 뒤에는 예전 코드를 실행할 수 없을 때가 종종 생기게 됩니다. 이럴 때를 위해 실험 환경을 정확하게 기술해두면 좋지만, 연구자로서는 local의 환경을 계속 확인하며 공유하는 게 번거로운 일이 됩니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Anaconda&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://www.anaconda.com/products/individual&quot;&gt;Anaconda&lt;/a&gt;는 데이터 과학을 위한 Python 패키지들과 라이브러리들을 모아두고 환경 관리 및 설치를 도와주는 소프트웨어입니다.&lt;/p&gt;

&lt;p&gt;Anaconda를 설치하시고 path를 설정해주시면 prompt 창에서 가장 왼쪽에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(base)&lt;/code&gt;로 표시가 됩니다. 이는 현재 base라는 기본 환경을 가지는 python을 실행하기 위해 준비되었다는 의미입니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/conda.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/pytorch-boilerplate/conda.png&quot; alt=&quot;conda가 준비되었을 때의 콘솔창&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;conda가 준비되었을 때의 콘솔창&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;한 개의 환경에 모든 라이브러리를 설치하지 않고 각 프로젝트를 위한 환경을 만들어서 관리하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda create -n torch_py37 python=3.7 anaconda&lt;/code&gt; 명령어로 torch_py36이라는 새로운 환경을 만들어줍니다. 그 다음 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda activate torch_py37&lt;/code&gt; 를 통해 환경을 교체할 수 있습니다.&lt;/p&gt;

&lt;p&gt;그리고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda install&lt;/code&gt;을 통해 여러 패키지를 설치하시고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda env export &amp;gt; env.yaml&lt;/code&gt; 로 환경을 공유할 수 있습니다. 그리고 다른 anaconda에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda create env -f env.yaml&lt;/code&gt; 를 통해 재사용할 수 있게됩니다!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Docker&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;“도커 컨테이너”는 일종의 소프트웨어를 소프트웨어의 실행에 필요한 모든 것을 포함하는 완전한 파일 시스템 안에 감싼다. 여기에는 코드, 런타임, 시스템 도구, 시스템 라이브러리 등 서버에 설치되는 무엇이든 아우른다. 이는 실행 중인 환경에 관계없이 언제나 동일하게 실행될 것을 보증한다. - 위키피디아&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Docker는 소프트웨어를 실행하기 위한 환경 관리 툴입니다.&lt;/p&gt;

&lt;p&gt;필자는 floydhub의 도커 이미지를 기반으로 실험을 진행합니다. 각 딥러닝 프레임워크를 위한 이미지들이 준비되어 있고 추가 패키지들과 CUDA 버전 별 관리가 되어있어 많은 추가 패키지들을 설치하지 않고 기본으로 사용하기 좋습니다 - &lt;a href=&quot;https://hub.docker.com/u/floydhub&quot;&gt;https://hub.docker.com/u/floydhub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;위의 Docker Image를 확장하여 자신만의 Dockerfile을 만들고 관리하는 것도 쉽습니다.&lt;/p&gt;

&lt;div class=&quot;language-docker highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; floydhub/pytorch:1.5.0-gpu.cuda10cudnn7-py3.55&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;app
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; app&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;git clone https://github.com/davinnovation/pytorch-boilerplate
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; pytorch-boilerplate&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; requirements.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Dockerfile을 만들고 base로 사용할 image를 FROM으로 기술합니다. 그리고 그 뒤에는 RUN을 통해 추가로 필요한 파이썬 패키지, 소프트웨어 등을 설치하는 명령어를 적어두면 어느 컴퓨터나 동일한 실험 환경을 사용할 수 있습니다!&lt;/p&gt;

&lt;p&gt;Docker에 관한 자세한 내용 등은 &lt;a href=&quot;https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html&quot;&gt;https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html&lt;/a&gt; 여기를 따라가 보시면 좋겠습니다!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;앗! 잠깐만!&lt;/p&gt;

    &lt;p&gt;그리고 아직은 윈도우 Docker에서는 GPU 사용이 불가능하여 리눅스를 사용하는 것을 권장해 드립니다만, WSL 2라는 윈도우 10의 가상화 머신이 곧 GPU를 지원한다고 하니 Follow UP!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;example-code&quot;&gt;Example Code&lt;/h3&gt;

&lt;p&gt;지금까지 살펴본 코드 과정의 전체 플로우를 지닌 프로젝트 예제를 보고 싶으시다면 &lt;a href=&quot;https://github.com/davinnovation/pytorch-boilerplate/&quot;&gt;https://github.com/davinnovation/pytorch-boilerplate/&lt;/a&gt;을 참고해주세요!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/davinnovation/pytorch-boilerplate/&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/3917185/84723043-ac25e380-afbf-11ea-9116-fbabd47b5cc0.png&quot; alt=&quot;image&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;참고-보면-좋은-것들&quot;&gt;참고. 보면 좋은 것들&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;AI 연구자를 위한 클린코드 작성법&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/KennethCeyer/ai-gdg-devfest-seoul-2019-187630418&quot;&gt;https://www.slideshare.net/KennethCeyer/ai-gdg-devfest-seoul-2019-187630418&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;추가하면 좋을 것들&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://openpai.readthedocs.io/&quot;&gt;OpenPAI&lt;/a&gt;, &lt;a href=&quot;https://github.com/horovod/horovod&quot;&gt;Horovord&lt;/a&gt;: 효과적인 학습을 위한 Distributed Training 플랫폼입니다&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://github.com/psf/black&quot;&gt;black&lt;/a&gt; : 코드 포맷팅&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://github.com/psf/black&quot;&gt;line_profiler&lt;/a&gt;, &lt;a href=&quot;https://github.com/what-studio/profiling&quot;&gt;profiling&lt;/a&gt; : 코드 성능 프로파일링&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;위 프로젝트들의 대체&lt;/p&gt;

    &lt;p&gt;Pytorch lightning → &lt;a href=&quot;https://github.com/pytorch/ignite&quot;&gt;Pytorch Ignite&lt;/a&gt; : Pytorch 그룹의 공식 High Level Framework입니다&lt;/p&gt;

    &lt;p&gt;NNI → &lt;a href=&quot;https://docs.ray.io/en/master/tune.html&quot;&gt;Ray.Tune&lt;/a&gt; : 널리 사용되는 Parameter 튜너입니다&lt;/p&gt;

    &lt;p&gt;OmegaConf → &lt;a href=&quot;https://github.com/google/python-fire&quot;&gt;python-fire&lt;/a&gt; : google에서 만든 옵션에 대한 명시 없이 class/function을 기반으로 cli interface를 만들어 주는 라이브러리입니다&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;다른-글도-읽으러-가기&quot;&gt;다른 글도 읽으러 가기&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://research.sualab.com/introduction/practice/2019/05/08/generative-adversarial-network.html&quot;&gt;DCGAN을 이용한 이미지 생성&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://research.sualab.com/introduction/2019/08/30/interpretable-machine-learning-overview-1.html&quot;&gt;머신러닝 모델에 대한 해석력 확보를 위한 방법&lt;/a&gt;&lt;/p&gt;</content><author><name>조동헌</name><email>david.cho@cognex.com</email></author><category term="Development" /><category term="pytorch-lightning" /><category term="boilerplate" /><summary type="html">TL;DR “OmegaConf”로 실험 설정을 관리하고 “Pytorch-Lightning”으로 실험 코드를 구성하고, “Microsoft NNI”+”Tensorboard”로 실험을 기록하는 과정을 “Docker” 환경을 구축해서 하자! 딥러닝 “실험”이란? 연구는 관찰, 가설 설정, 실험 그리고 반복의 과정입니다. 관찰과 가설 설정은 연구자의 번뜩이는 아이디어를 갈고 닦으며 사고 과정에서 이루어질 수 있으나 결국 ‘실험’을 통해 여러 동료 연구자 및 통계 모델들에게 검증을 받아야 비로소 논문이 나옵니다. (혹은 제품화까지 갈 수 있겠네요!) 그러면 딥러닝을 위한 실험은 어떻게 구성되어 있을까요? N명의 연구자가 있으면 N개의 연구 방법론들이 있겠습니다만, 필자는 딥러닝 실험을 “실험 도구”, “실험 수행” 그리고 “실험 환경”으로 나누어 접근하였습니다. 실험실의 삼요소 “실험 도구”는 우리의 가설을 현실에 구현할 매체입니다. 딥러닝에서는 실험을 수행할 Pytorch, Tensorflow, MXNet 등으로 생각해볼 수 있겠네요. “실험 수행”은 독립, 통제 변인과 그에 따른 종속 변인의 변화를 기록하며 가설을 수정하는 과정입니다. 코드에서 사용되는 여러 설정들을 관리하고 Loss/Metric들을 최적화시키는 프로세스라고 여겨집니다. 마지막으로 “실험 환경”은 실험이 진행되는 환경입니다. 실험자는 패키지를 설치 후 고정된 환경에서 실험합니다만, 오픈 소스 기여자분들 덕분에 패키지는 지속해서 업데이트되어 인터페이스 혹은 작동 방법 등이 변경되곤 합니다. Pytorch-Lightning : 실험 도구 Pytorch 코드를 Pytorch Lightning으로 변환하는 도식 “Pytorch Lightning”은 기존의 Pytorch 코드를 Research/Engineering/Non-essential 3가지로 구분하여 모델 정의 및 학습에 관련된 Research 코드 작성 외의 GPU 설정, 로깅, 실험 설정 등은 기본적으로 제공하여 적은 수정으로 사용할 수 있도록 제공합니다. Research Code == ‘LightningModule’ import pytorch-lightning as pl class PL(pl.LightningModule): def __init__(self, network: dict, dataloader: dict): super().__init__() self.network = network['network'] # nn.Module self.hparams = dict(network['network_option']) # Dict Configs self.loss = nn.CrossEntropyLoss() def forward(self, batch, network): # 네트워크 forward 정의 img = batch[0] Y = batch[1] pred = network(img) return pred, self.loss(pred.float(), Y.long()) def training_step(self, batch, batch_nb): # 1 train iteration 정의 pred, loss = self.forward(batch, self.network) return {'loss' : loss, 'progress_bar' : {'train_loss' : loss }} def validation_step(self, batch, batch_nb): # 1 val iteration 정의 pred, loss = self.forward(batch, self.network) return {&quot;val_loss&quot;: loss} def validation_epoch_end(self, outputs): # 1 val epoch 정의 avg_loss = torch.stack([x[&quot;val_loss&quot;] for x in outputs]).mean() logs = {&quot;val_loss&quot;: avg_loss} return {&quot;val_loss&quot;: avg_loss, &quot;log&quot;: logs, &quot;progress_bar&quot;: logs} def train_dataloader(self): # train dataloader 정의 return self.dataloader[&quot;train&quot;] def configure_optimizers(self): # optimizer 정의 return torch.optim.Adam(self.parameters(), lr=0.02) ... Research 부분인 LightningModule 은 기존 pytorch의 nn.Module 에 데이터, 로스, 옵티마이저 설정을 추가한 모듈입니다. __init__ forward 에 네트워크 구조를 정의하고, 추가적으로 training_step train_dataloader configure_optimizers 등의 함수들을 오버 라이딩하여 학습 데이터 및 옵티마이저를 정의하여 줍니다. training_step , validation_step, validataion_epoch_end에서 “log” key로 리턴하는 내용은 Logger에 batch step 기준으로 로깅되고, “progress_bar” key로 리턴하는 내용은 terminal에서 progress bar 프로그램인 tqdm을 통해 출력됩니다. tqdm을 통해 출력되는 예시 LightningMoudule에 관해 더 자세한 내용은 라이트닝 모듈 링크를 참고해주세요 Engineering &amp;amp; Non-essential from pytorch-lightning import Trainer from pytorch_lightning.logging import TensorBoardLogger ... trainer = Trainer( logger=TensorBoardLogger(save_dir=&quot;./Logs&quot;, name=&quot;exp&quot;), # 실험 로거 정의 gpus=1, # 사용할 gpu 개수 max_epochs=100, # 최대 epoch log_save_interval=1, # log 저장 간격 ) ... trainer.fit(pl) # train, validation 실행 trainer.test(pl) # test 실행 Engineering 관련 내용을 처리하는 Trainer는 다양한 option들이 있습니다만, 기본적으로 logger와 gpu, epoch을 설정하여 기존의 코드에서 사용되었던 gpu 설정 및 for 반복문, metric 로깅 과정을 일부 생략할 수 있습니다. 이후 trainer.fit을 통해 모델의 train/val 데이터를 이용한 학습 및 평가가 진행되고, trainer.test를 실행하여 test 데이터에 대한 최종 평가를 수행할 수 있습니다. 이 과정은 학습 중 tqdm을 통해 진행도가 출력되고 logger에 추가로 기록을 합니다. 위의 코드에서는 TensorBoardLogger를 사용하였으나 Comet, Neptune, WanDB 같은 다양한 로거들을 지원하고 있습니다 gpu 분산 학습인 distributed training, TPU 사용 옵션, Nvidia에서 개발한 뉴럴넷 최적화 툴 APEX에 관련된 설정 등 여러 유용한 옵션들이 있으니 https://pytorch-lightning.readthedocs.io/en/0.7.6/trainer.html를 참고하시길 바랍니다. 앗! 잠깐만! pytorch_lightning.seed_everything(seed) 를 코드 처음에 넣어두면 seed를 고정해 실험 reproducible를 확보할 수 있다는 사실~ MS NNI, OmegaConf : 실험 수행 위의 Pytorch Lightning 코드만 모두 작성하더라도, 실험 준비는 완료된 것이나 다름없습니다! 하지만, 실험은 한번 수행하고 끝나는 것이 아닌 무한 번의 실험을 통해 도출된 결과들을 분석하여 최적의 값을 찾는 것이 우리의 일입니다. 실험 수행 중 조금이나마 실험 관리 자동화를 도와줄 도구 Microsoft Neural Network Intelligence와 OmegaConf를 소개합니다. Microsoft Neural Network (aka NNI) NNI는 머신 러닝 과정 중 Hyper-parameter Tuning, feature engineering, neural architecture search 등의 과정을 도와주는 AutoML 오픈 소스 프로젝트입니다. 다양한 기능이 포함된 멋진 툴킷이지만, 오늘은 Hyper-parameter Tuning 정도만 소개를 드리려고 합니다. NNI는 3가지 과정을 통해 사용할 수 있습니다. MS NNI를 코드에 적용하는 3가지 단계 첫 번째 과정으로는 Tuning을 적용할 Search Space 정의입니다. 작게는 learning rate, batch size부터 크게는 네트워크 종류 선택까지 적용해볼 수 있겠네요. Auto Searcher (Grid, GP, PPO 등)의 종류에 따라 다르지만, search 범위는 범주형 choice, uniform , randint 정도의 타입으로 나눌 수 있겠습니다. 해당 설정은 .json타입으로 작성하시면 됩니다. { &quot;train.batch_size&quot;:{&quot;_type&quot;:&quot;choice&quot;,&quot;_value&quot;:[32,64,128]}, &quot;network.version&quot;:{&quot;_type&quot;:&quot;choice&quot;,&quot;_value&quot;:[&quot;1_0&quot;, &quot;1_1&quot;]} } 그다음에 search parameter를 코드에 연결하고 최적화할 목표 metric을 설정해야 합니다. 필수적으로는 report_final_result와 get_next_parameter를 사용하여야 하고, 선택적으로 학습 과정 중 관찰하고 싶은 metric을 report_intermediate_result로 NNI 플랫폼에 기록해볼 수 있습니다. def _main(cfg=dc.DefaultConfig) -&amp;gt; None: params = nni.get_next_parameter() # 정의된 search space에서 next step의 config 호출 params = search_params_intp(params) cfg = OmegaConf.structured(cfg) args = OmegaConf.merge(cfg, params) # nni search config와 기본 config 병합 ml = main_pl.MainPL( args.train, args.val, args.test, args.hw, args.network, args.data, args.opt, args.log, args.seed ) final_result = ml.run() nni.report_final_result(final_result) # nni에서 추적할 log 기록 마지막으로는 AutoML을 할 때 사용할 Tuner와 탐색 시간 및 자원 설정입니다. .yml 타입으로 작성하면 되고, 필수적으로는 useAnnotation 을 false로 지정하고 searchSpacePath 의 값으로 탐색할 값을 불러옵니다. 추가적인 옵션으로 trialConcurrency 로 동시에 수행할 실험, maxExecDuration maxTrialNum 으로 탐색 시간 및 횟수를 설정할 수 있습니다. 그리고 제일 중요한 tuner:builtinTuerName 을 통해 파라미터 탐색 시 사용할 Tuner를 고를 수 있습니다. Tuner는 기본적으로 모든 조합을 찾는 Grid Search, 강화학습 기반의 PPO Tuner, Random Search 등의 다양한 옵션들이 있습니다. 더욱 다양한 튜너에 대한 옵션과 사용법들은 https://nni.readthedocs.io/en/latest/hyperparameter_tune.html에서 확인해주시면 됩니다! authorName: davinnovation experimentName: example_mnist trialConcurrency: 1 maxExecDuration: 1h maxTrialNum: 10 trainingServicePlatform: local #choice: local, remote, pai searchSpacePath: config/search.json useAnnotation: false tuner: builtinTunerName: GridSearch trial: command: python run_nni.py codeDir: . gpuNum: 1 위의 단계를 마치고 NNI을 실행시킨다면 다음과 같은 결과를 localhost:8080 에 접속하시면 현재 탐색 실행 상황 및 최적 값 등을 확인하실 수 있습니다. NNI를 실행 후 parameter를 grid search하고 loss 결과에 매핑한 그래프 OmegaConf 딥러닝 프로젝트를 진행하다 보면 초반에는 batch size, epoch 등의 적은 옵션들만 설정하여 하드 코딩으로 관리하기 괜찮지만, 설정의 종류가 시간이 지나며 늘어나거나 cli, config.json 등등 옵션들에 대한 entry point가 많아지면 관리하기도 힘든데요, 이를 OmegaConf 를 통해 관리하면 조금 더 편하게 사용할 수 있습니다. 여러 사용법이 있습니다만, 필자는 데이터를 관리하는데 기본적인 매니징 기능이 추가된 dataclass 를 기반으로 설정 관리하는 것을 선호합니다. 우선 config.py에 관리하고자 하는 설정을 입력합니다. from dataclasses import dataclass, field from omegaconf import MISSING @dataclass class TrainConfig: batch_size: int = 256 epoch: int = 5 ... @dataclass class OptConfig: # flexible opt: str = &quot;Adam&quot; lr: float = 1e-3 @dataclass class DefaultConfig: train: TrainConfig = TrainConfig() val: ValConfig = ValConfig() test: TestConfig = TestConfig() hw: HWConfig = HWConfig() network: NetworkConfig = NetworkConfig() data: DataConfig = DataConfig() opt: OptConfig = OptConfig() log: LogConfig = LogConfig() seed: str = 42 그리고 위의 Config.py를 main에서 호출하여 기본 config로 사용하고, entry point를 다양화할 수 있도록 merge_with_cli 등을 통해 수정할 수 있도록 합니다. ... def _main(cfg=dc.DefaultConfig) -&amp;gt; None: args = OmegaConf.structured(cfg) args.merge_with_cli() ... OmegaConf의 장점은 계층적인 설정을 다양한 entry point에서 (dictionary, dataclass, cli args 변환, list etc) 입력받을 수 있고 통합할 수 있도록 지원합니다. 사용법을 추가로 확인하고 싶으시다면 omegaconf 2.0 slide를 확인해주세요 앗! 잠깐만! 아쉽게도 OmegaConf는 argparse나 python-fire 등의 argument manager들이 기본적으로 지원하는 auto doc을 생성하지 않아 —help 명령어를 사용하는 것이 어렵습니다 ㅠㅠ Anaconda, Docker : 실험 환경 딥러닝 실험 코드를 작성하고, 설정 관리 및 자동 탐색 툴까지 붙였는데 더 해야 하는 것이 있을까요? 이번 툴은 필수는 아니나 추후에 다시 코드 실행 환경을 복구하기 위한 Docker와 Anaconda를 통한 파이썬 라이브러리 관리를 소개드립니다. 논문 코드가 바로 실행될 때.jpg 논문을 탐색하며 github 링크가 있을 때는 정말 기쁩니다만, repo를 pull 해서 실행 시 requirements가 없으면 당황스럽죠. 많은 연구원들이 실험할 때 여러 패키지를 사용하며 코드를 작성합니다만, 사용되었던 패키지들은 업데이트가 되며 몇 년 뒤에는 예전 코드를 실행할 수 없을 때가 종종 생기게 됩니다. 이럴 때를 위해 실험 환경을 정확하게 기술해두면 좋지만, 연구자로서는 local의 환경을 계속 확인하며 공유하는 게 번거로운 일이 됩니다. Anaconda Anaconda는 데이터 과학을 위한 Python 패키지들과 라이브러리들을 모아두고 환경 관리 및 설치를 도와주는 소프트웨어입니다. Anaconda를 설치하시고 path를 설정해주시면 prompt 창에서 가장 왼쪽에 (base)로 표시가 됩니다. 이는 현재 base라는 기본 환경을 가지는 python을 실행하기 위해 준비되었다는 의미입니다. conda가 준비되었을 때의 콘솔창 한 개의 환경에 모든 라이브러리를 설치하지 않고 각 프로젝트를 위한 환경을 만들어서 관리하기 위해 conda create -n torch_py37 python=3.7 anaconda 명령어로 torch_py36이라는 새로운 환경을 만들어줍니다. 그 다음 conda activate torch_py37 를 통해 환경을 교체할 수 있습니다. 그리고 conda install을 통해 여러 패키지를 설치하시고, conda env export &amp;gt; env.yaml 로 환경을 공유할 수 있습니다. 그리고 다른 anaconda에서는 conda create env -f env.yaml 를 통해 재사용할 수 있게됩니다! Docker “도커 컨테이너”는 일종의 소프트웨어를 소프트웨어의 실행에 필요한 모든 것을 포함하는 완전한 파일 시스템 안에 감싼다. 여기에는 코드, 런타임, 시스템 도구, 시스템 라이브러리 등 서버에 설치되는 무엇이든 아우른다. 이는 실행 중인 환경에 관계없이 언제나 동일하게 실행될 것을 보증한다. - 위키피디아 Docker는 소프트웨어를 실행하기 위한 환경 관리 툴입니다. 필자는 floydhub의 도커 이미지를 기반으로 실험을 진행합니다. 각 딥러닝 프레임워크를 위한 이미지들이 준비되어 있고 추가 패키지들과 CUDA 버전 별 관리가 되어있어 많은 추가 패키지들을 설치하지 않고 기본으로 사용하기 좋습니다 - https://hub.docker.com/u/floydhub 위의 Docker Image를 확장하여 자신만의 Dockerfile을 만들고 관리하는 것도 쉽습니다. FROM floydhub/pytorch:1.5.0-gpu.cuda10cudnn7-py3.55 RUN mkdir app WORKDIR app RUN git clone https://github.com/davinnovation/pytorch-boilerplate WORKDIR pytorch-boilerplate RUN pip install -r requirements.txt Dockerfile을 만들고 base로 사용할 image를 FROM으로 기술합니다. 그리고 그 뒤에는 RUN을 통해 추가로 필요한 파이썬 패키지, 소프트웨어 등을 설치하는 명령어를 적어두면 어느 컴퓨터나 동일한 실험 환경을 사용할 수 있습니다! Docker에 관한 자세한 내용 등은 https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html 여기를 따라가 보시면 좋겠습니다! 앗! 잠깐만! 그리고 아직은 윈도우 Docker에서는 GPU 사용이 불가능하여 리눅스를 사용하는 것을 권장해 드립니다만, WSL 2라는 윈도우 10의 가상화 머신이 곧 GPU를 지원한다고 하니 Follow UP! Example Code 지금까지 살펴본 코드 과정의 전체 플로우를 지닌 프로젝트 예제를 보고 싶으시다면 https://github.com/davinnovation/pytorch-boilerplate/을 참고해주세요! 참고. 보면 좋은 것들 AI 연구자를 위한 클린코드 작성법 https://www.slideshare.net/KennethCeyer/ai-gdg-devfest-seoul-2019-187630418 추가하면 좋을 것들 OpenPAI, Horovord: 효과적인 학습을 위한 Distributed Training 플랫폼입니다 black : 코드 포맷팅 line_profiler, profiling : 코드 성능 프로파일링 위 프로젝트들의 대체 Pytorch lightning → Pytorch Ignite : Pytorch 그룹의 공식 High Level Framework입니다 NNI → Ray.Tune : 널리 사용되는 Parameter 튜너입니다 OmegaConf → python-fire : google에서 만든 옵션에 대한 명시 없이 class/function을 기반으로 cli interface를 만들어 주는 라이브러리입니다 다른 글도 읽으러 가기 DCGAN을 이용한 이미지 생성 머신러닝 모델에 대한 해석력 확보를 위한 방법</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/exp.png" /><media:content medium="image" url="http://github.sualab.io/exp.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">C++ 기반 윈도우 데스크톱 애플리케이션 의존성 관리</title><link href="http://github.sualab.io/development/2020/05/07/dependency-management-for-cpp-windows-app-using-nuget.html" rel="alternate" type="text/html" title="C++ 기반 윈도우 데스크톱 애플리케이션 의존성 관리" /><published>2020-05-07T23:00:00+09:00</published><updated>2020-05-07T23:00:00+09:00</updated><id>http://github.sualab.io/development/2020/05/07/dependency-management-for-cpp-windows-app-using-nuget</id><content type="html" xml:base="http://github.sualab.io/development/2020/05/07/dependency-management-for-cpp-windows-app-using-nuget.html">&lt;p&gt;라이브러리를 사용할 때는 프로젝트가 사용하는 라이브러리 목록과 버전을 명확히 관리해야 합니다. 개발 환경과 배포 환경의 라이브러리 버전 차이가 의도치 않은 동작을 만들 수 있기 때문입니다. 또한 모든 팀원이 같은 라이브러리 버전을 사용해야 팀원 개개인이 동일한 동작을 기대하고 기능을 개발할 수 있습니다.&lt;/p&gt;

&lt;p&gt;C++처럼 개발 플랫폼(운영체제, 컴파일러 등)이 다양한 환경에서는 목록과 버전 외에 확인해야 할 게 더 많습니다. 예를 들면 라이브러리의 아키텍처(x32, x64, arm)와 컴파일러 버전(vc16, gcc 등)과 컴파일러 옵션, CPU 명령어 셋 집합이 배포 환경에서 지원하는지 등을 확인해야 합니다.&lt;/p&gt;

&lt;p&gt;대부분의 개발 환경은 손쉽게 라이브러리 목록과 버전을 관리할 수 있습니다. 파이썬을 예로 들면, requirements.txt 파일을 통해 프로젝트가 사용하는 라이브러리와 버전을 명확하게 관리할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/python_requirement_txt.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/python_requirement_txt.png&quot; alt=&quot;Python 기반 프로젝트의 라이브러리 의존성 관리 파일(requirements.txt)&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Python 기반 프로젝트의 라이브러리 의존성 관리 파일(requirements.txt)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;그러나 모든 개발 환경이 이런 편리한 기능을 제공하지는 않습니다. 저희 코어 팀에서 사용하는 C++도 마찬가지였습니다. 제가 팀에 합류한 시점에는 라이브러리들을 모두 별도 디렉토리에 넣고 환경 변수로 관리했는데, 이는 개발 팀원 간 통일된 개발 환경을 만들기 어렵게 했습니다. 구체적으로는 다음과 같은 사례가 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;버그를 찾기 위해 환경 변수를 바꿔 라이브러리 버전을 변경했을 때, 이를 다시 직접 바꾸지 않으면 계속 변경된 라이브러리로 소프트웨어를 개발하게 됩니다. 이를 강제로 되돌릴 방법도 없고, 버그가 생기거나 빌드가 되지 않을 때까지 알아채기도 쉽지 않습니다.&lt;/li&gt;
  &lt;li&gt;개발 환경을 설정하기가 굉장히 까다로웠습니다. 소프트웨어를 빌드하기 위해 설정해야 할 환경 변수가 많고, 의존 관계가 명확하지 않다 보니 라이브러리가 누락되거나 파일 한 개가 바뀌었을 때 이를 발견하기가 쉽지 않았습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이러한 문제들을 해결하기 위해서는 라이브러리의 버전과 의존 관계를 명확히 해야 하고 모든 개발자들이 통일된 개발 환경에서 제품을 만들 수 있게 강제하는 시스템이 필요했습니다. 이 문제를 해결할 방법을 찾던 중, 저희는 Nuget 패키지를 활용하기로 결정했습니다. 이번 글에서는 간단하게 Nuget 패키지를 만드는 방법과 저희 팀에서 Nuget을 도입하면서 얻게 된 장점을 함께 설명하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;nuget-저장소&quot;&gt;Nuget 저장소?&lt;/h2&gt;

&lt;p&gt;Nuget은 파이썬의 PIP나 자바스크립트의 NPM처럼 .NET 소프트웨어의 라이브러리를 쉽게 사용할 수 있게 만든 패키지 저장소입니다. C++를 직접적으로 지원하는 건 아니지만 Visual Studio IDE와 연동되는 장점이 있기 때문에 몇 가지만 설정하면 충분히 C++ 라이브러리 저장소로도 사용이 가능합니다. Boost와 같은 범용적인 라이브러리는 이미 Nuget 공개 저장소에 올라왔을 정도로, 많은 사람들이 사용하고 있기도 합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/boost_in_nuget.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/boost_in_nuget.png&quot; alt=&quot;Nuget 저장소에 있는 boost 라이브러리&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Nuget 저장소에 있는 boost 라이브러리&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;nuget-c-라이브러리-만들기&quot;&gt;Nuget C++ 라이브러리 만들기&lt;/h2&gt;

&lt;p&gt;라이브러리를 만드는 작업은 그리 어렵지 않습니다. OpenCV를 예로 들어 설명하겠습니다. 먼저 패키지 정의(.nuspec) 파일을 생성한 다음, OpenCV 빌드 결과물을 다음과 같이 형식에 맞게 지정하면 됩니다(파일 안에 있는 각각의 항목에 대한 자세한 설명은 &lt;a href=&quot;https://docs.microsoft.com/ko-kr/nuget/reference/nuspec&quot; target=&quot;_blank&quot;&gt;여기&lt;/a&gt;를 참고해주세요).&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;nt&quot;&gt;&amp;lt;package&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;xmlns=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;http://schemas.microsoft.com/packaging/2010/07/nuspec.xsd&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt; 
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;metadata&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;id&amp;gt;&lt;/span&gt;opencv-vc14-win64&lt;span class=&quot;nt&quot;&gt;&amp;lt;/id&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;3.4.1&lt;span class=&quot;nt&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;authors&amp;gt;&lt;/span&gt;SuaKIT Development Team&lt;span class=&quot;nt&quot;&gt;&amp;lt;/authors&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;owners&amp;gt;&lt;/span&gt;SuaKIT&lt;span class=&quot;nt&quot;&gt;&amp;lt;/owners&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;requireLicenseAcceptance&amp;gt;&lt;/span&gt;false&lt;span class=&quot;nt&quot;&gt;&amp;lt;/requireLicenseAcceptance&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;description&amp;gt;&lt;/span&gt;Canonical OpenCV library&lt;span class=&quot;nt&quot;&gt;&amp;lt;/description&amp;gt;&lt;/span&gt; 
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;/metadata&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;files&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;file&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;opencv-3.4.1-vc14-x64\x64\vc14\bin\*.dll&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;target=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lib&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;file&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;opencv-3.4.1-vc14-x64\x64\vc14\lib\*&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;target=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lib&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;file&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;opencv-3.4.1-vc14-x64\x64\vc14\bin\*.exe&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;target=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bin&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;file&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;opencv-3.4.1-vc14-x64\include\**&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;target=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;include&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;file&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;opencv-3.4.1-vc14-x64\etc\**&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;target=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;etc&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;file&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;opencv-3.4.1-vc14-x64\LICENSE&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;file&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;opencv-3.4.1-vc14-x64\OpenCVConfig.cmake&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;file&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;opencv-3.4.1-vc14-x64\OpenCVConfig-version.cmake&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/files&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/package&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;작성한 nuspec 파일을 사용 가능한 패키지(nupkg) 파일로 만드는 작업도 간단합니다. &lt;a href=&quot;https://www.nuget.org/downloads&quot; target=&quot;_blank&quot;&gt;Nuget 홈페이지&lt;/a&gt;에 있는 Nuget 바이너리 파일 하나만 받아, 다음과 같이 파워셸이나 명령 프롬프트에서 실행하면 파일이 생성됩니다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cmd&quot;&gt;nuget.exe pack opencv-vc14-win64.nuspec
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;종속성-관리&quot;&gt;종속성 관리&lt;/h2&gt;
&lt;p&gt;때로는 라이브러리를 사용하기 위해 다른 라이브러리의 설치를 강제할 필요가 있습니다. Nuget을 이용하면 이 의존관계를 쉽게 표현할 수 있는데, 예를 들어 앞서 작성했던 nuspec 파일에 다음 내용을 추가하면&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;nt&quot;&gt;&amp;lt;description&amp;gt;&lt;/span&gt;Canonical OpenCV library&lt;span class=&quot;nt&quot;&gt;&amp;lt;/description&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;dependencies&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;dependency&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;boost-vc16-win64static&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;version=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1.71.0&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/dependencies&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;/metadata&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;저장소에서 이 라이브러리를 설치하는 것 만으로 boost 라이브러리가 함께 설치됩니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/dependency_example.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/dependency_example.png&quot; alt=&quot;설정된 boost 라이브러리 종속성&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;설정된 boost 라이브러리 종속성&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;nuget-라이브러리-배포하기&quot;&gt;Nuget 라이브러리 배포하기&lt;/h2&gt;

&lt;p&gt;이제 라이브러리 파일을 배포해야 합니다. 저희가 라이브러리로 사용하는 파일 중 일부는 외부에 공개할 수 없었기 때문에 사내 저장소를 만들기로 했습니다.&lt;/p&gt;

&lt;p&gt;일반적으로 사설 저장소를 만들려면 많은 환경 설정과 서버가 필요한데, Nuget은 사설 저장소를 구축하는 비용 또한 저렴합니다. 그 이유는 디렉토리 기반 저장소를 지원하기 때문입니다.&lt;/p&gt;

&lt;p&gt;다음 그림을 보면, 로컬 디렉토리가 사설 저장소로 설정된 것을 볼 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/set_local_directory_as_private_repository.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/set_local_directory_as_private_repository.png&quot; alt=&quot;로컬 디렉토리를 사설 저장소로 설정&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;로컬 디렉토리를 사설 저장소로 설정&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;저희 개발 팀은 개발 팀에서 사용하는 네트워크 스토리지(NAS)를 사설 저장소로 사용하기로 하고, 네트워크 스토리지에 의존 관계에 있는 모든 라이브러리를 넣었습니다. 지금은 보다 많은 사람들이 이 라이브러리를 사용할 수 있게 사내 웹 서버에 올려 두었습니다.&lt;/p&gt;

&lt;h2 id=&quot;nuget-라이브러리-사용-시-주의해야-할-점&quot;&gt;Nuget 라이브러리 사용 시 주의해야 할 점&lt;/h2&gt;

&lt;p&gt;이렇게 라이브러리의 버전을 명시하고 의존 관계를 만든 후, 저장소를 만드는 것 까지는 좋았으나, Nuget 저장소 자체가 C++에 특화된 것이 아니기 때문에 해결해야 할 문제가 있었습니다.&lt;/p&gt;

&lt;p&gt;처음에는 컴파일에 사용할 include 파일을 찾을 수 없는 문제가 생겼습니다. 이 문제는 추가 포함 디렉토리에 패키지 경로를 지정하면 해결이 가능했습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/add_path_of_package.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/add_path_of_package.png&quot; alt=&quot;추가 포함 디렉토리에 지정된 패키지 경로&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;추가 포함 디렉토리에 지정된 패키지 경로&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;두 번째 문제는, 런타임 시 사용할 DLL이 빌드 결과물에 포함되지 않는다는 것이었습니다. 그래서 컴파일이 되도 실행은 되지 않는 문제가 발생했습니다. 그러나 이 문제 또한 빌드 이벤트에 DLL을 복사하게 설정하는 것으로 쉽게 해결하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/add_build_event.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/dependency-management-for-cpp-windows-app-using-nuget/add_build_event.png&quot; alt=&quot;빌드 이벤트에 DLL 복사를 설정&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;빌드 이벤트에 DLL 복사를 설정&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 해결해야 할 문제는 컴파일 환경입니다. 앞서 이야기한 것처럼 C++ 언어는 아키텍처, 지원 명령어 셋, 라이브러리 컴파일 옵션 등 함께 관리해야 할 여러 요소들이 많습니다. 이 모든 것들을 Nuget으로 해결할 수는 없습니다. 저희 팀은 이를 보완하기 위해, 패키지 이름에 다음 내용을 반드시 명시하는 관습을 따르고 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;아키텍처(x64)와 빌드 환경(vc15, vc16 등)&lt;/li&gt;
  &lt;li&gt;사용 시 주의해야 할 컴파일 옵션 (static 빌드, 특정 옵션 제거 등)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;마무리&quot;&gt;마무리&lt;/h2&gt;

&lt;p&gt;이렇게 필요한 라이브러리들을 모두 옮기니, 저희 팀은 다음과 같은 문제들을 해결할 수 있었습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;프로젝트 빌드에 필요한 라이브러리는 모두 Nuget 저장소에 있고, 저장소 경로와 Include, Linking 경로도 모두 프로젝트에 포함되어 있습니다. 빌드 버튼을 누르면 자동으로 저장소에서 패키지를 받고 의존성 목록에 따라 설치합니다. 결과적으로는 소스코드만 받아도 쉽게 빌드 환경을 구성할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;제품 개발에 참여하는 모든 개발자가 프로젝트와 Nuget 저장소에 의해 고정된 라이브러리 파일을 사용하기 때문에 같은 개발 환경을 보장할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;라이브러리 버전을 보다 쉽게 확인할 수 있게 됐습니다. 저희 팀은 GIT을 사용하고 있고, 프로젝트 파일에 포함된 Include와 Linking 경로들 또한 git에 의해 추적되기 때문에 커밋하기 전에 사용중인 라이브러리 상태를 다시 한번 확인하고 검증할 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;개발 팀에서 윈도우 기반의 C++ 개발 환경을 구축하는 일은 쉽지 않습니다. 리눅스의 경우 APT, YUM/DNF와 같은 저장소를 사용할 수 있지만, 윈도우는 아직까지 표준이라고 할 패키징 관리 시스템이 없기 때문입니다. 그러나 Nuget을 이용하면 적은 리소스로 리눅스 못지않게 라이브러리의 목록과 버전을 관리할 수 있습니다.&lt;/p&gt;</content><author><name>이기곤</name><email>Gigone.Lee@cognex.com</email></author><category term="Development" /><category term="c++" /><category term="nuget" /><summary type="html">라이브러리를 사용할 때는 프로젝트가 사용하는 라이브러리 목록과 버전을 명확히 관리해야 합니다. 개발 환경과 배포 환경의 라이브러리 버전 차이가 의도치 않은 동작을 만들 수 있기 때문입니다. 또한 모든 팀원이 같은 라이브러리 버전을 사용해야 팀원 개개인이 동일한 동작을 기대하고 기능을 개발할 수 있습니다.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/boost_in_nuget.png" /><media:content medium="image" url="http://github.sualab.io/boost_in_nuget.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Boost Interprocess 자원 지속시간(Lifetime) 테스트</title><link href="http://github.sualab.io/development/2020/04/06/Lifetime-of-Boost-Interprocess-Mechanisms.html" rel="alternate" type="text/html" title="Boost Interprocess 자원 지속시간(Lifetime) 테스트" /><published>2020-04-06T23:00:00+09:00</published><updated>2020-04-06T23:00:00+09:00</updated><id>http://github.sualab.io/development/2020/04/06/Lifetime-of-Boost-Interprocess-Mechanisms</id><content type="html" xml:base="http://github.sualab.io/development/2020/04/06/Lifetime-of-Boost-Interprocess-Mechanisms.html">&lt;p&gt;이번 포스트에서는 Boost의 Interproces에 포함된 자원들의 지속시간(Lifetime)을 테스트해본 경험을 공유하려합니다. 이런 테스트를 해 본 계기는 개발 과정에서 프로그램이 비정상종료는 경우, 혹은 디버거를 이용한 디버깅 도중 자원을 해제하는 부분에 도달하기 전에 프로세스가 종료되는 경우 자원이 해제되지 않아(혹은, shared lock 같은 경우 unlock이 되지 않아) 문제가 발생하는 상황을 없애고 싶다는 요구에서였습니다. 이것을 위해선 프로세스가 종료될 경우 자동으로 해제되는 자원이 필요했습니다. Boost의 Interprocess 라이브러리에서 제공하는 자원들 중에서 이런 요구사항에 부합하는 자원을 찾기 위해 진행한 테스트들을 정리해보았습니다.&lt;/p&gt;

&lt;h2 id=&quot;서론-boost-interprocess-사용시-의문점&quot;&gt;서론: Boost Interprocess 사용시 의문점&lt;/h2&gt;

&lt;p&gt;C++ 프로그램에서 프로세스 간 통신(IPC, Inter-Process Communication)을 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;boost::interprocess&lt;/code&gt;에서 제공하는 객체(Object)들을 사용하게 되었습니다. 프로세스 간 공유 메모리(Inter-Process shared memory)나 프로세스 간 뮤텍스(Inter-Process Mutex)를 사용하려고 하다보니, 자연스럽게 다음과 같은 의문점이 생겼습니다.&lt;/p&gt;

&lt;p&gt;“어떤 프로세스가 내부적으로 할당하여 그 프로세스만 사용하게 될 자원들은 그 프로세스가 종료되는 순간 자동으로 반환된다. 명시적으로 자원을 반환하는 코드가 없더라도 운영체제 수준에서 이것을 처리한다. 그렇다면, 프로세스 간 통신을 위해 공유 메모리에 할당된 자원은 언제 반환되는가? 다시 말해서, 공유 메모리에 생성된 객체의 인스턴스는 언제 해제되는가(destructed)?”&lt;/p&gt;

&lt;p&gt;자원을 명시적으로 해제할 경우 당연히 그 자원은 반환될 것입니다. 그러나 자원을 명시적으로 해제하지 않는다면 어떻게 될까요? 시스템을 재부팅해야 자원이 해제가 될까요? 아니면 재부팅하지 않더라도 그 자원을 사용했던 프로세스들이 종료되면 자동으로 해제가 될까요? &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;boost::interprocess&lt;/code&gt; 관련 문서를 뒤적인 결과 다음과 같은 &lt;a href=&quot;https://www.boost.org/doc/libs/1_71_0/doc/html/interprocess/some_basic_explanations.html#interprocess.some_basic_explanations.persistence&quot; target=&quot;_blank&quot;&gt;항목&lt;/a&gt;을 발견할 수 있었습니다. 원문은 영어로 되어있고 의역하면 다음과 같습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;프로세스 간 통신에 사용되는 자원들은 언제까지 지속되는가&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;프로세스 간 통신에 사용되는 자원들에 대한 중요한 이슈 중 하나는 그 자원들의 지속시간이다. 어떤 자원이 언제 시스템에서 사라지는지를 아는 것은 매우 중요하다. Boost.Interprocess에서 제공하는 자원들은 다음과 같이 세 가지 경우로 나뉜다:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;프로세스 수준 지속: 어떤 공유 자원을 사용(open)한 모든 프로세스들이 그 자원을 더 이상 사용하지 않게 되거나(close) 정상종료(exit)되거나 비정상종료(crash)될 때까지 유지된다.&lt;/li&gt;
    &lt;li&gt;커널 수준 지속: 운영체제의 커널이 재시작(reboot)되거나 자원이 명시적으로 삭제(delete)될 때까지 유지된다.&lt;/li&gt;
    &lt;li&gt;파일시스템 수준 지속: 자원이 명시적으로 삭제(delete)될 때까지 유지된다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;각각의 수준에 대해 매우 잘 정리되어 있음을 알 수 있습니다. 위의 내용을 토대로 제게 필요했던 기능은 &lt;strong&gt;프로세스 수준 지속&lt;/strong&gt;을 지원하는 자원이라는 것도 명확하게 판단할 수 있었습니다. 하지만 문제는, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;boost::interprocess&lt;/code&gt;가 제공하는 자원들이 위 세가지 중 어디에 속하는지가 명확히 정리되어 있지 않았다는 점입니다. 그래서 테스트 코드를 직접 짜 보면서 실험을 해보기로 했습니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;boost::interprocess&lt;/code&gt;에서 제공하는 모든 종류의 자원을 테스트해보지는 못했고, 제가 확인이 필요했던 4가지 자원들에대해 테스트해보았습니다. 이 포스트에서 테스트해보지 못한 다른 종류의 자원들도 이 포스트에서 소개하는 방식과 동일하게 테스트해볼 수 있을 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;boost-interprocess-자원별-지속시간-테스트-및-결과&quot;&gt;Boost Interprocess 자원별 지속시간 테스트 및 결과&lt;/h2&gt;

&lt;h3 id=&quot;managed_xsi_shared_memory--커널-수준-지속&quot;&gt;managed_xsi_shared_memory : 커널 수준 지속&lt;/h3&gt;

&lt;p&gt;이 공유 메모리는 리눅스에서만 사용할 수 있습니다. 리눅스에서 진행되었던 모든 테스트는 Ubuntu 18.04(g++ 7.5.0), x86_64 환경에서 진행되었습니다. Boost 버전은 1.71.0 을 사용하였고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local/boost-1.71.0&lt;/code&gt;에 설치되었다고 가정하였습니다.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/nglee/5892c346d26f9282160581e7fde4d3cf.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;위 테스트코드에서는 공유 자원을 해제하는 과정을 고의적으로 생략하였습니다. 자원을 명시적으로 해제하지 않고 프로세스를 종료시켰을 때 자원이 자동으로 해제되는지(즉, &lt;strong&gt;프로세스 수준 지속&lt;/strong&gt;인지) 확인해보고 싶은 것입니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ g++ bitest1.cpp -I/usr/local/boost-1.71.0/include -std=c++14 -pthread -o bitest1
$ ./bitest1
shared memory created
$ ./bitest1
File exists
shared memory creation failed, opened instead
$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위와 같이 프로세스가 종료되었음에도 자원이 해제되지 않는 것을 보니 일단 프로세스 수준 지속은 아님을 알 수 있습니다. 그런데 이 상태로 재부팅하고 나면 다음과 같음을 볼 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ./bitest1
shared memory created
$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;따라서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;managed_xsi_shared_memory&lt;/code&gt; 인스턴스의 경우 커널이 재시작 하고 나서 자원이 해재된 것을 보니 &lt;strong&gt;커널 수준 지속&lt;/strong&gt;임을 알 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;managed_windows_shared_memory--프로세스-수준-지속&quot;&gt;managed_windows_shared_memory : 프로세스 수준 지속&lt;/h3&gt;

&lt;p&gt;이 공유 메모리는 윈도우에서만 사용할 수 있습니다. 다음과 같은 코드로 컴파일 후 연속적으로 실행하면 항상 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shared memory created&lt;/code&gt;가 출력됩니다. 따라서 &lt;strong&gt;프로세스 수준 지속&lt;/strong&gt;임을 알 수 있습니다. (테스트 환경: Windows 10, Visual Studio 2019, NuGet 패키지 인스톨러 이용하여 boost 1.71.0 버전 설치)&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/nglee/f89b6448a05edcb9ed382a94e8aa13d5.js&quot;&gt;&lt;/script&gt;

&lt;h3 id=&quot;interprocess_mutex--커널-수준-지속-managed_xsi_shared_memory에-할당된-경우&quot;&gt;interprocess_mutex : 커널 수준 지속 (managed_xsi_shared_memory에 할당된 경우)&lt;/h3&gt;

&lt;script src=&quot;https://gist.github.com/nglee/a03ed8c2de387608f6d9d98ce53beb51.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ g++ bitest2.cpp -I/usr/local/boost-1.71.0/include -std=c++14 -pthread -o bitest2
$ ./bitest2
interprocess_mutex create success
$ ./bitest2
boost::interprocess_exception::library_error
interprocess_mutex create fail
$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위와 같이 프로세스가 종료되어도 공유 메모리에 할당된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interprocess_mutex&lt;/code&gt;가 자동으로 해제되지 않는 것을 보니 프로세스 수준 지속은 아님을 알 수 있습니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interprocess_mutex&lt;/code&gt;가 할당된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;managed_xsi_shared_memory&lt;/code&gt;가 &lt;strong&gt;커널 수준 지속&lt;/strong&gt;이므로 이 경우 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interprocess_mutex&lt;/code&gt;도 &lt;strong&gt;커널 수준 지속&lt;/strong&gt;임을 알 수 있습니다. (재부팅시에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;managed_xsi_shared_memory&lt;/code&gt;가 사라지므로 그 안에 할당된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interprocess_mutex&lt;/code&gt;도 사라지게 됨)&lt;/p&gt;

&lt;p&gt;뮤텍스의 경우 한 가지 더 궁금한 점이 생깁니다. 비록 자원은 해제되지 않더라도, 혹시 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unlock&lt;/code&gt;은 자동으로 해줄까요? 다시 말해, 어떤 프로세스가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interprocess_mutex&lt;/code&gt;를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock&lt;/code&gt;하고 나서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unlock&lt;/code&gt;하지 않고 프로세스가 종료된다면, 다른 프로세스가 그 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interprocess_mutex&lt;/code&gt;를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock&lt;/code&gt;할 수 있을까요?&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/nglee/b9dd4d8f55010d47a48f30f4967b86d1.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ g++ bitest3.cpp -I/usr/local/boost-1.71.0/include -std=c++14 -pthread -o bitest3
$ ./bitest3
This is the first process.
The first process locked an interprocess_mutex
and is going to exit without unlocking the mutex
$ ./bitest3
This is the second process.
The second process is going to lock an interprocess mutex,
and if it succeeds, then a message will be shown.
If it fails, it will run indefinitely.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위의 예제에서 볼 수 있듯이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unlock&lt;/code&gt;이 자동으로 되지 않음을 알 수 있습니다. 이런 상황을 막기 위해서는 다음과 같이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scoped_lock&lt;/code&gt;을 사용해야 합니다.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/nglee/a6eb842a92f1b44ca3d3880b255aac80.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ g++ bitest4.cpp -I/usr/local/boost-1.71.0/include -std=c++14 -pthread -o bitest4
$ ./bitest4
This is the first process.
The first process locked an interprocess_mutex
and is going to exit without unlocking the mutex
$ ./bitest4
This is the second process.
The second process is going to lock an interprocess mutex,
and if it succeeds, then a message will be shown.
If it fails, it will run indefinitely.
The second process locked the mutex!
$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;하지만 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scoped_lock&lt;/code&gt;도 완벽한 해답이 되지는 못합니다. 만약 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scoped_lock&lt;/code&gt;의 인스턴스가 해제되기 전에 프로세스가 종료된다면, 결국 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unlock&lt;/code&gt;이 되지 않는 것이나 마찬가지입니다. 다음은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gdb&lt;/code&gt;를 이용해서 인위적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scoped_lock&lt;/code&gt;이 해제되기 전에 프로세스를 종료시킨 후에 다른 프로세스에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock&lt;/code&gt;을 시도하는 예제입니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ g++ bitest4.cpp -I/usr/local/boost-1.71.0/include -std=c++14 -pthread -o bitest4d -g
$ gdb ./bitest4d
(gdb) break 30
(gdb) run
This is the first process.

Breakpoint 1, first_process (shm=0x638c20) at bitest4.cpp:30
30	    scoped_lock&amp;lt;interprocess_mutex&amp;gt; lock(*mtx);
(gdb) n
32	    std::cout &amp;lt;&amp;lt; &quot;The first process locked an interprocess_mutex&quot; &amp;lt;&amp;lt; std::endl;
(gdb) kill
Kill the program being debugged? (y or n) y
(gdb) run
This is the second process.
The second process is going to lock an interprocess mutex,
and if it succeeds, then a message will be shown.
If it fails, it will run indefinitely.
^C
Program received signal SIGINT, Interrupt.
__lll_lock_wait () at ../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:135
135	../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S: No such file or directory.
(gdb)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위에서 볼 수 있듯이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unlock&lt;/code&gt;이 자동으로 되지 않음을 알 수 있습니다. 이는 디버깅 과정에서 문제가 될 수 있습니다. 이럴 때 사용할 수 있는 것이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;file_lock&lt;/code&gt;입니다.&lt;/p&gt;

&lt;h3 id=&quot;file_lock--프로세스-수준-지속&quot;&gt;file_lock : 프로세스 수준 지속&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;file_lock&lt;/code&gt;은 &lt;a href=&quot;https://www.boost.org/doc/libs/1_71_0/doc/html/interprocess/synchronization_mechanisms.html#interprocess.synchronization_mechanisms.file_lock&quot; target=&quot;_blank&quot;&gt;문서&lt;/a&gt;에도 명시되어 있듯이 &lt;strong&gt;프로세스 수준 지속&lt;/strong&gt;입니다. 따라서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;file_lock&lt;/code&gt;의 장점은 무엇보다 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock&lt;/code&gt;을 한 프로세스가 명시적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unlock&lt;/code&gt;을 하지 않은 상태로 종료되더라도(정상종료이든, 비정상종료이든 관계 없이) 자동으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unlock&lt;/code&gt;이 된다는 점입니다.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/nglee/164bde324c6c03a50047648c222d0b28.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ g++ bitest5.cpp -I/usr/local/boost-1.71.0/include -std=c++14 -pthread -o bitest5
$ ./bitest5
The process locked an file_lock,
and is going to exit without unlocking the lock.
$ ./bitest5
The process locked an file_lock,
and is going to exit without unlocking the lock.
$ g++ bitest5.cpp -I/usr/local/boost-1.71.0/include -std=c++14 -pthread -o bitest5d -g
$ gdb ./bitest5d
(gdb) break 11
(gdb) run
The process locked an file_lock,

Breakpoint 1, main() at bitest5.cpp:11
11          std::cout &amp;lt;&amp;lt; &quot;and is going to exit without unlocking the lock.&quot; &amp;lt;&amp;lt; std::endl;
(gdb) kill
Kill the program being debugged? (y or n) y
(gdb) disable break
(gdb) run
The process locked an file_lock,
and is going to exit without unlocking the lock.
(gdb)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;결론&quot;&gt;결론&lt;/h2&gt;

&lt;p&gt;여기까지 C++ 프로세스 통신간 사용하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Boost::interprocess&lt;/code&gt;에서 제공하는 자원 중 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;managed_xsi_shared_memory&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;managed_windows_shared_memory&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interprocess_mutex&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;file_lock&lt;/code&gt; 에 대해 지속 시간을 테스트 해보았습니다. 각 자원에 대해 조금씩 다른 테스트 방법을 사용하여 각각 커널 수준, 프로세스 수준, 커널 수준, 프로세스 수준 지속임을 확인하였습니다. 앞서도 말씀드렸지만 본 글에서 테스트 해본 것들 이외의 자원에 대해서도 테스트를 해보고 싶다면, 이를 참고하여 비슷한 방식으로 테스트 할 수 있을 것이라 생각합니다.&lt;/p&gt;</content><author><name>이남구</name><email>Namgoo.Lee@cognex.com</email></author><category term="Development" /><category term="boost" /><category term="c++" /><category term="gdb" /><summary type="html">이번 포스트에서는 Boost의 Interproces에 포함된 자원들의 지속시간(Lifetime)을 테스트해본 경험을 공유하려합니다. 이런 테스트를 해 본 계기는 개발 과정에서 프로그램이 비정상종료는 경우, 혹은 디버거를 이용한 디버깅 도중 자원을 해제하는 부분에 도달하기 전에 프로세스가 종료되는 경우 자원이 해제되지 않아(혹은, shared lock 같은 경우 unlock이 되지 않아) 문제가 발생하는 상황을 없애고 싶다는 요구에서였습니다. 이것을 위해선 프로세스가 종료될 경우 자동으로 해제되는 자원이 필요했습니다. Boost의 Interprocess 라이브러리에서 제공하는 자원들 중에서 이런 요구사항에 부합하는 자원을 찾기 위해 진행한 테스트들을 정리해보았습니다.</summary></entry><entry><title type="html">Anomaly Detection 개요： (2) Out-of-distribution(OOD) Detection 문제 소개 및 핵심 논문 리뷰</title><link href="http://github.sualab.io/introduction/review/2020/02/20/anomaly-detection-overview-2.html" rel="alternate" type="text/html" title="Anomaly Detection 개요： (2) Out-of-distribution(OOD) Detection 문제 소개 및 핵심 논문 리뷰" /><published>2020-02-20T12:00:00+09:00</published><updated>2020-02-20T12:00:00+09:00</updated><id>http://github.sualab.io/introduction/review/2020/02/20/anomaly-detection-overview-2</id><content type="html" xml:base="http://github.sualab.io/introduction/review/2020/02/20/anomaly-detection-overview-2.html">&lt;p&gt;안녕하세요, 이번 포스팅에서는 지난 포스팅에 이어 Anomaly Detection(이상 탐지)에 대한 내용을 다룰 예정이며 Anomaly Detection 연구 분야 중 Out-of-distribution(OOD) Detection 문제에 대해 여러 논문을 토대로 깊이 있게 소개를 드릴 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;out-of-distributionood-detection-이란&quot;&gt;Out-of-distribution(OOD) Detection 이란?&lt;/h2&gt;
&lt;p&gt;이전 포스팅인 &lt;a href=&quot;http://research.sualab.com/introduction/review/2020/01/30/anomaly-detection-overview-1.html&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; ” Anomaly Detection 개요: (1) 이상치 탐지 분야에 대한 소개 및 주요 문제와 핵심 용어, 산업 현장 적용 사례 정리” &lt;/b&gt;&lt;/a&gt; 에서 잠시 언급했던 Out-of-distribution(OOD) Detection은 현재 보유하고 있는 In-distribution 데이터 셋을 이용하여 multi-class classification network를 학습시킨 뒤, test 단계에서 In-distribution test set은 정확하게 예측하고 Out-of-distribution 데이터 셋은 걸러내는 것을 목표로 하고 있다고 말씀드렸습니다.&lt;/p&gt;

&lt;p&gt;하지만 기존 classification network는 분류를 위해 feature extractor 마지막 부분에 softmax를 붙여서 사용하는데, softmax 계산 식이 exponential 함수를 이용하기 때문에 예측 확률이 거의 1에 근접하는 &lt;strong&gt;high-confidence&lt;/strong&gt; 예측이 자주 관찰됩니다. In-distribution 데이터 셋을 정확하게 high-confidence로 예측하는 것은 큰 문제가 되지 않습니다. 다만 Out-of-distribution 데이터 셋을 test 단계에 넣어주는 경우, 이상적인 결과는 어떠한 class로도 예측되지 않도록 각 class를 (1/class 개수) 확률로 uniform 하게 예측하는 것입니다. 하지만 대부분의 경우 Out-of-distribution 데이터 셋을 test 단계에 넣어주면 high-confidence 예측이 관찰되기 때문에 단순한 방법으로는 걸러 내기 힘든 문제가 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/high-confidence.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/high-confidence.PNG&quot; alt=&quot;High-confidence prediction을 보여주는 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;High-confidence prediction을 보여주는 예시&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 유저가 직접 마우스로 input을 주면 CNN(Convolutional Neural Network)와 MLP(Multi-Layer Perceptron)를 통해 0부터 9까지 숫자를 예측하는 과정을 데모로 구현한 &lt;a href=&quot;https://mnist-demo.herokuapp.com/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; MNIST Web Demo &lt;/b&gt;&lt;/a&gt; 페이지에서, 숫자 대신 전혀 다른 sample인 미소 짓는 얼굴을 Out-of-distribution sample로 가정하고 입력한 예시입니다. 0 ~ 9 중 어느 class에도 속하지 않기 때문에 이상적인 경우에는 어떠한 class로도 예측되지 않아야 하지만 CNN의 경우는 매우 높은 확률로 3이라는 class로, MLP의 경우 0이라는 class에 가장 높은 확률, 5라는 class에 2번째로 높은 확률 값으로 예측하는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Classifier를 학습시킬 때 0~9에 속하지 않는 sample들을 모아서 &lt;strong&gt;Unknown&lt;/strong&gt; 이라는 11번째 class로 가정하여 11-way classification으로 문제를 바꿔서 학습시키는 것을 가장 먼저 생각해볼 수 있습니다. 다만 이러한 경우 이미 학습된 망을 다시 학습하여야 하는 문제와, Unknown sample을 다양하게 취득하여야 하는 한계가 존재합니다.&lt;/p&gt;

&lt;p&gt;이러한 문제점에 집중해서 Unknown class를 추가하는 단순한 방법 대신 보다 더 효율적이면서 효과적인 방법을 제시한 논문들을 시간 순으로 소개를 드리겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;out-of-distribution-detection을-다룬-논문들-소개&quot;&gt;Out-of-distribution Detection을 다룬 논문들 소개&lt;/h2&gt;
&lt;p&gt;이번 장에서는 Out-of-distribution Detection을 다룬 여러 논문들을 핵심 아이디어 위주로 설명을 드릴 예정이며 가장 최초의 논문부터 글을 작성하고 있는 시점의 최신 논문까지 소개를 드릴 예정입니다.&lt;/p&gt;

&lt;h3 id=&quot;---a-baseline-for-detecting-misclassified-and-out-of-distribution-examples-in-neural-networks-2017-iclr-&quot;&gt;- &lt;a href=&quot;https://arxiv.org/pdf/1610.02136.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks”, 2017 ICLR &lt;/b&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Out-of-distribution Detection을 다룬 최초의 논문이며, Out-of-distribution Detection에 대한 문제 정의와 평가 방법 등을 제시하였고, 간단한 &lt;strong&gt;abnormality module&lt;/strong&gt; 구조를 제안하였고 약간의 성능 향상을 달성한 내용을 논문에서 언급하고 있습니다.&lt;/p&gt;

&lt;p&gt;본 논문에서는 Computer Vision과 Natural Language Processing, Automatic Speech Recognition에 대해 실험을 하였지만 이번 포스팅에서는 Computer Vision에 대해서만 다룰 예정입니다. 실험에는 In-distribution 데이터 셋으로는 MNIST, CIFAR-10, CIFAR-100을 이용하였고 Out-of-distribution 데이터 셋으로는 Scene Understanding(SUN) 데이터 셋, Omniglot 데이터 셋,  Synthetic 데이터인 Gaussian Random Noise 데이터 셋, Uniform Random Noise 데이터 셋 등을 이용하였습니다.&lt;/p&gt;

&lt;p&gt;In-distribution 데이터 셋으로 network를 학습을 시킨 뒤, test 단계에서 in-distribution 데이터 셋과 out-of-distribution 데이터 셋을 test set으로 사용을 하게 됩니다. 당연한 이야기지만 학습에는 in-distribution 데이터 셋만 사용을 하게 됩니다. 학습이 끝난 뒤 network에 test set을 넣어주면 class 개수만큼 softmax 값이 계산이 되는데, 이 중 가장 큰 값(Maximum Softmax Probability)을 사용하여 Out-of-distribution detection에 활용할 수 있습니다. 모종의 threshold 값을 정해두고, Maximum Softmax Probability 값이 threshold보다 크면 in-distribution, 작으면 out-of-distribution sample이라고 분류할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/auroc_aupr.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/auroc_aupr.PNG&quot; alt=&quot;AUROC vs AUPR&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;AUROC vs AUPR&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;성능 평가를 위한 metric으로는 threshold 값에 무관한 지표인 &lt;strong&gt;AUROC&lt;/strong&gt;와 &lt;strong&gt;AUPR&lt;/strong&gt;을 사용하였습니다. AUROC는 ROC 커브의 면적을 통해 계산할 수 있으며 ROC 커브는 positive class와 negative class의 sample 개수가 다른 경우에 이를 반영하지 못하기 때문에 추가로 Precision-Recall 커브의 면적인 &lt;strong&gt;AUPR&lt;/strong&gt; 도 사용하였습니다. AUPR을 계산할 때에는 In-distribution 데이터 셋을 Positive로 간주하였을 때의 값인 &lt;strong&gt;AUPR In&lt;/strong&gt; 과 Out-of-distribution 데이터 셋을 Positive로 간주하였을 때의 값인 &lt;strong&gt;AUPR Out&lt;/strong&gt; 2가지 값을 각각 계산합니다.&lt;/p&gt;

&lt;p&gt;마지막으로 out-of-distribution 데이터 셋으로 test를 하였을 때의 각 sample 별 예측한 class의 확률 값들을 평균 낸 지표인 &lt;strong&gt;Pred. Prob(mean)&lt;/strong&gt; 지표도 제안하였습니다. 이 지표가 높다는 것은 모델이 out-of-distribution 데이터 셋을 high-confidence로 잘못 예측하고 있음을 시사합니다. 다만 이 지표는 후속 논문 들에서는 사용되지 않습니다.&lt;/p&gt;

&lt;p&gt;본 논문에서는 Out-of-distribution Detection 문제에 대한 Baseline을 잘 정립해주었고, 실험 프로토콜과 3가지 평가 지표를 제안하였다는 점에서 큰 의미를 가지고 있다고 생각합니다.&lt;/p&gt;

&lt;h3 id=&quot;---enhancing-the-reliability-of-out-of-distribution-image-detection-in-neural-networks-2018-iclr-&quot;&gt;- &lt;a href=&quot;https://arxiv.org/pdf/1706.02690.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks”, 2018 ICLR &lt;/b&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;2018년 ICLR에 발표된 논문이며 ODIN(Out-of-DIstribution detector for Neural networks)이라는 기억하기 쉬운 방법론 이름을 제안하여 유명세를 얻은 논문입니다. 이미 학습이 끝난 neural network에 어떠한 추가 학습 없이 간단하면서 효과적으로 out-of-distribution sample들을 찾아낼 수 있다는 점이 가장 큰 장점이며 실제로도 간단히 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;핵심 아이디어는 temperature scaling과 input preprocessing 2가지이며, 각각은 새로 고안해낸 것이 아니라 기존 유명한 논문 들에서 제안된 방법을 적절히 가져와서 사용했다는 점이 인상 깊으며 각각에 대해 자세히 설명을 드리겠습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/odin_key.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/odin_key.PNG&quot; alt=&quot;ODIN 핵심 방법론&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ODIN 핵심 방법론&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;우선 Temperature Scaling은 &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Hinton 교수의 Knowledge Distillation &lt;/b&gt;&lt;/a&gt;에서 제안된 방법이며 &lt;a href=&quot;https://arxiv.org/pdf/1706.04599.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; classification task에서 prediction confidence를 calibration&lt;/b&gt;&lt;/a&gt; 하는데 사용이 되기도 합니다. 학습 중에는 temperature scaling parameter \(T\) 를 1로 두고 학습을 진행한 뒤, test 단계에서 주어진 input의 예측 확률을 구하기 위해 softmax score를 구하게 되는데 이때 각 logit들을 \(T\) 로 나눠주는 방식으로 구현이 가능합니다. 이를 Out-of-distribution Detection 문제에 적용하면 in-distribution sample과 out-of-distribution sample의 softmax score를 서로 더 멀어지게 하여 out-of-distribution sample을 구별하기 쉽게 도와주는 역할을 합니다. 여기서 temperature scaling parameter \(T\) 가 hyper-parameter이며 적절한 값을 선택하는 것이 중요합니다.&lt;/p&gt;

&lt;p&gt;Input Preprocessing은 adversarial attack의 시초가 된 논문에서 제안한 &lt;a href=&quot; https://arxiv.org/pdf/1412.6572.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Fast Gradient Sign Method(FGSM) &lt;/b&gt;&lt;/a&gt; 방식에서 아이디어를 얻었으며, Back Propagation을 통해 loss를 최소화하도록 학습하는 것을 반대로 이용하여, loss를 증가시키는 방향의 gradient를 계산하여 얻은 극소량의 perturbation을 input에 더해 줌으로써 true label에 대한 softmax score를 낮춰주는 역할을 하는 것이 증명이 된 바 있습니다. 본 논문에서는 이를 역으로 이용하여 극소량의 perturbation을 input에서 빼 줌으로써 주어진 input에 대한 softmax score를 높여주는 것을 목표로 하고, 이를 통해 in-distribution sample에 대한 예측을 강화하여 out-of-distribution sample과 더 잘 분리될 수 있도록 도와주는 역할을 합니다. 여기서 perturbation magnitude parameter \(\epsilon\) 이 hyper-parameter이며 적절한 값을 선택하는 것이 중요합니다.&lt;/p&gt;

&lt;p&gt;위의 2가지 과정을 거쳐서 얻어진 maximum softmax score를 기반으로 단순히 thresholding을 거쳐 in-distribution sample 인지 out-of-distribution sample 인지를 판단하게 되며, 이때 사용하는 threshold 값 또한 hyper-parameter입니다. 즉 총 3개의 hyper-parameter가 전반적인 성능에 관여하게 됩니다.&lt;/p&gt;

&lt;p&gt;본 논문에선 in-distribution sample의 TPR(True Positive Rate)가 95%가 되는 시점에서의 hyper-parameter를 선택하였고, 실험을 통해 최적 값으로 발견해낸 \(T\) =1000, \(\epsilon\) =0.0012 값으로 고정하여 성능을 측정하였습니다. 각 hyper-parameter의 변화에 따른 성능 변화 양상은 논문에서 자세히 확인하실 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/odin_result.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/odin_result.PNG&quot; alt=&quot;ODIN 실험 결과 표&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ODIN 실험 결과 표&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;위의 표는 ODIN 논문에서 Baseline 논문과 성능을 비교한 결과이며, 성능 평가 지표로는 baseline과 동일하게 AUROC, AUPR을 사용하였고, 추가로 FPR at 95% TPR 지표와 Detection Error 지표를 사용하였습니다.&lt;/p&gt;

&lt;p&gt;FPR at 95% TPR은 말 그대로 TPR이 95%가 되는 시점에서의 FPR이며 FPR은 1-TNR로 계산이 가능하기 때문에 후속 논문들 에서는 TNR at 95% TPR 지표로 사용이 되기도 합니다.&lt;/p&gt;

&lt;p&gt;Detection Error는 test set에 positive sample과 negative sample 이 같은 비율로 존재한다는 가정하에 0.5(1-TPR) + 0.5FPR로 계산이 됩니다. 다만 이 지표는 개인적으로는 유의미한 정보를 담고 있지 않다고 생각이 되며 최신 논문들에서는 사용이 되지 않는 지표입니다.&lt;/p&gt;

&lt;p&gt;Baseline을 큰 격차로 따돌리며 Out-of-distribution 연구의 성장 가능성을 보여준 논문이며 추후에도 계속 언급이 될 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;---training-confidence-calibrated-classifiers-for-detecting-out-of-distribution-samples-2018-iclr-&quot;&gt;- &lt;a href=&quot;https://arxiv.org/pdf/1610.02136.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples”, 2018 ICLR &lt;/b&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;ODIN 논문이 나온 지 얼마 지나지 않아 공개된 논문이며 ODIN 논문과 마찬가지로 2018년 ICLR에 발표가 된 논문입니다.&lt;/p&gt;

&lt;p&gt;기존 방법들은 학습이 끝난 뒤 inference 단계에만 집중하고 있다면, 본 논문에서는 Out-of-distribution Detection을 잘하기 위한 training 방법을 제안하고 있는 점이 가장 큰 특징입니다.&lt;/p&gt;

&lt;p&gt;총 2가지 방법을 제안하였는데, 첫 번째는 학습 시에 out-of-distribution sample이 덜 confident 하도록 기존 cross-entropy loss에 &lt;strong&gt;confidence loss&lt;/strong&gt; 라는 loss term을 추가하는 방법을 제안하였습니다. Out-of-distribution sample의 예측 distribution이 uniform distribution과 같아지도록 하기 위해 둘 간의 KL-divergence를 구한 뒤, 이를 최소화하도록 loss function에 녹여냈다고 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;다음은 confidence loss를 사용할 때, in-distribution sample 근처의 out-of-distribution sample들을 이용하면 더 decision boundary를 잘 얻을 수 있음을 보여주기 위한 toy-example 실험을 수행한 뒤, 이를 바탕으로 새로운 loss term을 추가하는 방법을 제안하고 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/cl_boundary.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/cl_boundary.PNG&quot; alt=&quot;유의미한 Out-of-distribution sample을 이용하면 얻을 수 있는 효과&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;유의미한 Out-of-distribution sample을 이용하면 얻을 수 있는 효과&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;위의 그림에서 빨간 네모와 파란 동그라미는 in-distribution sample이고, 초록색 별은 가상으로 생성해낸 out-of-distribution sample을 의미합니다. 그 뒤 confidence loss를 추가하여 classifier를 학습시켰을 때의 decision boundary가 (b)와 (d)에 해당합니다. 아무 데나 흩뿌려 놓은 (a)보다 비교적 in-distribution sample 근처에 생성한 (c)를 이용하여 학습을 시킬 때 더 decision boundary를 잘 얻을 수 있음을 보여주고 있습니다. 다만 in-distribution 근처의 out-of-distribution sample을 취득하는 것이 어렵기 때문에 본 논문에서는 Generative Adversarial Network(GAN)을 이용한 &lt;strong&gt;GAN loss&lt;/strong&gt;를 추가하는 방향으로 접근하였습니다. 즉, in-distribution sample과 유사한 out-of-distribution sample을 다른 데이터 셋으로부터 가져오는 대신, GAN을 통해 in-distribution sample과 유사한 out-of-distribution sample을 생성하는 방법을 제안하고 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/cl_loss.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/cl_loss.PNG&quot; alt=&quot;본 논문에서 제안하는 loss function&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;본 논문에서 제안하는 loss function&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;결과적으로 기존 cross-entropy loss term에 &lt;strong&gt;confidence loss&lt;/strong&gt;와 &lt;strong&gt;GAN loss&lt;/strong&gt;가 추가가 되며, 전체 loss function은 위의 그림과 같이 설계가 될 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/cl_result.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/cl_result.PNG&quot; alt=&quot;본 논문의 confidence loss만 적용하였을 때의 실험 결과&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;본 논문의 confidence loss만 적용하였을 때의 실험 결과&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;본 논문도 baseline 논문과 성능을 비교하였으며, 위의 실험 결과는 confidence loss만 사용하였을 때의 성능을 보여주고 있습니다. Out-of-distribution 데이터 셋 중 confidence loss 학습을 위해 사용된 데이터셋을 (seen) 이라 표현을 하고 있습니다. confidence loss를 최소화하기 위해 사용된 out-of-distribution 데이터 셋에 대해서는 이미 봤던 데이터 셋이고, 이를 Uniform Distribution으로 예측하도록 학습시켰기 때문에, 당연한 결과지만 월등한 검출 성능을 보입니다. 다만, 학습에 관여하지 않은 out-of-distribution 데이터 셋(unseen)에 대해서는 성능이 좋아지기도 하고 나빠지기도 하는 결과를 보여주고 있습니다. 이는 confidence loss를 최소화하기 위해 사용된 out-of-distribution을 적절히 잘 골라야 성능 향상을 기대할 수 있음을 의미합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/cl_result2.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/cl_result2.PNG&quot; alt=&quot;본 논문의 전체 loss function을 적용하였을 때의 실험 결과&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;본 논문의 전체 loss function을 적용하였을 때의 실험 결과&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;즉 confidence loss만 사용하면 불안 요소가 많지만 GAN loss까지 같이 적용하면 전반적으로 성능이 올라가는 것을 실험을 통해 보여주고 있습니다. 위의 그림에서 청록색은 confidence loss만 사용하였을 때의 결과이고, 파란색은 전체 loss function을 사용하였을 때의 결과를 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;이 논문은 기존에는 inference 단계만 고려하고 있었는데, Out-of-distribution 문제를 풀기 위해 training 단계까지 고려하면 더 좋다는 결과를 보여준 논문이며 후속 연구에도 좋은 방향을 제시해준 논문이라 할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;---a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks-2018-nips-&quot;&gt;- &lt;a href=&quot;http://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks”, 2018 NIPS &lt;/b&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;이 논문은 방금 소개 드린 “Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples” 논문의 저자들이 진행한 후속 논문이며, loss function을 재설계하는 방향에서 벗어나 다시 inference 단계에 집중을 하였으며, 이미 학습이 끝난 network의 feature들은 class-conditional Gaussian distribution을 따른다는 가정하에 가장 가까운 class conditional distribution에 대한 &lt;strong&gt;Mahalanobis distance&lt;/strong&gt; 를 구하여 confidence score로 사용하겠다는 것이 논문의 핵심 내용입니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/mahalanobis_algorithm.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/mahalanobis_algorithm.PNG&quot; alt=&quot;본 논문의 알고리즘 설명&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;본 논문의 알고리즘 설명&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;논문에서 알고리즘을 설명하기 위해 만든 그림이며 핵심 내용들이 다 그림에 들어있습니다. 우선 가장 가까운 class를 찾은 뒤, ODIN에서 사용했던 것처럼 input에 perturbation을 가하는 방법을 사용합니다. 다만 ODIN에서는 softmax score를 증가시키는 방향으로 perturbation을 주었다면, 본 논문에서는 Mahalanobis distance를 증가시키는 방향으로 perturbation을 주는 것이 다른 점입니다.&lt;/p&gt;

&lt;p&gt;이렇게 해서 구해진 Mahalanobis distance를 confidence score로 사용하며, 가장 마지막 feature만 사용하던 기존 논문들과는 다르게 모든 low-level feature까지 사용하는 &lt;strong&gt;feature ensemble&lt;/strong&gt; 방식도 적용을 하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/mahalanobis_result.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/mahalanobis_result.PNG&quot; alt=&quot;본 논문의 실험 결과 표&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;본 논문의 실험 결과 표&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Input pre-processing과 Feature ensemble을 동시에 적용하면 모든 지표에서 ODIN보다 높은 성능을 달성할 수 있으며, 본 논문에서는 Out-of-distribution detection에서 끝나는 것이 아니라, Class-incremental learning에도 적용 가능성을 보여주고 있고, 나아가 adversarial attack에도 어느 정도 robust 해질 수 있음을 보이고 있습니다. Class incremental learning은 주어진 class에 대해 학습이 끝난 classifier에 새로운 class의 sample이 발생할 때, 이를 반영하여 새로운 class도 추가하여 학습을 하는 방법론을 의미합니다. 이 때 Mahalanobis distance 기반의 score를 활용하여 Class incremental learning을 하는 방법을 논문에서 제안하고 있습니다. Adversarial attack은 FGSM, BIM, DeepFool, CW 등의 방법을 통해 생성해낸 adversarial image들을 Out-of-distribution sample로 간주하여 얼마나 잘 걸러낼 수 있는지를 실험하였습니다. 하나의 방법으로 총 3가지 적용 사례를 제안했다는 점이 인상 깊은 논문입니다. Class incremental learning과 Adversarial attack에 대한 내용과 실험 결과가 궁금하신 분들은 논문을 참고하시면 좋을 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;---deep-anomaly-detection-with-outlier-exposure-2019-iclr-&quot;&gt;- &lt;a href=&quot;https://openreview.net/pdf?id=HyxCxhRcY7&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Deep Anomaly Detection with Outlier Exposure”, 2019 ICLR &lt;/b&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;이 논문은 첫번째 소개 드린 Baseline 논문의 저자가 진행한 후속 연구이며, Auxiliary(보조) 데이터 셋을 활용하여 Out-of-distribution Detection의 성능을 높이는 실용적인 방법을 제안하고 있습니다.&lt;/p&gt;

&lt;p&gt;보조 데이터 셋을 활용하려는 시도는 “Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples” 논문에서 사용된 confidence loss 아이디어에서 이미 등장한 바 있습니다. 다만, out-of-distribution test set의 검출 성능을 높이기 위해 기존 방법 들에서는 loss 간의 weight를 조절하거나, input pre-processing, output confidence calibration 등에 다양한 parameter들을 tuning 하는 과정이 필요했는데, 본 논문에서는 hyper-parameter에 민감하지 않다는 것을 장점으로 언급하고 있습니다. 그 외에 label이 없는 상황에서 density estimation을 하는 Generative model 기반 방법에도 Outlier Exposure 방법이 적용이 가능하며 이 때는 margin ranking loss를 사용했다는 점 등 약간의 차이점들이 존재합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/oe_result.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/oe_result.PNG&quot; alt=&quot;본 논문의 실험 결과 표&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;본 논문의 실험 결과 표&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Outlier Exposure는 기존 방법들에 독립적으로 추가가 가능한 아이디어여서, 기존 detector 들에 Outlier Exposure를 추가하였을 때 얼마나 성능이 향상되는지를 논문에서 결과로 제시하고 있습니다. 다만 Outlier Exposure로 어떤 데이터 셋을 사용하는지에 따라 성능이 크게 달라질 수 있다는 점이 풀어야 할 문제이며, Gaussian noise나 GAN으로 생성한 sample 등을 활용하는 것은 크게 효과적이지 않음을 한계라 설명하고 있습니다. 이에 대해선 Outlier Exposure로 사용하는 데이터 셋을 최대한 realistic 하면서 size도 크고, 다양하게 구축하는 것이 좋은 성능을 달성하는 데 도움을 준다고 가이드를 제시해주고 있습니다.&lt;/p&gt;

&lt;p&gt;본 논문은 기존에 존재하던 Out-of-distribution Detection 알고리즘들에 추가로 적용이 가능하면서도 손쉽게 구현이 가능한 방법론을 제안하였고, 실제로 쏠쏠한 성능 향상을 이뤄냈다는 점이 의미가 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;--generative-model-기반-접근법을-다룬-논문들&quot;&gt;- Generative model 기반 접근법을 다룬 논문들&lt;/h3&gt;
&lt;p&gt;오늘 소개 드린 5편의 Out-of-distribution 논문들은 모두 classifier 기반의 접근법을 이용하였는데, 학계에서는 2018년 무렵 Generative model을 기반으로 한 연구들도 제안되기 시작했습니다. 주로 label이 없는 상황에서 training sample의 분포를 예측해내는 것을 density estimation이라 부르고, Autoregressive and invertible model인 &lt;a href=&quot;https://arxiv.org/pdf/1701.05517.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; PixelCNN++ &lt;/b&gt;&lt;/a&gt; 방식과 &lt;a href=&quot;https://arxiv.org/pdf/1807.03039.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Glow &lt;/b&gt;&lt;/a&gt; 방식이 제안이 되었습니다. 이러한 Density estimator에 out-of-distribution sample을 넣어주면 낮은 likelihood 값으로 예측하기를 기대합니다.
다만, 기대와는 다르게 Generative model로부터 구해진 likelihood 값은 in-distribution 데이터 셋과 out-of-distribution 데이터 셋을 잘 구분하지 못한다는 문제가 &lt;a href=&quot;https://arxiv.org/pdf/1810.01392.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “WAIC, but why? Generative ensembles for robust anomaly detection” 2018 arXiv &lt;/b&gt;&lt;/a&gt; 논문에서 처음 제기가 되었으며 아래의 그림이 이 문제를 잘 보여주고 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/generative_problem.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/generative_problem.PNG&quot; alt=&quot;Generative Model의 likelihood 값을 보여주는 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Generative Model의 likelihood 값을 보여주는 예시&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;CIFAR-10으로 training을 시킨 뒤, CIFAR-10, TrafficSign, SVHN 등으로 test를 하는 경우에, generative model이 in-distribution 데이터 셋인 CIFAR-10보다 out-of-distribution 데이터 셋인 SVHN에서 더 높은 likelihood 값을 가지기도 하고, TrafficSign은 낮은 likelihood를 가지는 등 일관적이지 못한 현상이 관찰이 됩니다.&lt;/p&gt;

&lt;p&gt;이에 대해 분석한 논문 &lt;a href=&quot;https://openreview.net/pdf?id=SyxIWpVYvr&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models” 2020 ICLR &lt;/b&gt;&lt;/a&gt; 에 accept이 되었습니다. 본 논문에서는 input image의 복잡도가 클수록 likelihood가 낮게 관측된다는 것을 발견하였고, 이를 통해 input의 복잡도를 추정한 뒤, 이를 활용하여 Out-of-distribution score로 활용하는 방법을 제안하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/generative_result.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-2/generative_result.PNG&quot; alt=&quot;본 논문의 실험 결과 표&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;본 논문의 실험 결과 표&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;실험은 간단하게 FashionMNIST와 CIFAR-10을 in-distribution 데이터 셋으로 이용하고, 각각 다른 out-of-distribution 데이터 셋으로 test를 하여 기존 방법들과 검출 성능을 비교하였습니다. Classifier 기반의 방법론들과 견줄 만한 성능을 달성하였지만 데이터 셋에 따라 성능의 편차가 크다는 점에서 아직 더 연구해 볼 만한 여지가 있다고 생각이 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;결론&quot;&gt;결론&lt;/h2&gt;
&lt;p&gt;이번 포스팅에서는 Anomaly Detection 연구 분야 중 Out-of-distribution(OOD) Detection 문제를 다룬 여러 논문들을 바탕으로 소개를 드렸습니다. 초기 논문들은 Classifier를 기반으로 연구가 진행이 되어왔고 가장 초기에 나온 baseline 논문에서는 Maximum Softmax Probability를 이용하는 실험 프로토콜을 제안하였습니다.&lt;/p&gt;

&lt;p&gt;후속 논문에선 input pre-processing, softmax score에 calibration을 적용하는 방법과 confidence score를 Mahalanobis distance를 이용하여 계산하는 방법 등 test 단계에 집중하는 시도들과, GAN을 이용하는 방법, auxiliary 데이터 셋을 Outlier Exposure로 이용하여 network를 fine-tuning 하는 방법 등 training 단계에 집중하는 시도들이 제안이 되었고, 각 논문의 핵심 내용을 정리하여 소개 드렸습니다. 또한 Classifier 기반이 아닌 Generative model을 기반으로 한 연구들도 최근 제안이 되고 있음을 살펴보았습니다.&lt;/p&gt;

&lt;p&gt;Out-of-distribution Detection 문제는 Computer Vision 분야 외에도 자연어 처리, 음성 인식 등 다양한 분야에서 활용이 될 수 있습니다. 오늘 소개 드린 내용이 독자분들이 공부하시는 데 도움이 되었으면 좋겠습니다. 긴 글 읽어 주셔서 감사합니다!&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; AUROC, AUPR 그림 인용 블로그 글&lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mnist-demo.herokuapp.com/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; MNIST Web Demo &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1610.02136.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks”, &lt;/b&gt;&lt;/a&gt; 2017 ICLR&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.02690.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks”, &lt;/b&gt;&lt;/a&gt; 2018 ICLR&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Distilling the Knowledge in a Neural Network”, &lt;/b&gt;&lt;/a&gt; 2014 NIPS Workshop&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.04599.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “On Calibration of Modern Neural Networks”, &lt;/b&gt;&lt;/a&gt; 2017 ICML&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot; https://arxiv.org/pdf/1412.6572.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Explaining and Harnessing Adversarial Examples”, &lt;/b&gt;&lt;/a&gt; 2015 ICLR&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1610.02136.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples”, &lt;/b&gt;&lt;/a&gt; 2018 ICLR&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks”, &lt;/b&gt;&lt;/a&gt; 2018 NIPS&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=HyxCxhRcY7&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Deep Anomaly Detection with Outlier Exposure”, &lt;/b&gt;&lt;/a&gt; 2019 ICLR&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1701.05517.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications”,  &lt;/b&gt;&lt;/a&gt; 2017 ICLR&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1807.03039.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Glow: Generative Flow with Invertible 1x1 Convolutions”, &lt;/b&gt;&lt;/a&gt; 2018 NeurIPS&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.01392.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “WAIC, but why? Generative ensembles for robust anomaly detection”, &lt;/b&gt;&lt;/a&gt; 2018 arXiv&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=SyxIWpVYvr&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models”, &lt;/b&gt;&lt;/a&gt; 2020 ICLR&lt;/li&gt;
&lt;/ul&gt;</content><author><name>이호성</name><email>Hoseong.Lee@cognex.com</email></author><category term="Introduction" /><category term="Review" /><category term="Anomaly-Detection" /><summary type="html">안녕하세요, 이번 포스팅에서는 지난 포스팅에 이어 Anomaly Detection(이상 탐지)에 대한 내용을 다룰 예정이며 Anomaly Detection 연구 분야 중 Out-of-distribution(OOD) Detection 문제에 대해 여러 논문을 토대로 깊이 있게 소개를 드릴 예정입니다.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/high-confidence.PNG" /><media:content medium="image" url="http://github.sualab.io/high-confidence.PNG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Anomaly Detection 개요： (1) 이상치 탐지 분야에 대한 소개 및 주요 문제와 핵심 용어, 산업 현장 적용 사례 정리</title><link href="http://github.sualab.io/introduction/review/2020/01/30/anomaly-detection-overview-1.html" rel="alternate" type="text/html" title="Anomaly Detection 개요： (1) 이상치 탐지 분야에 대한 소개 및 주요 문제와 핵심 용어, 산업 현장 적용 사례 정리" /><published>2020-01-30T12:00:00+09:00</published><updated>2020-01-30T12:00:00+09:00</updated><id>http://github.sualab.io/introduction/review/2020/01/30/anomaly-detection-overview-1</id><content type="html" xml:base="http://github.sualab.io/introduction/review/2020/01/30/anomaly-detection-overview-1.html">&lt;p&gt;안녕하세요. 이번 포스팅에서는 Anomaly Detection(이상 탐지)에 대해 소개를 드리고자 합니다. Anomaly Detection이란, Normal(정상) sample과 Abnormal(비정상, 이상치, 특이치) sample을 구별해내는 문제를 의미하며 수아랩이 다루고 있는 제조업뿐만 아니라 CCTV, 의료 영상, Social Network 등 다양한 분야에서 응용이 되고 있습니다. 그러나 Anomaly Detection 용어 외에도 다양한 용어가 비슷한 의미로 사용되고 있어서 이 용어들을 기준에 따라 정리하고 각 용어에 대해 자세히 설명하겠습니다. 이어질 포스팅에서는 Anomaly Detection 연구 분야에서 다루는 Out-of-distribution(OOD) Detection 문제에 대해 여러 논문과 함께 깊이 있게 소개할 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;anomaly-detection-연구-분야-용어-정리&quot;&gt;Anomaly Detection 연구 분야 용어 정리&lt;/h2&gt;

&lt;p&gt;Anomaly Detection은 학습 데이터 셋에 비정상적인 sample이 포함되는지, 각 sample의 label이 존재하는지, 비정상적인 sample의 성격이 정상 sample과 어떻게 다른지, 정상 sample의 class가 단일 class 인지 Multi-class 인지 등에 따라 다른 용어를 사용합니다. 이 용어들을 정리하기 위해 학계에서 다뤄지고 있는 Anomaly Detection 논문 서베이를 수행하고 각 논문을 참고하여 용어를 정리해보았습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 서베이 자료는 &lt;a href=&quot;https://github.com/hoya012/awesome-anomaly-detection&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “awesome-anomaly-detection” GitHub Repository &lt;/b&gt;&lt;/a&gt; 에서 확인하실 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-학습시-비정상-sample의-사용여부-및-label-유무에-따른-분류&quot;&gt;1. 학습시 비정상 sample의 사용여부 및 label 유무에 따른 분류&lt;/h3&gt;
&lt;h4 id=&quot;supervised-anomaly-detection&quot;&gt;Supervised Anomaly Detection&lt;/h4&gt;
&lt;p&gt;주어진 학습 데이터 셋에 정상 sample과 비정상 sample의 Data와 Label이 모두 존재하는 경우 Supervised Learning 방식이기 때문에 Supervised Anomaly Detection이라 부릅니다. Supervised Learning 방식은 다른 방법 대비 정확도가 높은 특징이 있습니다. 그래서 높은 정확도를 요구로 하는 경우에 주로 사용되며, 비정상 sample을 다양하게 보유할수록 더 높은 성능을 달성할 수 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 Anomaly Detection이 적용되는 일반적인 산업 현장에서는 정상 sample보다 비정상 sample의 발생 빈도가 현저히 적기 때문에 &lt;strong&gt;Class-Imbalance(불균형)&lt;/strong&gt; 문제를 자주 겪게 됩니다. 이러한 문제를 해결하기 위해 Data Augmentation(증강), Loss function 재설계, Batch Sampling 등 다양한 연구가 수행되고 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;장점: 양/불 판정 정확도가 높다.&lt;/li&gt;
  &lt;li&gt;단점: 비정상 sample을 취득하는데 시간과 비용이 많이 든다. Class-Imbalance 문제를 해결해야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;semi-supervised-one-class-anomaly-detection&quot;&gt;Semi-supervised (One-Class) Anomaly Detection&lt;/h4&gt;
&lt;p&gt;Supervised Anomaly Detection 방식의 가장 큰 문제는 비정상 sample을 확보하는데 많은 시간과 비용이 든다는 것입니다. 제조업의 경우를 예로 들면, 수백만 장의 정상 sample이 취득되는 동안 단 1~2장의 비정상 sample이 취득되는 상황이 종종 발생합니다.&lt;/p&gt;

&lt;p&gt;제조업에서 Supervised Learning 방식으로 학습하기 위해 각 class 당 최소 100장의 이미지가 필요하다고 가정하면, 실제로는 sample 1억 장을 모아야 100장 정도의 비정상 sample을 확보할 수 있습니다. 이런 상황에서는 데이터 셋을 확보하는데 굉장히 오랜 시간이 소요되겠죠?&lt;/p&gt;

&lt;p&gt;이처럼 Class-Imbalance가 매우 심한 경우 정상 sample만 이용해서 모델을 학습하기도 하는데, 이 방식을 One-Class Classification(혹은 Semi-supervised Learning)이라 합니다. 이 방법론의 핵심 아이디어는 정상 sample들을 둘러싸는 discriminative boundary를 설정하고, 이 boundary를 최대한 좁혀 boundary 밖에 있는 sample들을 모두 비정상으로 간주하는 것입니다. &lt;a href=&quot;http://www.jmlr.org/papers/volume2/manevitz01a/manevitz01a.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; One-Class SVM &lt;/b&gt;&lt;/a&gt;이 One-Class Classification을 사용하는 대표적인 방법론으로 잘 알려져 있으며, 이 아이디어에서 확장해 Deep Learning을 기반으로 One-Class Classification 방법론을 사용하는 &lt;a href=&quot;http://data.bit.uni-bonn.de/publications/ICML2018.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Deep SVDD &lt;/b&gt;&lt;/a&gt; 논문이 잘 알려져 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/deep_svdd.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/deep_svdd.PNG&quot; alt=&quot;Deep SVDD 방법론 모식도&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Deep SVDD 방법론 모식도&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이 외에도 Energy-based 방법론 &lt;a href=&quot;https://arxiv.org/pdf/1605.07717.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Deep structured energy based models for anomaly detection, 2016 ICML” &lt;/b&gt;&lt;/a&gt;, Deep Autoencoding Gaussian Mixture Model 방법론  &lt;a href=&quot;https://sites.cs.ucsb.edu/~bzong/doc/iclr18-dagmm.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Deep autoencoding gaussian mixture model for unsupervised anomaly detection, 2018 ICLR” &lt;/b&gt;&lt;/a&gt; , Generative Adversarial Network 기반 방법론 &lt;a href=&quot;https://arxiv.org/pdf/1809.04758.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Anomaly detection with generative adversarial networks, 2018 arXiv” &lt;/b&gt;&lt;/a&gt;, Self-Supervised Learning 기반  &lt;a href=&quot;https://papers.nips.cc/paper/8183-deep-anomaly-detection-using-geometric-transformations.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Deep Anomaly Detection Using Geometric Transformations, 2018 NeurIPS” &lt;/b&gt;&lt;/a&gt;
등 다양한 시도가 이뤄지고 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;장점: 비교적 활발하게 연구가 진행되고 있으며, 정상 sample만 있어도 학습이 가능하다.&lt;/li&gt;
  &lt;li&gt;단점: Supervised Anomaly Detection 방법론과 비교했을 때 상대적으로 양/불 판정 정확도가 떨어진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;unsupervised-anomaly-detection&quot;&gt;Unsupervised Anomaly Detection&lt;/h4&gt;
&lt;p&gt;위에서 설명드린 One-Class(Semi-supervised) Anomaly Detection 방식은 정상 sample이 필요합니다. 수많은 데이터 중에 어떤 것이 정상 sample 인지 알기 위해서는 반드시 정상 sample에 대한 Label을 확보하는 과정이 필요합니다. 이러한 점에 주목해, 대부분의 데이터가 정상 sample이라는 가정을 하여 Label 취득 없이 학습을 시키는 Unsupervised Anomaly Detection 방법론도 연구가 이뤄지고 있습니다.&lt;/p&gt;

&lt;p&gt;가장 단순하게는 주어진 데이터에 대해 Principal Component Analysis(PCA, 주성분 분석)를 이용하여 차원을 축소하고 복원을 하는 과정을 통해 비정상 sample을 검출할 수 있습니다. , Neural Network 기반으로는 대표적으로 Autoencoder 기반의 방법론이 주로 사용되고 있습니다. Autoencoder는 입력을 code 혹은 latent variable로 압축하는 Encoding과, 이를 다시 원본과 가깝게 복원해내는 Decoding 과정으로 진행이 되며 이를 통해 데이터의 중요한 정보들만 압축적으로 배울 수 있다는 점에서 데이터의 주성분을 배울 수 있는 PCA와 유사한 동작을 한다고 볼 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/autoencoder.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/autoencoder.PNG&quot; alt=&quot;autoencoder 기반 unsupervised anomaly detection&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;autoencoder 기반 unsupervised anomaly detection&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Autoencoder를 이용하면 데이터에 대한 labeling을 하지 않아도 데이터의 주성분이 되는 정상 영역의 특징들을 배울 수 있습니다. 이때, 학습된 autoencoder에 정상 sample을 넣어주면 위의 그림과 같이 잘 복원을 하므로 input과 output의 차이가 거의 발생하지 않는 반면, 비정상적인 sample을 넣으면 autoencoder는 정상 sample처럼 복원하기 때문에 input과 output의 차이를 구하는 과정에서 차이가 도드라지게 발생하므로 비정상 sample을 검출할 수 있습니다.&lt;/p&gt;

&lt;p&gt;다만 Autoencoder의 압축 정도(= code size = latent variable의 dimension) 같은 hyper-parameter에 따라 전반적인 복원 성능이 좌우되기 때문에 양/불 판정 정확도가 Supervised Anomaly Detection에 비해 다소 불안정하다는 단점이 존재합니다. 또한 autoencoder에 넣어주는 input과 output의 차이를 어떻게 정의할 것인지(= 어떤 방식으로 difference map을 계산할지) 어느 loss function을 사용해 autoencoder를 학습시킬지 등 여러 가지 요인에 따라 성능이 크게 달라질 수 있습니다. 이렇듯 성능에 영향을 주는 요인이 많다는 약점이 존재하지만 별도의 Labeling 과정 없이 어느정도 성능을 낼 수 있다는 점에서 장단이 뚜렷한 방법론이라 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 Autoencoder를 이용하여 Unsupervised Anomaly Detection을 적용하여 Defect(결함)을 Segment 하는 대표적인 논문들에서는 Unsupervised 데이터 셋이 존재하지 않아서 실험의 편의를 위해 학습에 정상 sample들만 사용하는 Semi-Supervised Learning 방식을 이용하였으나, Autoencoder를 이용한 방법론은 Unsupervised Learning 방식이며 Unsupervised 데이터 셋에도 적용할 수 있습니다. Autoencoder 기반 Unsupervised Anomaly Detection을 다룬 논문들은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1807.02011.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Improving Unsupervised Defect Segmentation by Applying Structural Similarity to Autoencoders &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1804.04488.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain MR Images &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mvtec.com/fileadmin/Redaktion/mvtec.com/company/research/mvtec_ad.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; MVTec AD – A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Autoencoder 기반의 anomaly detection 방법론에 대한 설명은 &lt;a href=&quot;https://kh-kim.github.io/blog/2019/12/15/Autoencoder-based-anomaly-detection.html&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 마키나락스 김기현님 블로그 글 &lt;/b&gt;&lt;/a&gt;에 잘 정리가 되어있어 따로 다루진 않을 예정입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;장점: Labeling 과정이 필요하지 않다.&lt;/li&gt;
  &lt;li&gt;단점: 양/불 판정 정확도가 높지 않고 hyper parameter에 매우 민감하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-비정상-sample-정의에-따른-분류&quot;&gt;2. 비정상 sample 정의에 따른 분류&lt;/h3&gt;
&lt;p&gt;다음은 비정상 sample의 정의에 따른 분류입니다. 이 분류는 엄밀하게 정의가 되지 않아 틀린 부분이 있을 수도 있습니다. 이 점 미리 양해 바라며 나름대로 정리한 내용을 설명하겠습니다.&lt;/p&gt;

&lt;p&gt;저는 비정상 sample을 정의하는 방식에 따라 크게 Novelty Detection과 Outlier Detection으로 구분합니다. 다만 종종 두 방법론을 합쳐서 Anomaly Detection라 부르기도 합니다. 개인적인 생각으로는 Novelty Detection과 Outlier Detection은 용어가 가지는 뉘앙스의 차이가 존재하다고 느껴서, 예시를 통해 두 용어의 차이를 설명을 드리겠습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/novelty_outlier.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/novelty_outlier.PNG&quot; alt=&quot;Anomaly Detection 용어 정리를 위한 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Anomaly Detection 용어 정리를 위한 예시&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;강아지를 normal class로 정의를 한 경우를 예로 들겠습니다. 현재 보유 중인 데이터 셋에 이전에 없던 형태의 새로운 강아지가 등장하는 경우, 이러한 sample을 Novel sample, Unseen sample 등으로 부를 수 있습니다. 그리고 이러한 sample을 찾아내는 방법론을 Novelty Detection이라 부를 수 있습니다.&lt;/p&gt;

&lt;p&gt;마찬가지로 새로운 sample이 등장했을 때, 이번엔 강아지가 아닌 호랑이, 말, 운동화, 비행기 등 강아지와 전혀 관련 없는 sample이 등장한다고 가정해보겠습니다. 이러한 sample들을 Outlier sample, 혹은 Abnormal sample이라 부르며, 이러한 sample을 찾아내는 문제를 Outlier Detection이라 부를 수 있습니다.&lt;/p&gt;

&lt;p&gt;사실 두 용어의 기준이 명확하진 않습니다. 어떻게 보면 혼재되어서 사용이 되는 것이 당연한 현상이라 볼 수 있습니다. 굳이 두 용어를 구분하면, 위에서 예시로 설명드렸던 관점에서의 뉘앙스 차이로 구분하는 방법이 있다고 이해하시면 좋을 것 같습니다.&lt;/p&gt;

&lt;p&gt;Novelty Detection은 지금까지 등장하지 않았지만 충분히 등장할 수 있는 sample을 찾아내는 연구, 즉 데이터가 오염이 되지 않은 상황을 가정하는 연구와 관련된 용어라고 할 수 있고, Outlier Detection은 등장할 가능성이 거의 없는, 데이터에 오염이 발생했을 가능성이 있는 sample을 찾아 내는 연구와 관련된 용어 정도로 구분하여 정리할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;3-정상-sample의-class-개수에-따른-분류&quot;&gt;3. 정상 sample의 class 개수에 따른 분류&lt;/h3&gt;
&lt;p&gt;앞서 설명드린 두 가지 기준은 데이터 셋이 정상 sample이 단일 class로 구성이 되어있고, 단순 양/불 판정을 하는 경우에 대해서만 가정했지만, 실제 환경에서는 정상 sample이 여러 개의 class로 구성될 수 있습니다.&lt;/p&gt;

&lt;p&gt;그러나 정상 sample이 Multi-Class인 상황에서도 위의 Novelty Detection, Outlier Detection 기준을 똑같이 적용할 수 있습니다. 보통 이러한 경우 정상 sample이라는 표현 대신 In-distribution sample이라는 표현을 사용합니다.&lt;/p&gt;

&lt;p&gt;In-distribution 데이터 셋에 위의 예시 그림처럼 흰색 강아지만 있는 것이 아니라, 골든 레트리버, 닥스훈트, 도베르만, 말티즈 등 4가지 종류의 강아지 sample들이 존재한다고 가정하면, 불독 sample은 Novel sample, 호랑이 sample은 Outlier sample로 간주할 수 있습니다. 
&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/ood_example.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/ood_example.PNG&quot; alt=&quot;Out-of-distribution sample 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Out-of-distribution sample 예시&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이렇게 In-distribution 데이터 셋으로 network를 학습시킨 뒤, test 단계에서 비정상 sample을 찾는 문제를 &lt;strong&gt;Out-of-distribution Detection&lt;/strong&gt; 이라 부르며 학계에서는 널리 사용되는 주요 Benchmark 데이터 셋들을 이용하여 실험을 수행하고 있습니다. 예를 들면 In-distribution 데이터 셋으로 CIFAR-10을 가정하고 Classifier를 학습시킵니다. 그 뒤, 실내 이미지 데이터 셋인 LSUN, 숫자 관련 데이터 셋인 SVHN 등을 Out-of-distribution 데이터 셋으로 가정한 뒤 test 시에 In-distribution 데이터 셋인 CIFAR-10은 얼마나 정확하게 분류를 하는지, LSUN, SVHN 등 Out-of-distribution 데이터 셋은 얼마나 잘 걸러낼 수 있는지를 살펴보는 방식을 사용하고 있습니다.&lt;/p&gt;

&lt;p&gt;대부분의 연구에서 주로 사용하는 SoftMax 기반 classifier는 class 개수를 정해 놓고 가장 확률이 높은 class를 결과로 출력하는 방식이기 때문에, 위에서 예시로 들었던 4가지 종류의 강아지를 구분하는 classifier에 호랑이 이미지를 넣어주면 사람은 비정상 sample이라고 구분할 수 있는 반면 classifier는 4가지 class 중 하나의 class로 예측을 하게 됩니다. 이러한 Outlier sample을 걸러 내기 위해 Out-of-distribution Detection 알고리즘을 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 불독 이미지처럼 Novel 한 sample이 관찰됐을 때 이를 걸러낸 뒤, classifier가 기존에 있는 4가지 class 대신 불독이 새로 추가된 5가지 class를 구분하도록 학습하는 &lt;strong&gt;Incremental Learning&lt;/strong&gt; 방법론과도 응용할 수 있습니다. Out-of-distribution Detection의 문제 정의와 주요 논문들은 다음 포스팅에서 더 구체적으로 다룰 예정이니 다음 포스팅을 기대해주세요!&lt;/p&gt;

&lt;p&gt;이렇게 총 3가지 분류 방법으로 용어를 정리하였고, 이를 한 장의 그림으로 요약하면 다음과 같습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/summary.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/summary.PNG&quot; alt=&quot;Anomaly Detection의 3가지 용어의 분류 방법 정리&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Anomaly Detection의 3가지 용어의 분류 방법 정리&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;각 용어가 지니는 의미와 문제 상황 등을 잘 이해하신다면 추후 Anomaly Detection 관련 논문을 읽으실 때 도움이 될 것이라 생각합니다.&lt;/p&gt;

&lt;h2 id=&quot;anomaly-detection의-다양한-적용-사례&quot;&gt;Anomaly Detection의 다양한 적용 사례&lt;/h2&gt;

&lt;p&gt;위에서 Anomaly Detection의 방법론, 문제 상황 등을 정리하였다면 이번엔 산업 전반적인 분야의 대표적인 적용 사례들을 하나씩 소개드릴 예정입니다. 대부분의 예시는 &lt;a href=&quot;https://arxiv.org/abs/1901.03407&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “Deep Learning for Anomaly Detection: A Survey,” 2019 arXiv &lt;/b&gt;&lt;/a&gt; 2019년에 작성된 서베이 논문을 참고하여 작성하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/applications.PNG&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/anomaly-detection-overview-1/applications.PNG&quot; alt=&quot;Anomaly Detection의 적용 사례&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Anomaly Detection의 적용 사례&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Anomaly Detection이 적용될 수 있는 주요 사례는 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cyber-Intrusion Detection: 컴퓨터 시스템 상에 침입을 탐지하는 사례. 주로 시계열 데이터를 다루며 RAM, file system, log file 등 일련의 시계열 데이터에 대해 이상치를 검출하여 침입을 탐지함.&lt;/li&gt;
  &lt;li&gt;Fraud Detection: 보험, 신용, 금융 관련 데이터에서 불법 행위를 검출하는 사례. 주로 표로 나타낸(tabular) 데이터를 다루며 &lt;a href=&quot;https://www.kaggle.com/mlg-ulb/creditcardfraud&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Kaggle Credit Card Fraud Detection &lt;/b&gt;&lt;/a&gt;과 같은 공개된 challenge도 진행된 바 있음.&lt;/li&gt;
  &lt;li&gt;Malware Detection: Malware(악성코드)를 검출해내는 사례. Classification과 Clustering이 주로 사용되며 Malware tabular 데이터를 그대로 이용하기도 하고 이를 gray scale image로 변환하여 이용하기도 함.&lt;/li&gt;
  &lt;li&gt;Medical Anomaly Detection: 의료 영상, 뇌파 기록 등의 의학 데이터에 대한 이상치 탐지 사례. 주로 신호 데이터와 이미지 데이터를 다루며 X-ray, CT, MRI, PET 등 다양한 장비로부터 취득된 이미지를 다루기 때문에 난이도가 높음.&lt;/li&gt;
  &lt;li&gt;Social Networks Anomaly Detection: Social Network 상의 이상치들을 검출하는 사례. 주로 Text 데이터를 다루며 Text를 통해 스팸 메일, 비매너 이용자, 허위 정보 유포자 등을 검출함.&lt;/li&gt;
  &lt;li&gt;Log Anomaly Detection: 시스템이 기록한 log를 보고 실패 원인을 추적하는 사례. 주로 Text 데이터를 다루며 pattern matching 기반의 단순한 방법을 사용하여 해결할 수 있지만 failure message가 새로운 것이 계속 추가, 제외가 되는 경우에 딥러닝 기반 방법론을 사용하는 것이 효과적임.&lt;/li&gt;
  &lt;li&gt;IoT Big-Data Anomaly Detection: 사물 인터넷에 주로 사용되는 장치, 센서들로부터 생성된 데이터에 대해 이상치를 탐지하는 사례. 주로 시계열 데이터를 다루며 여러 장치들이 복합적으로 구성이 되어있기 때문에 난이도가 높음.&lt;/li&gt;
  &lt;li&gt;Industrial Anomaly Detection: 산업 속 제조업 데이터에 대한 이상치를 탐지하는 사례. 각종 제조업 도메인 이미지 데이터에 대한 외관 검사, 장비로부터 측정된 시계열 데이터를 기반으로 한 고장 예측 등 다양한 적용 사례가 있으며, 외관상에 발생하는 결함과, 장비의 고장 등의 비정상적인 sample이 굉장히 적은 수로 발생하지만 정확하게 예측하지 못하면 큰 손실을 유발하기 때문에 난이도가 높음.&lt;/li&gt;
  &lt;li&gt;Video Surveillance: 비디오 영상에서 이상한 행동이 발생하는 것을 모니터링하는 사례. 주로 CCTV를 이용한 사례가 주를 이루며, 보행로에 자전거, 차량 등이 출현하는 비정상 sample, 지하철역에서 넘어짐, 싸움 등이 발생하는 비정상 sample 등 다양한 종류의 비정상 케이스가 존재함.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;소개 드린 9가지 예시 외에도 다양한 분야에서 Anomaly Detection이 적용될 수 있으며, 하나의 모델로 모든 문제를 잘 풀기가 어려워서 각 도메인의 특성을 잘 반영하려는 시도들이 논문을 통해 드러나고 있는 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;결론&quot;&gt;결론&lt;/h2&gt;
&lt;p&gt;지금까지 이상치 탐지 (Anomaly Detection) 분야에 대한 전반적인 내용을 크게 연구 분야 용어 정리를 통한 연구 방향 소개와 각종 산업 현장의 적용 사례로 나눠서 설명을 드렸습니다.&lt;/p&gt;

&lt;p&gt;학계와 여러 게시물 등에서 Anomaly Detection, Novelty Detection, Outlier Detection 등 여러 용어가 혼재된 채 사용이 되고 있어서 이를 처한 문제 상황에 따라 용어를 정리를 해보았고, 보유하고 있는 데이터 셋의 특징에 따라 Supervised, One-Class(Semi-Supervised), Unsupervised로 나눠서 각각의 특징과 장단점을 소개 드렸습니다.
또한 정상 sample의 class 개수가 여러 개인 상황일 때에도 Anomaly Detection 문제를 다룰 수 있으며, 이때는 Out-of-distribution Detection이라는 용어로 주로 사용이 되며, 학계에서 어떻게 이 문제를 정의하고 실험하는지, 왜 중요한지 등에 대해 알아보았습니다.&lt;/p&gt;

&lt;p&gt;또한 Anomaly Detection이 실제 산업 현장에서 적용되는 대표적인 9가지 사례를 소개 드리고, 각각 사례가 어떤 문제를 풀고 있는지, 어떠한 데이터를 주로 다루는지 등을 알아보았습니다.&lt;/p&gt;

&lt;p&gt;이어지는 포스팅에서는 위에서 소개 드린 Out-of-distribution Detection이 학계에서 어떻게 연구가 되고 있는지 초창기 논문부터 최신 논문까지 논문을 리뷰하며 각각 논문들의 특징들을 요약하여 설명을 드릴 예정입니다.&lt;/p&gt;
&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/hoya012/awesome-anomaly-detection&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; “awesome-anomaly-detection” GitHub Repository &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume2/manevitz01a/manevitz01a.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Larry M. Manevitz, Malik Yousef. “One-Class SVMs for Document Classification.” Journal of Machine Learning Research, 2001. &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://data.bit.uni-bonn.de/publications/ICML2018.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Lukas Ruff, et al. “Deep One-Class Classification.” In International Conference on Machine Learning (ICML), 2018. &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1605.07717.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Shuangfei Zhai, et al. “Deep structured energy based models for anomaly detection.” In International Conference on Machine Learning (ICML), 2016. &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sites.cs.ucsb.edu/~bzong/doc/iclr18-dagmm.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Bo Zong, et al. “Deep autoencoding gaussian mixture model for unsupervised anomaly detection.” In International Conference on Learning Representations (ICLR), 2018&lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1809.04758.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Dan Li, et al. “Anomaly detection with generative adversarial networks.” arXiv, 2018. &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/8183-deep-anomaly-detection-using-geometric-transformations.pdf&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Izhak Golan, Ran El-Yaniv. “Deep Anomaly Detection Using Geometric Transformations.” In Conference on Neural Information Processing Systems (NeurIPS), 2018. &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kh-kim.github.io/blog/2019/12/15/Autoencoder-based-anomaly-detection.html&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; 마키나락스 김기현님 블로그 글 &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.03407&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Raghavendra Chalapathy, Sanjay Chawla. “Deep Learning for Anomaly Detection: A Survey.” arXiv, 2019. &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/mlg-ulb/creditcardfraud&quot; target=&quot;_blank&quot;&gt;&lt;b&gt; Kaggle Credit Card Fraud Detection &lt;/b&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>이호성</name><email>Hoseong.Lee@cognex.com</email></author><category term="Introduction" /><category term="Review" /><category term="Anomaly-Detection" /><summary type="html">안녕하세요. 이번 포스팅에서는 Anomaly Detection(이상 탐지)에 대해 소개를 드리고자 합니다. Anomaly Detection이란, Normal(정상) sample과 Abnormal(비정상, 이상치, 특이치) sample을 구별해내는 문제를 의미하며 수아랩이 다루고 있는 제조업뿐만 아니라 CCTV, 의료 영상, Social Network 등 다양한 분야에서 응용이 되고 있습니다. 그러나 Anomaly Detection 용어 외에도 다양한 용어가 비슷한 의미로 사용되고 있어서 이 용어들을 기준에 따라 정리하고 각 용어에 대해 자세히 설명하겠습니다. 이어질 포스팅에서는 Anomaly Detection 연구 분야에서 다루는 Out-of-distribution(OOD) Detection 문제에 대해 여러 논문과 함께 깊이 있게 소개할 예정입니다.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/novelty_outlier.PNG" /><media:content medium="image" url="http://github.sualab.io/novelty_outlier.PNG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Transfer Learning in SUALAB: 데이터셋 제작부터 Adaptive Transfer Learning까지</title><link href="http://github.sualab.io/introduction/review/2019/12/19/transfer-learning-in-sualab.html" rel="alternate" type="text/html" title="Transfer Learning in SUALAB: 데이터셋 제작부터 Adaptive Transfer Learning까지" /><published>2019-12-19T09:00:00+09:00</published><updated>2019-12-19T09:00:00+09:00</updated><id>http://github.sualab.io/introduction/review/2019/12/19/transfer-learning-in-sualab</id><content type="html" xml:base="http://github.sualab.io/introduction/review/2019/12/19/transfer-learning-in-sualab.html">&lt;p&gt;본 글에서는 전이 학습(transfer learning)이라는 분야에 대해 소개해드리고, 해당 분야를 수아랩이 어떤 식으로 연구하고 있는지 직접 수행한 작업들과 간단한 실험 결과 일부를 공유드리면서, 추후 연구 방향에 대해 이야기해보려고 합니다. 지금까지 수아랩 기술 블로그에서는 다른 연구자의 연구에 대한 소개가 주를 이루었었는데, 처음으로 수아랩에서 직접 연구한 내용을 소개해드리게 되었습니다. 흥미롭게 읽어주시면 감사하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;서론-수아랩의-칼-솜씨&quot;&gt;서론: 수아랩의 칼 솜씨&lt;/h2&gt;
&lt;p&gt;솜씨 좋은 킬러가 수식어가 무색하게 공중목욕탕에서 미끄러지면서 머리를 다쳐 기억을 잃고 맙니다. 이와 더불어 목욕탕의 물품 보관함 키가 바뀌는 바람에 새로운 사람의 인생을 살게 된 킬러는 우여곡절 끝에 분식집에서 일하게 되는데, 뛰어난 칼 실력으로 재료 손질과 김밥 썰기를 순식간에 처리하고 단무지 꽃 등으로 손님들의 마음을 사로잡아 분식집에 사람이 몰리게 만듭니다.  ‘럭키’라는 영화에 등장하는 주인공의 이야기입니다. 그가 갑자기 분식집에서 일하게 되었음에도 불구하고 빠르게 적응하고 뛰어난 성과를 이뤄낸 것은 킬러로써의 기술력(칼 솜씨)이 있었기 때문이라고 생각합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/intro-luckkey.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/intro-luckkey.jpg&quot; alt=&quot;분식집에서 칼을 가는 전직 킬러(영화 \'럭키\')&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;분식집에서 칼을 가는 전직 킬러(영화 '럭키')&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;킬러가 아닌 수아랩의 기술력은 무엇일까요? 연구 인력, 제품(수아킷, 장비), 특허, 논문 등이 될 수도 있지만, 딥러닝을 연구하는 입장으로는 가장 큰 부분 중 하나가 &lt;strong&gt;내부적으로 축적된 데이터셋&lt;/strong&gt;이라고 생각합니다. 수아랩은 데이터셋을 자체적으로 제작하기도하고 고객사로부터 연구를 의뢰받으면서 취득하기도 합니다. 그리고 이렇게 내부적으로 축적된 비전 검사 데이터셋들을 연구시 벤치마크 데이터셋으로 사용하고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h2&gt;

&lt;p&gt;이렇게 축적된 데이터셋들을 단순 벤치마킹 이외의 방법으로 활용할 수는 없을까요? 이러한 문제의식에서 출발하여 수아랩이 집중하고 있는 연구 분야 중 &lt;strong&gt;전이 학습(transfer learning)&lt;/strong&gt;이라는 분야가 있습니다.  전이 학습은 &lt;strong&gt;하나의 문제를 풀기 위한 지식을 추출하여 다르지만 관련 있는 문제에 적용&lt;/strong&gt;하는 시나리오를 연구하고 있는 분야입니다. 이때 ‘하나의 문제’를 일반적으로 &lt;strong&gt;소스(source)&lt;/strong&gt;라고 표현을 하고 소스에서 가져온 &lt;strong&gt;지식(knowledge)&lt;/strong&gt;을 적용하는 ‘다르지만 관련 있는 문제’를 &lt;strong&gt;타겟(target)&lt;/strong&gt;이라고 지칭합니다. 영화 ‘럭키’를 예를 들면 소스인 &lt;strong&gt;킬러의 일&lt;/strong&gt;에서 배운 지식인 &lt;strong&gt;칼 솜씨&lt;/strong&gt;를 타겟인 &lt;strong&gt;분식집 일&lt;/strong&gt;에 적용한 경우라고 볼 수 있습니다.즉 아래와 같은 그림으로 간단하게 표현될 수 있는 문제를 다루고 있는 분야입니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/transfer-learning-overview.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/transfer-learning-overview.svg&quot; alt=&quot;전이 학습 도식도&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;전이 학습 도식도&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;일반적으로 전이 학습은 타겟에서의 부족한 정보를 소스로부터 보충하고자 할 때 많이 쓰입니다.(위 도식도에서 타겟의 원을 조금 작게 그린 이유도 이 때문입니다.) 이미지 분류를 예로 들면 &lt;a href=&quot;http://www.image-net.org&quot; target=&quot;_blank&quot;&gt;이미지넷(ImageNet)&lt;/a&gt;과 같은 대용량 데이터셋을 이용해 &lt;strong&gt;사전학습(pre-training)&lt;/strong&gt;한 모델을 가져와서 본 목적에 해당하는 학습(e.g. &lt;a href=&quot;http://research.sualab.com/practice/2018/01/17/image-classification-deep-learning.html&quot; target=&quot;_blank&quot;&gt;개, 고양이 분류&lt;/a&gt;)시 모델을 초기화하고 &lt;strong&gt;파인튜닝(fine-tuning)&lt;/strong&gt;하는 경우에 보통 ‘전이 학습’을 적용했다고 표현합니다. 이 경우 소스 데이터셋은 &lt;strong&gt;이미지넷&lt;/strong&gt;,  지식은 &lt;strong&gt;사전학습된 모델(pre-trained model)&lt;/strong&gt;, 타겟 작업은 &lt;strong&gt;개, 고양이 분류 문제&lt;/strong&gt;가 되겠습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/transfer-learning-using-imagenet.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/transfer-learning-using-imagenet.svg&quot; alt=&quot;개/고양이 분류 문제를 위한 이미지넷 전이 학습 도식도&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;개/고양이 분류 문제를 위한 이미지넷 전이 학습 도식도&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이외에도 전이 학습은 많은 연구에서 광범위하게 쓰이는 용어인데, 지금부터는 수아랩에서 전이 학습과 관련된 어떤 연구를 했었는지 수행한 작업들과 실험 결과들을 공유드리겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning-in-sualab&quot;&gt;Transfer Learning in Sualab&lt;/h2&gt;

&lt;h3 id=&quot;데이터-정리-및-통합-데이터셋-제작&quot;&gt;데이터 정리 및 통합 데이터셋 제작&lt;/h3&gt;

&lt;p&gt;시작은 데이터를 정리하는 작업이였습니다. 수아랩의 주력 분야인 &lt;strong&gt;비전 검사&lt;/strong&gt;라는 하나의 목표로 수집 및 제작된 데이터셋들은 저장 구조, 파일 종류, 레이블링 등이 모두 중구난방의 형태로 존재하였습니다. 따라서 효율적인 사용을 위해 통합된 형식으로의 정리가 필요하였습니다.  조금 구체적으로는 &lt;a href=&quot;http://host.robots.ox.ac.uk/pascal/VOC/voc2012/&quot; target=&quot;_blank&quot;&gt;Pascal VOC Challenge&lt;/a&gt;에서 사용했던 데이터셋 구조 등을 참고하여 데이터 정리 규약을 만들고 정리하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/pascal-voc-devset-format.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/pascal-voc-devset-format.svg&quot; alt=&quot;Pascal VOC 챌린지에서 사용했던 데이터셋 구조. 레이블링 정보(Annotations, SegmentationClass, SegmentationObject), 실제 이미지(JPEGImages), 학습/검증용으로 분할된 이미지 이름 목록(Imagesets) 등이 구분되어 저장되어있습니다.&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Pascal VOC 챌린지에서 사용했던 데이터셋 구조. 레이블링 정보(Annotations, SegmentationClass, SegmentationObject), 실제 이미지(JPEGImages), 학습/검증용으로 분할된 이미지 이름 목록(Imagesets) 등이 구분되어 저장되어있습니다.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;그리고 정리된 &lt;strong&gt;단일 이미지 분류 문제&lt;/strong&gt;(한 장의 이미지를 이용하여 클래스를 구분) 데이터셋을 통합하여 이미지넷과 같은 하나의 큰 데이터셋을 만들었습니다. 통합 데이터셋의 클래스 구성은 통합되기 이전의 어떤 데이터셋에 속해 있었는지와 해당 데이터셋의 클래스를 모두 구분하도록 구성하였습니다. 예를 들어 통합 전의 개별 데이터셋이 ‘가’와 ‘나’가 있었고, 데이터셋 ‘가’는 클래스 1, 클래스 2, ‘나’ 데이터셋은 클래스 1, 클래스 2, 클래스 3이 있었다고 하면 통합한 데이터셋은 개별 데이터셋의 종류와 클래스를 모두를 구분하는 5개의 클래스로 구성하는 식입니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/srisc-class-structure.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/srisc-class-structure.png&quot; alt=&quot;통합 데이터셋의 클래스 구성 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;통합 데이터셋의 클래스 구성 예시&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&quot;데이터셋-전처리&quot;&gt;데이터셋 전처리&lt;/h4&gt;
&lt;p&gt;해당 데이터셋을 만들 때 2가지의 문제점이 있었습니다. 첫 번째는 &lt;strong&gt;이미지 크기 다양성&lt;/strong&gt;의 문제입니다. 예를 들면 이미지 크기가 가로x세로 100x100인 이미지부터 2000x2000인 이미지까지 다양한 이미지가 존재하였습니다. 보통 딥러닝 모델의 입력값으로 크기가 동일한 이미지를 받는 경우가 많으므로, 크기가 통일되면 좀 더 편리하게 데이터셋을 사용할 수 있습니다. 이를 해결하기 위해서 일부 데이터셋에 대해 리사이징 및 제로 패딩을 하였고, 지나치게 크고 결함의 크기가 작은 데이터셋 같은 경우에는 결함 정보가 리사이징으로 사라질 수 있기 때문에 패치 크로핑을 이용해서 모든 이미지들을 가로와 세로를 모두 최소 224, 최대 256 사이로 맞추었습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/image_size_distribution.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/image_size_distribution.svg&quot; alt=&quot;전처리 전/후의 이미지 크기(픽셀 수의 제곱근) 분포 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;전처리 전/후의 이미지 크기(픽셀 수의 제곱근) 분포 예시&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;두 번째는 &lt;strong&gt;클래스 불균형&lt;/strong&gt; 문제입니다. 통합된 데이터셋은 적게는 10장 이하의 이미지를 포함하는 클래스부터 많게는 10만 장을 넘어가는 이미지를 포함하고 있는 클래스도 존재하였습니다. 클래스가 이처럼 불균형하게 분포하게 되면 소수에 해당하는 클래스는 학습이 되지 않거나, 학습이 불안정하게 진행되는 경우가 있으므로 이를 완화하는 것이 좋습니다. 해당 문제를 해결하기 위해서 클래스당 이미지 수의 기준값을 정해두고 기준값을 넘는 클래스에 대해서는 기준값만큼의 이미지만 랜덤으로 추출해서 사용하고 나머지는 사용하지 않는 방식의 &lt;a href=&quot;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&quot; target=&quot;_blank&quot;&gt;언더 샘플링(under-sampling)&lt;/a&gt;을 수행하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/class_imbalance.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/class_imbalance.svg&quot; alt=&quot;언더샘플링 전/후의 클래스별 이미지 수 분포 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;언더샘플링 전/후의 클래스별 이미지 수 분포 예시&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&quot;통합-데이터셋-sriscsualab-research-image-single-classification&quot;&gt;통합 데이터셋: SRISC(Sualab Research Image Single Classification)&lt;/h4&gt;
&lt;p&gt;이제 이미지 크기의 다양성이 적고, 클래스가 어느 정도 균형이 잡힌 데이터셋이 생겼습니다. 이를 &lt;strong&gt;SRISC(Sualab Research Images Single Classification)&lt;/strong&gt;라고 지칭하였습니다. 추후 계속 발전되었지만 초기 SRISC는 PCB, 필름, 디스플레이, 태양광, 반도체 등 다양한 산업군의 데이터를  포함하고 있는 대용량 데이터셋이었습니다. 특징으로는 산업용 이미지다 보니 자연계 이미지인 이미지넷에 비해 1 채널 이미지들이 많이 있다는 점이 있었습니다.&lt;/p&gt;

&lt;h3 id=&quot;srisc를-이용한-사전학습&quot;&gt;SRISC를 이용한 사전학습&lt;/h3&gt;
&lt;p&gt;이미지넷을 이용해서 사전학습을 하듯이 완성된 SRISC를 이용해서 사전학습하는 방식을 통해 본격적으로 전이 학습에 활용하였습니다. 기본적으로 소스 데이터셋에서의 각 클래스들을 구분하도록 사전학습한 뒤 새로운 (타겟) 데이터셋 학습 시 모델의 초기값으로 해당 사전학습된 모델을 사용하였습니다. 그리고 사전학습 시 사용하는 소스 데이터셋의 종류와 사전학습 방법에 따른 전이 효과를 알아보기 위해 아래와 같이 5가지 사전학습 방법에 따라 타겟 작업에서의 성능이 어떻게 변화하는지 비교하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;사전학습 하지 않음(from_scratch)&lt;/li&gt;
  &lt;li&gt;이미지넷 사전학습(ImageNet_pre-trained)&lt;/li&gt;
  &lt;li&gt;SRISC 사전학습(SRISC_pre-trained)&lt;/li&gt;
  &lt;li&gt;이미지넷 1차 사전학습 후에 SRISC 2차 사전학습(ImageNet_SRISC_pre-trained)&lt;/li&gt;
  &lt;li&gt;데이터를 통합하여 이미지넷과 SRISC을 동시에 사전학습(ImageNet-SRISC_joint-pre-trained)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사전학습 이후 사전학습한 모델을 이용하여 SRISC에 포함되지 않았던 4개의 새로운 비전 검사 데이터셋에 대한 학습 및 테스트를 하였고, 그 결과는 아래와 같습니다. 타겟 데이터셋의 산업군은 전자부품(electronic component),  음료(beverage), 필름(film), 디스플레이(display)로 다양하게 구성하였습니다. 여러 번 반복 실험한 뒤 &lt;strong&gt;테스트 셋에 대한 오류율(error rate)의 평균&lt;/strong&gt;을 각 사전학습 방법 별로 나타내었습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/target-err_by_pt-method.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/target-err_by_pt-method.svg&quot; alt=&quot;사전학습 방법에 따른 전이 성능(타겟 테스트셋에 대한 오류율)&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;사전학습 방법에 따른 전이 성능(타겟 테스트셋에 대한 오류율)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;결과는 각 타겟 데이터셋 별로 어느정도 차이가 있었지만 &lt;strong&gt;이미지넷과 SRISC를 함께 이용할 경우 이미지넷만을 이용한 경우보다 모든 데이터셋에서 오류율이 줄어드는 결과(그래프에서 붉은색 막대)&lt;/strong&gt;를 보여주었습니다. 또한 SRISC만 이용하는 경우 성능이 이미지넷으로만 사전학습한 경우보다 좋은 경우(전자부품, 음료)도 있었고, 방법적인 측면에서는 이미지넷과 SRISC를 동시에 사전학습하는 것보다는 순차적으로 사전학습하는 방법이 좋은 결과를 보였습니다.&lt;/p&gt;

&lt;p&gt;이러한 결과는 &lt;strong&gt;비전 검사를 위한 데이터셋을 학습할 때는, 같은 비전 검사라는 공통점을 가진 데이터셋을 이용하여 전이 학습하면 성능을 향상할 수 있음&lt;/strong&gt;을 보여주는 결과입니다.  여기서 한발 더 나아가 비전 검사 내 세부 영역으로 더 범위를 좁힌다면 성능이 올라가지 않을까요?&lt;/p&gt;

&lt;h3 id=&quot;sriscpcb를-이용한-사전학습&quot;&gt;SRISCPCB를 이용한 사전학습&lt;/h3&gt;
&lt;p&gt;그리하여 SRISC에서 많은 비중을 차지하고 있던 &lt;strong&gt;PCB&lt;/strong&gt; 산업군의 데이터셋만을 모아서 선별적으로 전이 시 성능이 더 많이 올라가는지 확인해보고자 PCB 산업군에 속하는 데이터셋을 모아서 &lt;strong&gt;SRISCPCB&lt;/strong&gt;를 만들었습니다. SRISCPCB를 만들 때는 클래스 개수가 상대적으로 적었으므로, 클래스 불균형 해결보다 절대적인 데이터의 양을 늘리는데 초점을 맞추기 위해 사용할 수 있는 모든 데이터셋을 사용하였습니다. 그리고 앞 선 실험의 결과를 참고하여 아래 3가지 방법으로 사전학습한 뒤, 각 사전학습된 모델을 초기값으로 5개의 사전학습에 사용되지 않은 타겟 PCB 데이터셋에 대해 학습하여 성능을 측정하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이미지넷으로 사전학습(대조군, ImageNet_pre-trained)&lt;/li&gt;
  &lt;li&gt;이미지넷, SRISC순으로 사전학습(대조군, ImageNet_SRISC_pre-trained)&lt;/li&gt;
  &lt;li&gt;이미지넷, SRISC, SRISCPCB 순으로 사전학습(실험군, ImageNet_SRISC_SRSICPCB_pre-trained)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;역시 타겟 테스트 데이터셋에 대한 오류율로 성능을 표시하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/tl-for-pcb.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/tl-for-pcb.svg&quot; alt=&quot;PCB 도메인 대상 사전학습 방법에 따른 전이 성능(타겟 테스트셋에 대한 오류율)&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;PCB 도메인 대상 사전학습 방법에 따른 전이 성능(타겟 테스트셋에 대한 오류율)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;모든 타겟 PCB 데이터셋에서 SRISCPCB를 이용하여 추가적으로 사전학습하는 경우 성능이 좋아지는 경향&lt;/strong&gt;을 보였습니다. 소스 데이터셋을 더욱 선별적으로 사용한 효과를 본 것입니다.&lt;/p&gt;

&lt;h4 id=&quot;도메인-좁히기의-한계&quot;&gt;도메인 좁히기의 한계&lt;/h4&gt;

&lt;p&gt;그런데 한발 더 나아가서 PCB를 다시한번 하위 도메인으로 나눠 실험하였을 때는 성능의 향상이 없는 결과를 보였습니다. 아마 너무 도메인을 세분화한 나머지 소스 데이터셋 내 전이할 지식의 양이 줄어들었기 때문일 것입니다. 그렇다면 소스 데이터셋을 많이 사용하면 성능이 많이 향상시킬 수 있을 것이라는 생각에 SRISC의 크기를 증가시켜가면서 사전학습 및 파인튜닝 과정의 성능을 측정해본 결과, 성능이 향상되다가 특정 지점 이후 줄어드는 경향을 보였습니다. 원인으로 데이터가 많아지면서 전이에 부정적인 영향을 미치는 데이터셋이 포함되었을 가능성을 생각해볼 수 있습니다. 이렇게 &lt;strong&gt;소스 데이터셋들을 사람의 주관대로 선택하면서 실험을 하다보니, 결론적으로는 전이 시 성능이 좋아질지 아니면 오히려 나빠질지 확신하기 어렵겠다는 생각이 들었습니다.&lt;/strong&gt; 좀 더 객관적인 방법으로 소스 데이터셋을 선택하는 방법이 없을까요?&lt;/p&gt;

&lt;h3 id=&quot;adaptive-transfer-learning&quot;&gt;Adaptive Transfer Learning&lt;/h3&gt;
&lt;p&gt;이미 위와 같은 문제의식을 공유하는 연구들이 있습니다. 이러한 연구들은 소스를 선택하는 기준이나 방식을 연구하는데, 저는 이러한 연구분야를 &lt;strong&gt;적응적 전이 학습(Adaptive Tansfer Learning&lt;/strong&gt;)이라고 지칭하겠습니다. 몇 가지 예시 연구를 열거해보면 아래와 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Garbor filter를 이용하는 등 저수준(low-level)의 피쳐를 이용하여 이미지 간 유사도를 계산해 타겟과 유사한 소스 이미지들을 선택하여 이용하는 방법론 &lt;a href=&quot;https://arxiv.org/pdf/1702.08690.pdf&quot; target=&quot;_blank&quot;&gt;(Ge and Yu, 2017)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;미리 학습된 딥러닝 모델의 피쳐를 이용해서 계산한 클래스 간 유사도를 이용하는 방법론 &lt;a href=&quot;https://arxiv.org/pdf/1806.06193.pdf&quot; target=&quot;_blank&quot;&gt;(Cui et al., 2018)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;소스 선택 자체를 데이터를 기반으로 학습하는 방법론 &lt;a href=&quot;https://arxiv.org/pdf/1805.09622.pdf&quot; target=&quot;_blank&quot;&gt;(Litany and Freedman, 2018)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;본 글에서는 여러 방법론들 중에서 가장 간단한 방식이라고 할 수 있는 아래와 같은 방법론&lt;a href=&quot;https://arxiv.org/pdf/1811.07056.pdf&quot; target=&quot;_blank&quot;&gt;(Ngiam et al., 2018)&lt;/a&gt;을 기반으로 실험을 진행해보았습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/ngiam-2018.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/ngiam-2018.png&quot; alt=&quot;Ngiam et al.(2018)의 Adaptive Transfer Learning 방법론&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Ngiam et al.(2018)의 Adaptive Transfer Learning 방법론&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;해당 방법론은 아래와 같은 4단계로 이루어져 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;미리 소스 데이터셋을 이용해서 학습해놓은 모델을 이용해서 타겟 데이터셋에 대해서 추론(inference)&lt;/li&gt;
  &lt;li&gt;이전 단계에서 얻은 결과에서 확률이 높게 나온 클래스가 높은 가중치를 가지도록 소스 클래스별 가중치(importance weight) 산출&lt;/li&gt;
  &lt;li&gt;이전 단계에서 얻은 가중치를 기반으로 배치 샘플링하여 다시 한번 사전학습&lt;/li&gt;
  &lt;li&gt;이전 단계에서 사전학습한 모델을 최종 타겟 학습의 초기값으로 사용&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;SRISC를 이용한 전이 학습 시나리오를 예를 들면 타겟이 PCB 산업군의 데이터셋이었다면 초기 추론과정에서 타겟과 유사한 소스 데이터의 다른 PCB 데이터셋 클래스로 추론이 될 것이고, 이렇게 추론된 PCB 클래스들을 위주로 사전학습을 하겠다는 의도입니다.&lt;/p&gt;

&lt;p&gt;실험은 기본적으로 이미지넷 사전학습은 공통으로 수행하고 이후 아래 3가지 방법으로 사전학습을 달리해가면서 성능을 측정하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;전체 SRISC의 클래스를 균일하게 사전학습(ImageNet_SIRSC_pre-trained)&lt;/li&gt;
  &lt;li&gt;위 방법론을 이용해서 SRISC에 대해 사전학습(ImageNet_Adaptive-SRISC_pre-trained)의&lt;/li&gt;
  &lt;li&gt;글쓴이 개인적인 직관에 따라 임의로 선택한 클래스들을 이용해서 사전학습(ImageNet_Human-curated-SRISC_pre-trained)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;단, 세번째 항목의 경우, 일부 산업군의 데이터셋은 어떤 클래스들을 골라야 할지 감이 잡히지 않아 고르지 못한 데이터셋도 있습니다. 이렇게 &lt;strong&gt;사람 기준에서 애매한 데이터셋들에 대해서도 적용될 수 있다는 점&lt;/strong&gt;은 여기서 시도해본 방법론을 포함하여 적응적 전이 학습 방법론들의 강점입니다.&lt;/p&gt;

&lt;p&gt;성능은 역시 타겟 테스트 셋의 오류율로 표기하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/ada-tl.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/ada-tl.svg&quot; alt=&quot;Adaptive Transfer Learning의 방법론을 적용하여 사전학습한 경우의 전이 성능&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Adaptive Transfer Learning의 방법론을 적용하여 사전학습한 경우의 전이 성능&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;아쉽게도, 전반적으로 긍정적인 결과는 아니였습니다. SRISC를 단순하게 이용해서 사전학습한 경우보다 오류율이 줄어드는 경우도 있었지만, 높아지는 경우도 있었습니다. SRISC가 아직 데이터를 선택적으로 이용할 만큼 충분한 크기의 소스 데이터셋이 아니였거나, 선정기준이었던 타겟과 유사한 소스 데이터셋이 실제 사전학습을 통해 타겟에서 성능을 올려주는 데이터셋은 아니었을 수 있습니다. 추가적으로 사람의 기준으로 데이터셋을 고를 경우에도 성능이 좋아지는 경우가 있었지만 나빠지는 경우도 있었는데, 이는 사람의 주관이 부정적인 영향을 미친 사례라고 볼 수 있습니다. 아직 적응적 전이 학습, 특히 소스 데이터셋 선정 기준에 대해서는 좀 더 고민해볼 여지가 있는 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;사전학습-방식의-transfer-learning의-한계&quot;&gt;사전학습 방식의 Transfer Learning의 한계&lt;/h3&gt;

&lt;p&gt;앞서 보여드린 실험 결과들에서도 나타나지만 지금까지 보여드린 방식의 전이 학습은 아직 많은 한계점을 가지고 있습니다. 가장 대표적이고 쉽게 생각할 수 있는 것은 이미 타겟 데이터셋이 많은 양을 보유한 경우에는 그 효용이 그렇게 크지 않다는 점입니다. 이를 보여주는 그래프는 아래와 같습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/acc-gain-by-tvsize.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/acc-gain-by-tvsize.svg&quot; alt=&quot;타겟 데이터셋 크기에 따른 전이 학습 적용 후 정확도 이득&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;타겟 데이터셋 크기에 따른 전이 학습 적용 후 정확도 이득&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;타겟 학습 데이터셋 크기에 따른 SRISC를 이용한 전이 학습 적용 후 정확도 이득(Accuracy Gain, 적용 전에 비해 정확도가 얼마나 증가했느냐)을 나타낸 그래프입니다. 확실히 &lt;strong&gt;타겟 데이터셋이 커질수록 전이 학습 방법 차이에 따른 효용은 작아지는 양상&lt;/strong&gt;을 보입니다. 현재 방식의 전이 학습은 초기값의 차이만을 만들어내는데, 많은 타겟 데이터셋을 학습하는 과정에서 그 차이의 효과가 희석되기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning-추후-연구-방향&quot;&gt;Transfer Learning 추후 연구 방향&lt;/h2&gt;

&lt;p&gt;앞서 언급한 한계점을 극복하거나 더 높은 성능 향상을 위해 앞으로 전이 학습 연구를 발전시킬 수 있는 방향은 구체적으로 크게 3가지로 나눌 수 있는데, 글 초입부에서 보여준 그림을 기준으로 각 단계를 나누어서 볼 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/future-work.svg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/transfer-learning-in-sualab/future-work.svg&quot; alt=&quot;전이 학습 추후 연구 방향 도식도&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;전이 학습 추후 연구 방향 도식도&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(1) 첫 번째로는 &lt;strong&gt;소스를 선택하는 과정&lt;/strong&gt;을 지금보다 더 현명하게 할 수 있는 방법을 고안해 볼 수 있습니다. 어떤 소스 데이터셋이 얼마나 타겟에서 효과가 있느냐를 전이성(transferability)이라고 할 때, 전이성의 대변자(proxy)로써 앞서 살펴본 방법론은 미리 소스 데이터셋으로 학습된 모델의 결과를 이용했는데 다른 방법으로 전이성을 측정하여 선택할 수 있을 것입니다. 적응적 전이 학습(Adaptive Transfer Learning) 연구들이 해당 계열에 속한 연구의 대표적인 예시라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;(2) 두 번째는 &lt;strong&gt;전이되는 지식이 무엇이냐&lt;/strong&gt;, 어떻게 전이시키느냐를 달리 해볼 수 있을 것 같습니다. 지금까지 소개된 연구들은 학습된 모델을 지식으로 보고, 사전학습 및 파인튜닝 방식을 이용한 모델 전이 방식이었습니다. 하지만 이 방법 이외의 소스 데이터셋 자체를 지식으로 보고 데이터 자체를 전이하여 조인트 트레이닝(joint-training) 혹은 다중 작업 학습(multi-task learning) 방식으로 타겟 도메인에서 학습하거나, 웨이트가 아닌 아키텍쳐만 옮겨오는 아키텍처 전이 등 다양한 방법이 있을 것 같습니다.&lt;/p&gt;

&lt;p&gt;(3) 마지막으로 &lt;strong&gt;전이해온 지식을 타겟에 적용&lt;/strong&gt;할 때도 다양한 방법을 시도해볼 수 있습니다. 현재까지는 대부분 모든 레이어를 동일한 학습률(learning rate)로 학습하는 방식을 이용하였지만, 일부 레이어를 프리즈 시키거나, 혹은 전이해온 웨이트가 학습을 하면서 초기값에서 크게 변하지 않게 정규화를 해주는 식의 시도도 해볼 수 있다고 생각합니다. &lt;a href=&quot;https://arxiv.org/pdf/1802.01483.pdf&quot; target=&quot;_blank&quot;&gt;(Li et al., 2018)&lt;/a&gt;와 &lt;a href=&quot;https://openreview.net/pdf?id=rkgbwsAcYm&quot;&gt;(Li et al., 2019)&lt;/a&gt;와 같은 연구를 참고할 수 있습니다.&lt;/p&gt;

&lt;p&gt;좀 더 나아가면 이렇게 3가지 이외에 작업 자체를 분류 말고 세그멘테이션, 디텍션 등 이종 작업 간의 전이를 시도해볼 수도 있고, 전이 학습 자체가 포괄적인 의미를 담고 있는 분야라 전이 학습의 하위 이름으로 시도해볼 수 있는 다양한 연구들이 존재합니다.&lt;/p&gt;

&lt;h2 id=&quot;마무리&quot;&gt;마무리&lt;/h2&gt;

&lt;p&gt;여기까지 전이 학습의 개념과 수아랩에서 전이 학습을 연구했던 내용을 간략히 공유해드리고, 앞으로 나아가야 할 연구 방향에 대해서도 이야기해보았습니다. 앞으로도 수아랩의 가장 큰 칼솜씨(기술력) 중 하나인 데이터셋을 이용하기 위한 전이 학습은 계속해서 연구가 될 예정이니 많은 관심 부탁드립니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1702.08690.pdf&quot; target=&quot;_blank&quot;&gt;Ge and Yu. “Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.06193.pdf&quot; target=&quot;_blank&quot;&gt;Cui et al. “Large scale fine-grained categorization and domain-specific transfer learning.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1805.09622.pdf&quot; target=&quot;_blank&quot;&gt;Litany and Freedman. “SOSELETO: A Unified Approach to Transfer Learning and Training with Noisy Labels.” arXiv preprint arXiv:1805.09622 (2018).&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.07056.pdf&quot; target=&quot;_blank&quot;&gt;Ngiam et al. “Domain adaptive transfer learning with specialist models.” arXiv preprint arXiv:1811.07056 (2018)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.01483.pdf&quot; target=&quot;_blank&quot;&gt;Li et al. “Explicit Inductive Bias for Transfer Learning with Convolutional Networks” ICML. 2018.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=rkgbwsAcYm&quot;&gt;Li et al. “DELTA: DEep Learning Transfer using Feature Map with Attention for Convolutional Networks” ICLR. 2019.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;분식집에서 칼을 가는 전직 킬러
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://showbox.co.kr/Movie/LineUpStcut?mvcode=DE150193&quot;&gt;쇼박스 라인업 영화 ‘럭키’ 스틸컷&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이미지넷(ImageNet) 웹사이트
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.image-net.org&quot; target=&quot;_blank&quot;&gt;http://www.image-net.org&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;개 고양이 분류 문제
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://research.sualab.com/practice/2018/01/17/image-classification-deep-learning.html&quot; target=&quot;_blank&quot;&gt;김길호, “이미지 Classification 문제와 딥러닝: AlexNet으로 개vs고양이 분류하기”, SUALAB Research Blog&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pascal VOC Challenge 웹사이트
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://host.robots.ox.ac.uk/pascal/VOC/voc2012/&quot; target=&quot;_blank&quot;&gt;http://host.robots.ox.ac.uk/pascal/VOC/voc2012/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;언더샘플링(undersampling)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&quot; target=&quot;_blank&quot;&gt;https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>기홍도</name><email>hongdo.ki@cognex.com</email></author><category term="Introduction" /><category term="Review" /><category term="transfer learning" /><summary type="html">본 글에서는 전이 학습(transfer learning)이라는 분야에 대해 소개해드리고, 해당 분야를 수아랩이 어떤 식으로 연구하고 있는지 직접 수행한 작업들과 간단한 실험 결과 일부를 공유드리면서, 추후 연구 방향에 대해 이야기해보려고 합니다. 지금까지 수아랩 기술 블로그에서는 다른 연구자의 연구에 대한 소개가 주를 이루었었는데, 처음으로 수아랩에서 직접 연구한 내용을 소개해드리게 되었습니다. 흥미롭게 읽어주시면 감사하겠습니다.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/intro-luckkey.jpg" /><media:content medium="image" url="http://github.sualab.io/intro-luckkey.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Interpretable Machine Learning 개요: (2) 이미지 인식 문제에서의 딥러닝 모델의 주요 해석 방법</title><link href="http://github.sualab.io/introduction/2019/10/23/interpretable-machine-learning-overview-2.html" rel="alternate" type="text/html" title="Interpretable Machine Learning 개요: (2) 이미지 인식 문제에서의 딥러닝 모델의 주요 해석 방법" /><published>2019-10-23T09:00:00+09:00</published><updated>2019-10-23T09:00:00+09:00</updated><id>http://github.sualab.io/introduction/2019/10/23/interpretable-machine-learning-overview-2</id><content type="html" xml:base="http://github.sualab.io/introduction/2019/10/23/interpretable-machine-learning-overview-2.html">&lt;p&gt;앞선 글에서 머신러닝 모델에 대한 해석력 확보를 위한 Interpretable Machine Learning(이하 IML)의 개요를 다뤘습니다. 이번 글에서는 IML에 대한 지금까지의 이해를 바탕으로, 많은 분들이 관심을 가지고 계실 딥러닝 모델에 대한 주요 IML 방법론들에 대해 알아보고자 합니다.&lt;/p&gt;

&lt;p&gt;본래 앞서 소개했던 Post-hoc(Model-agnostic) 계열의 IML 방법론들은 전통적인 머신러닝 모델뿐만 아니라 신경망(neural network)에도 적용될 수 있도록 범용적으로 디자인되어 있었습니다. 그런데 오늘날 딥러닝 모델이 다양한 문제들에 대하여 특출나게 높은 성능을 발휘하게 되면서 다른 종류의 모델보다 집중적인 조명을 받게 되었고, 자연스럽게 깊은 신경망 자체에 대한 해석에 초점을 맞추는 연구들이 하나둘씩 늘어나기 시작했습니다. 이에 따라 2010년대 중반부터는 깊은 신경망의 구조적 특징을 고려한 IML 방법론들이, 전통적인 IML 방법론 계열들과는 별도의 독자적인 흐름을 형성하는 경향을 보여 왔습니다. 특히, 결과 해석에 있어서의 상대적인 용이성 때문에, 이미지 인식(image recognition) 문제에 적용된 딥러닝 모델에 대한 연구가 활발하게 진행되어 왔습니다.&lt;/p&gt;

&lt;p&gt;앞선 글에서 머신러닝 전반에 대한 IML 방법론들의 거시적인 개요에 대해 소개해 드렸다면, 본 글에서는 관심의 대상을 딥러닝으로 좁혀 이미지 인식 문제에서의 딥러닝 모델을 위해 특화된 IML 방법론에 대하여 집중적으로 소개해 드리고자 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;주의: 본 글은 아래와 같은 분들을 대상으로 합니다.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;딥러닝 알고리즘의 기본 구동 원리 및 딥러닝 모델(특히 컨볼루션 신경망)에 대한 기초적인 내용들을 이해하고 계신 분들&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;현재도 딥러닝 모델에 대한 IML 방법론들의 분류 방식이 학계에서 다양하게 존재하며, 아직까지 하나로 완벽하게 정립되지는 않은 상태입니다. 본 글에서는 Google Brain 팀에서 포스팅한 블로그 글 &lt;a href=&quot;https://distill.pub/2017/feature-visualization&quot; target=&quot;_blank&quot;&gt;(1)&lt;/a&gt;, &lt;a href=&quot;https://distill.pub/2018/building-blocks&quot; target=&quot;_blank&quot;&gt;(2)&lt;/a&gt; &lt;small&gt;(Christopher Olah et al.)&lt;/small&gt;에서 소개한 분류 방식을 주로 참조하되, 원활한 내용 전개를 위해 약간의 변형을 가하였음을 알려드립니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;activationweight-visualization&quot;&gt;Activation/Weight Visualization&lt;/h2&gt;

&lt;p&gt;오늘날 이미지 인식 문제에 사용되는 전형적인 딥러닝 모델인 &lt;a href=&quot;http://github.sualab.io/introduction/2017/10/10/what-is-deep-learning-1.html#컨볼루션-신경망&quot; target=&quot;_blank&quot;&gt;컨볼루션 신경망(CNN: convolutional neural network)&lt;/a&gt;을 단순화시키면 아래 그림과 같이 나타낼 수 있습니다. 어느 이미지가 입력되면, 사전에 학습된 복수 개의 layer(층)들을 거치면서 feature extraction(요인 추출)을 수행하여 feature map(요인 맵)들이 생성되고, 이것이 뒤따르는 복수 개의 layer들을 거쳐 확률 형태의 prediction(예측) 결과를 산출하는 구조입니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/image-features-prediction-diagram.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/image-features-prediction-diagram.png&quot; alt=&quot;컨볼루션 신경망의 기본 구성&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;컨볼루션 신경망의 기본 구성&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;컨볼루션 신경망이 특정 예측 결과를 도출하게 된 근거를 해석하기 위해 가장 직관적으로 생각할 수 있는 방법으로, 중간 계산 과정에서 생성된 feature map 각각을 들여다보는 방법이 있습니다. 보통 일반적인 머신러닝 모델에서 feature를 ‘들여다본다’ 함은, feature로 계산된 값의 크기를 비교하고 통계적 분포 등을 조사하는 것이 될 것입니다. 그러나 컨볼루션 신경망의 feature map은 그 특유의 2차원적 구조 때문에 이를 마치 이미지처럼 그려볼 수 있다는 장점이 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/activation-visualization-example.jpeg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/activation-visualization-example.jpeg&quot; alt=&quot;컨볼루션 신경망의 각 layer에서의 activation visualization 결과 예시 &amp;lt;small&amp;gt;(Andrej Karpathy)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;컨볼루션 신경망의 각 layer에서의 activation visualization 결과 예시 &lt;small&gt;(Andrej Karpathy)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 3개 layer들로 구성된 아주 간단한 컨볼루션 신경망을 학습한 뒤, 여기에 자동차가 포함된 이미지를 입력했을 때의 각 layer에서의 feature map들을 이미지 형태로 표시한 결과를 보여주고 있습니다. 특정 이미지를 입력했을 때의 각 feature map이 ‘활성화된(activation)’ 정도를 보여준다는 맥락에서, 이러한 해석 방법을 보통 ‘&lt;strong&gt;Activation Visualization&lt;/strong&gt;(활성값 시각화)’이라고 부릅니다. Activation Visualization을 통해, 여러분들은 학습이 완료된 컨볼루션 신경망의 각 layer에서 어떤 feature map이 이미지 상의 어떠한 특징들을 커버하고 있는지 추측할 수 있습니다. 이를테면 아래 그림과 같이, 사람 또는 동물의 얼굴이 포함되어 있는 이미지들을 입력할 때, 공통적으로 최대로 활성화되는 feature map이 무엇인지 조사하고, 이들을 시각화함으로써 해당 feature map이 실제로 ‘얼굴’을 커버하는 경향을 보이는지 확인할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/face-activation-example.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/face-activation-example.png&quot; alt=&quot;사람 또는 동물의 '얼굴'을 커버하는 feature map의 Activation Visualization 결과 예시 &amp;lt;small&amp;gt;(Jason Yosinski et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;사람 또는 동물의 ‘얼굴’을 커버하는 feature map의 Activation Visualization 결과 예시 &lt;small&gt;(Jason Yosinski et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Activation Visualization의 필수 조건을 꼽는다면, 해석 대상이 되는 컨볼루션 신경망에 반드시 예시 이미지들을 입력해 준 뒤 그 결과의 경향성을 관찰해야 한다는 점이 있습니다. 이러한 조건을 회피하기 위한 방법으로, 학습된 컨볼루션 신경망이 보유한 weights(가중치, 필터) 자체를 그대로 시각화해볼 수 있습니다. 컨볼루션 신경망의 weights 또한 2차원적 구조를 지니기 때문에, feature map처럼 이미지 형태로 그려볼 수 있습니다. 이를 보통 ‘&lt;strong&gt;Weight Visualization&lt;/strong&gt;(가중치 시각화)’이라고 부릅니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/weight-visualization-example.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/weight-visualization-example.png&quot; alt=&quot;컨볼루션 신경망의 1, 2, 3번째 컨볼루션 layer에서의 weight visualization 결과 예시&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;(Jost Tobias Springenberg et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;컨볼루션 신경망의 1, 2, 3번째 컨볼루션 layer에서의 weight visualization 결과 예시&lt;br /&gt;&lt;small&gt;(Jost Tobias Springenberg et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;다만, 여러분이 위 예시 그림을 보면서도 느끼시겠지만, Weight Visualization 결과만을 관찰하는 것만으로는 신경망의 각 weights가 이미지 상의 어떤 시각적 특징을 커버하는지 직관적으로 이해하기가 어렵습니다. 관찰 대상이 되는 layer를 상위 레벨로 이동할수록 더 많은 필터들이 등장하기 때문에, weights에 대한 해석의 난해함은 점점 심해지는 경향을 보입니다. 그렇기 때문에 보통 Weight Visualization의 경우 현재 학습 중인 컨볼루션 신경망의 상태를 점검하기 위한 목적 정도로만 쓰이는 경우가 많으며, 그로부터 그 이상의 시사점을 확인하기는 현실적으로 어렵다고 볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;activation-maximization&quot;&gt;Activation Maximization&lt;/h2&gt;

&lt;p&gt;Activation/Weight Visualization 이후로, 각 feature map이 커버하는 시각적 특징이 정확히 무엇인지 더욱 효과적으로 찾고 가시화하고자 하는 시도가 자연스럽게 늘어나기 시작하였습니다. 그 중 하나로, 컨볼루션 신경망 상의 어느 타겟 출력값을 하나 고정해 놓고, 이를 최대로 활성화하는 입력 이미지를 찾거나 생성하는 방법을 생각해볼 수 있는데, 이를 ‘&lt;strong&gt;Activation Maximization&lt;/strong&gt;(활성값 최대화)’이라고 부릅니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/activation-maximization-concept.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/activation-maximization-concept.png&quot; alt=&quot;Activation Maximization의 기본 컨셉&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;('argmax': 'TARGET'을 최대로 활성화하는 'INPUT' 탐색 또는 생성)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Activation Maximization의 기본 컨셉&lt;br /&gt;&lt;small&gt;(‘argmax’: ‘TARGET’을 최대로 활성화하는 ‘INPUT’ 탐색 또는 생성)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;예를 들어, 위 그림과 같이 어느 feature map 상의 특정 neuron(뉴런)을 조사 타겟으로 설정하고, 이를 &lt;em&gt;최대 수준으로 활성화시키는&lt;/em&gt; 입력 이미지의 형태를 조사하는 것이 가장 대표적인 Activation Maximization이라고 할 수 있습니다. 이때, feature map 상의 특정 neuron 외에도 feature map(=channel) 또는 복수 개의 feature map들을 포괄하는 하나의 layer 등도 타겟으로 설정할 수 있으며, 최종 prediction layer의 특정 logit(로짓) 또한 타겟으로 설정할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/types-of-target-activation.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/types-of-target-activation.png&quot; alt=&quot;컨볼루션 신경망 상의 타겟 출력값 후보 &amp;lt;small&amp;gt;(Christopher Olah et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;컨볼루션 신경망 상의 타겟 출력값 후보 &lt;small&gt;(Christopher Olah et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;maximally-activating-images&quot;&gt;Maximally Activating Images&lt;/h3&gt;

&lt;p&gt;Activation Maximization을 위한 나이브한 방법으로, 컨볼루션 신경망 상의 타겟 출력값을 최대로 활성화하는 입력 이미지들을 현재 가지고 있는 데이터셋 상에서 탐색할 수 있는데, 해당 이미지들을 여기에서는 ‘&lt;strong&gt;Maximally Activating Images&lt;/strong&gt;(활성값 최대화 이미지)’라고 부르겠습니다. 예를 들어, 아래 그림과 같이 타겟 출력값을 어느 하나의 feature map으로 설정할 경우, 데이터셋 상의 이미지들을 하나씩 컨볼루션 신경망에 입력하면서 그 과정에서 해당 feature map을 최대로 활성화시켰던 이미지가 무엇인지를 탐색합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximally-activating-images-concept.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximally-activating-images-concept.png&quot; alt=&quot;Maximally Activating Images 탐색 예시&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;('argmax': 'TARGET'을 최대로 활성화하는 'INPUT'을 데이터셋 상에서 탐색)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Maximally Activating Images 탐색 예시&lt;br /&gt;&lt;small&gt;(‘argmax’: ‘TARGET’을 최대로 활성화하는 ‘INPUT’을 데이터셋 상에서 탐색)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Maximally Activating Images는 각 feature map을 최대로 활성화시키는 입력 이미지를 실제 데이터셋으로부터 탐색한다는 점에서, feature map이 커버하는 현실 이미지 상의 시각적 특징을 좀 더 직접적으로 파악하는 데 도움을 줍니다. 보통은 상위 몇 개의 Maximally Activating Images를 일단 뽑아낸 뒤 이들 각각에 대하여 Activation Visualization을 추가로 수행함으로써, 해당 feature map이 Maximally Activating Images 상의 어느 부분에 초점을 맞추는지 보다 면밀하게 확인할 수 있습니다. 그 결과, 특정 layer 내 각 feature map이 커버하는 현실 이미지 상의 시각적 특징들(e.g. 검은색 원형 부분, 물체의 곡선 경계, 알파벳 글자 등)을 아래 그림과 같이 patch(패치) 형태로 자세히 확인해볼 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximally-activating-patches-example.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximally-activating-patches-example.png&quot; alt=&quot;컨볼루션 신경망의 6, 9번째 컨볼루션 layer 내 각 feature map의 상위 10개 Maximally Activating *Patches* 예시 &amp;lt;small&amp;gt;(Jost Tobias Springenberg et al.)&amp;lt;/small&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;(각 행은 하나의 feature map에 대응되며, 좌측부터 활성값의 내림차순으로 상위 10개 이미지를 나타냄)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;컨볼루션 신경망의 6, 9번째 컨볼루션 layer 내 각 feature map의 상위 10개 Maximally Activating &lt;em&gt;Patches&lt;/em&gt; 예시 &lt;small&gt;(Jost Tobias Springenberg et al.)&lt;/small&gt;&lt;br /&gt;&lt;small&gt;(각 행은 하나의 feature map에 대응되며, 좌측부터 활성값의 내림차순으로 상위 10개 이미지를 나타냄)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;maximization-by-optimization&quot;&gt;Maximization by Optimization&lt;/h3&gt;

&lt;p&gt;Maximally Activating Images는 feature map 등이 커버하는 시각적 특징을 파악하기에는 용이하나, 그 탐색 범위가 현재 보유한 데이터셋 상의 이미지들로 한정된다는 약점이 있습니다. 이에 따라 만약 현재 보유하고 있는 이미지 수가 매우 적다면, Maximally Activating Images를 탐색하였을 시의 효용이 그다지 크지 않을 것이라고 예상할 수 있습니다.&lt;/p&gt;

&lt;p&gt;보유한 데이터셋에 의존하지 않고 feature map 등이 커버하는 시각적 특징을 좀 더 직접적으로 조사하고자, gradient ascent(경사 상승법)에 기반한 optimization(최적화)을 통해 타겟 출력값을 최대로 활성화하는 입력 이미지를 직접 &lt;em&gt;생성&lt;/em&gt;하는 접근이 시도되었습니다. 이러한 방법을 여기에서는 ‘&lt;strong&gt;Maximization by Optimization&lt;/strong&gt;(최적화 기반 최대화)’이라고 부르겠습니다.  &lt;small&gt;(엄밀하게는, 보통 Activation Maximization이라고 하면, 이 Maximization by Optimization 방법을 의미합니다. Christopher Olah et al. 에서는 이를 ‘&lt;a href=&quot;https://distill.pub/2017/feature-visualization&quot; target=&quot;_blank&quot;&gt;Feature Visualization&lt;/a&gt;‘이라고 표현하기도 합니다.)&lt;/small&gt;&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximization-by-optimization-concept.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximization-by-optimization-concept.png&quot; alt=&quot;Maximization by Optimization 예시&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;('argmax': 'TARGET'을 최대로 활성화하는 'INPUT'을 생성)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Maximization by Optimization 예시&lt;br /&gt;&lt;small&gt;(‘argmax’: ‘TARGET’을 최대로 활성화하는 ‘INPUT’을 생성)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;예를 들어, 랜덤 노이즈(random noise) 형태의 이미지 \(X_0\)에서 출발하여, 위 그림과 같이 특정한 하나의 feature map \(f_k\)를 타겟 출력값으로 지정했다고 가정하겠습니다. 현재 이미지 \(X_0\)를 기준으로 feature map \(f_k\)의 이미지 \(X\)에 대한 gradient \(\partial f_{\cdot,\cdot,k} / \partial X\)를 계산하여 이를 현재 이미지에 더해주면, 기존보다 해당 feature map을 더 강하게 활성화시키는 새로운 이미지 \(X_1\)을 얻을 수 있습니다. 매 반복 회차 \(t\)마다 아래의 수식에 따라 이러한 과정을 반복하면서, 전체 반복 횟수 \(T\)를 충분히 많이 가져가면, 해당 feature map을 ‘최대로 활성화시키는 이미지’를 얻을 수 있습니다.&lt;/p&gt;

\[X_{t+1} = X_t + \alpha \cdot \frac{\partial f_{\cdot,\cdot,k}}{\partial X} \quad (t=0,1,...,T-1)\]

&lt;p&gt;아래의 예시 그림들은 타겟 출력값을 서로 다른 feature map으로 설정했을 시 Maximization by Optimization 결과가 달라지는 것을 잘 보여주고 있습니다. 대체로 앞쪽 layer에 위치한 feature map들의 경우 단순하고 반복적인 패턴(edges, textures)을 커버하는 경향을 보이며, 뒷쪽 layer에 위치한 feature map들의 경우 그보다는 좀 더 복잡한 무늬, 모종의 사물의 일부분 또는 전체를 커버하는 경향을 보임을 확인할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximization-by-optimization-on-feature-map-examples.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximization-by-optimization-on-feature-map-examples.png&quot; alt=&quot;타겟 출력값을 서로 다른 feature map으로 설정했을 시의 다양한 Maximization by Optimization 수행 결과 예시 &amp;lt;small&amp;gt;(Christopher Olah et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;타겟 출력값을 서로 다른 feature map으로 설정했을 시의 다양한 Maximization by Optimization 수행 결과 예시 &lt;small&gt;(Christopher Olah et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;만일 Maximization by Optimization의 타겟 출력값을 feature map 대신 layer로 설정할 경우, 아래의 예시 그림들과 같이 상당히 드라마틱한 결과를 얻을 수 있습니다. 이들이 마치 꿈속에서만 등장할 것 같은 생소한 인상을 주었기 때문에, 연구자들은 여기에 ‘&lt;strong&gt;DeepDream&lt;/strong&gt;‘이라는 이름을 붙였습니다. 그림에서도 확인하실 수 있듯이 layer 상의 feature map들에서 커버하는 패턴 또는 모양이 하나로 융합된 듯한 결과물을 생산해 내는데, 해석 가능성의 측면에서 봤을 땐 feature map 각각에 대한 관찰 결과에 비해 다소 난해한 듯한 특징을 보여주고 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/deepdream-result-examples.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/deepdream-result-examples.png&quot; alt=&quot;타겟 출력값을 서로 다른 layer로 설정했을 시의 다양한 Maximization by Optimization(DeepDream) 수행 결과 예시 &amp;lt;small&amp;gt;(DeepDreaming with TensorFlow로 저자가 직접 생성한 결과물)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;타겟 출력값을 서로 다른 layer로 설정했을 시의 다양한 Maximization by Optimization(DeepDream) 수행 결과 예시 &lt;small&gt;(DeepDreaming with TensorFlow로 저자가 직접 생성한 결과물)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;한편, Maximization by Optimization의 타겟 출력값을 최종 prediction layer의 logit으로 설정할 경우, 아래의 예시 그림들과 같이 좀 더 온전한 사물에 가까운 결과를 확인할 수 있습니다. 해당 logit에 대응되는 클래스(class)를 대표하는 ‘전형적인’ 사물을 만들어 냈다고 할 수 있습니다. 이에 대한 관찰을 통해, 각 클래스에 대하여 주어진 학습 데이터셋으로부터 컨볼루션 신경망이 학습을 수행한 결과가 대략 어떤 형태인지 집약적으로 확인해 볼 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximization-by-optimization-on-logit-examples.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/maximization-by-optimization-on-logit-examples.png&quot; alt=&quot;타겟 출력값을 서로 다른 클래스 logit으로 설정했을 시의 다양한 Maximization by Optimization 수행 결과 예시 &amp;lt;small&amp;gt;(Anh Nguyen et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;타겟 출력값을 서로 다른 클래스 logit으로 설정했을 시의 다양한 Maximization by Optimization 수행 결과 예시 &lt;small&gt;(Anh Nguyen et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;attribution&quot;&gt;Attribution&lt;/h2&gt;

&lt;p&gt;지금까지 살펴본 Activation Maximization 방법의 경우, 대부분 컨볼루션 신경망 중간에서 얻어지는 feature 또는 logit 등에 초점을 맞추어 그것들 각각이 이미지 상의 어떤 시각적 특징을 커버하는지 분석하는 것을 목표로 하였습니다. 즉 이는 학습이 완료된 컨볼루션 신경망을 구성하는 요소들 자체의 ‘일반적 행동 방식에 대한 이해’의 목적이 강하다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;반면 지금부터 소개할 ‘&lt;strong&gt;Attribution&lt;/strong&gt;(귀착, 귀속)’의 경우, 어떤 입력 이미지에 대한 컨볼루션 신경망의 예측 결과가 이미지 상의 어느 부분에 기인하였는지 찾기 위한 방법에 해당하며, 이미지 인식 문제에서의 딥러닝 모델의 예측 결과를 ‘설명(explanation)’하는 것을 목적으로 합니다.&lt;/p&gt;

&lt;p&gt;사실 앞서 살펴보았던 Activation Visualization도 컨볼루션 신경망의 예측 결과에 대한 Attribution의 수단으로 활용될 수는 있으나, 아래 그림과 같이 많은 수의 feature map들을 매번 동시적으로 확인해야 한다는 점에서 그다지 매력적이지는 못합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/deep-visualization-example.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/deep-visualization-example.jpg&quot; alt=&quot;Activation Visualization을 통한 예측 결과 설명 예시 &amp;lt;small&amp;gt;(Jason Yosinski et al.)&amp;lt;/small&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;(좌측 상단: 입력 이미지, 중앙: 5번째 컨볼루션 layer의 모든 feature map들에 대한 시각화 결과)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Activation Visualization을 통한 예측 결과 설명 예시 &lt;small&gt;(Jason Yosinski et al.)&lt;/small&gt;&lt;br /&gt;&lt;small&gt;(좌측 상단: 입력 이미지, 중앙: 5번째 컨볼루션 layer의 모든 feature map들에 대한 시각화 결과)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;saliency-map&quot;&gt;Saliency Map&lt;/h3&gt;

&lt;p&gt;컨볼루션 신경망의 Attribution을 보여주기 위한 대표적인 수단이 ‘&lt;strong&gt;Saliency Map&lt;/strong&gt;(현저성 맵)’입니다. 보통 Saliency Map은 이미지 상의 두드러진 부분을 지칭하나, 컨볼루션 신경망의 예측 결과에 대한 설명의 맥락에서는, 예측 결과를 이끌어낸 이미지 상의 주요한 부분을 표현하기 위한 목적으로 생성됩니다.&lt;/p&gt;

&lt;p&gt;컨볼루션 신경망의 예측 결과로부터 Saliency Map을 도출하기 위한 가장 간단한 방법은, 예측 클래스 logit \(y_c\)의 입력 이미지 \(X\)에 대한 gradient \(\partial y_c / \partial X\)를 계산하는 것입니다. 마치 앞서 소개했던 Maximization by Optimization과 유사해 보일 것인데, Maximization by Optimization이 랜덤한 이미지에서 출발하여 feature map의 gradient를 반복적으로 더해주는 gradient ascent를 통해 가상의 이미지를 생성하였다면, Saliency Map의 경우 실제 입력 이미지에 대한 예측 클래스 logit의 gradient를 한 번만 계산하여 이를 그대로 활용한다는 점이 차이라고 할 수 있겠습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/saliency-map-with-gradient-concept.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/saliency-map-with-gradient-concept.png&quot; alt=&quot;예측 클래스에 대한 gradient 계산을 통해 얻어진 Saliency Map 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;예측 클래스에 대한 gradient 계산을 통해 얻어진 Saliency Map 예시&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Saliency Map을 관찰함으로써, 컨볼루션 신경망의 특정 예측 결과가 이미지 상의 어느 부분에 기인하였는지 아래 그림과 같이 확인할 수 있습니다. 뿐만 아니라, 이렇게 얻어진 Saliency Map을 적절하게 가공하여, 이를 &lt;a href=&quot;http://github.sualab.io/introduction/2017/11/29/image-recognition-overview-2.html#segmentation&quot; target=&quot;_blank&quot;&gt;Segmentation 문제&lt;/a&gt;에 적용하는 시도도 이루어지고 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/saliency-map-examples.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/saliency-map-examples.png&quot; alt=&quot;컨볼루션 신경망의 예측 결과에 대한 Saliency Map 도출 결과 예시 &amp;lt;small&amp;gt;(Karen Simonyan et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;컨볼루션 신경망의 예측 결과에 대한 Saliency Map 도출 결과 예시 &lt;small&gt;(Karen Simonyan et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;class-activation-map&quot;&gt;Class Activation Map&lt;/h3&gt;

&lt;p&gt;Attribution을 위해 Saliency Map 외에 많이 사용되는 또 다른 수단으로 ‘&lt;strong&gt;Class Activation Map&lt;/strong&gt;(클래스 활성화 맵)’이 있습니다. 이는 최종 prediction layer 직전에 위치한 layer의 각 feature map에 대하여 &lt;em&gt;global average pooling(GAP)&lt;/em&gt;을 수행하도록 설계된 컨볼루션 신경망에 대하여 범용적으로 적용할 수 있는 Attribution 방법에 해당합니다.&lt;/p&gt;

&lt;p&gt;앞서 보았던 Activation Visualization이 feature map의 시각화 결과를 각각 시각화하는 방법이었다면, Class Activation Map은 prediction layer 직전의 weights를 사용하여 해당 feature map들의 가중합을 계산한 결과물만을 시각화함으로써, 특정 예측 클래스에 대한 전체 feature map들의 ‘평균적인’ 활성화 결과를 확인하는 방법이라고 할 수 있습니다. Class Activation Map을 제안한 저자&lt;small&gt;(Bolei Zhou et al.)&lt;/small&gt;의 논문에 수록된 아래 그림을 통해, 이러한 컨셉을 한눈에 이해할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/class-activation-mapping.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/class-activation-mapping.png&quot; alt=&quot;예측 클래스에 대해 얻어진 Class Activation Map 예시 &amp;lt;small&amp;gt;(Bolei Zhou et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;예측 클래스에 대해 얻어진 Class Activation Map 예시 &lt;small&gt;(Bolei Zhou et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Saliency Map이 입력 이미지 상에서 Attribution을 수행하여 다소 산개된 점 형태의 결과물을 도출한다면, Class Activation Map은 컨볼루션 layer 상에서 Attribution을 수행하기 때문에 상대적으로 부드러운 Attribution 결과를 보여준다는 특징이 있습니다. 아래 그림을 통해 이러한 특징을 엿볼 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/class-activation-map-examples.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/class-activation-map-examples.png&quot; alt=&quot;컨볼루션 신경망의 예측 결과에 대한 Class Activation Map 도출 결과 예시 &amp;lt;small&amp;gt;(Bolei Zhou et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;컨볼루션 신경망의 예측 결과에 대한 Class Activation Map 도출 결과 예시 &lt;small&gt;(Bolei Zhou et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;dataset-visualization&quot;&gt;Dataset Visualization&lt;/h2&gt;

&lt;p&gt;Attribution이 단일 입력 이미지에 대한 컨볼루션 신경망의 예측 결과에 대한 설명을 제공한다면, ‘&lt;strong&gt;Dataset Visualization&lt;/strong&gt;(데이터셋 시각화)’은 데이터셋 상에 포함된 전체 이미지들에 대한 컨볼루션 신경망의 예측 결과의 일반적 경향성에 대한 설명을 제공합니다. 예를 들어, 아래 그림과 같이 하나의 컨볼루션 layer를 관찰 대상으로 고정해 놓고, 데이터셋 상의 이미지들을 하나씩 입력하여 이들 각각에 대한 feature map들을 산출한 뒤, 여기에 &lt;em&gt;Dimensionality Reduction&lt;/em&gt;(차원 축소) 방법을 적용하여 2차원(2D) 또는 3차원(3D) feature space(요인 공간) 상의 점으로 도시할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/dataset-visualization-concept.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/dataset-visualization-concept.png&quot; alt=&quot;컨볼루션 layer 상의 feature maps의 2D feature space로의 Dataset Visualization 예시&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;컨볼루션 layer 상의 feature maps의 2D feature space로의 Dataset Visualization 예시&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Dataset Visualization을 위한 대표적인 Dimensionality Reduction 방법으로는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot; target=&quot;_blank&quot;&gt;PCA(principal component analysis)&lt;/a&gt;, &lt;a href=&quot;http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&quot; target=&quot;_blank&quot;&gt;t-SNE(t-distributed stochastic neighbor embedding)&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/1802.03426&quot; target=&quot;_blank&quot;&gt;UMAP(uniform manifold approximation and projection)&lt;/a&gt; 등이 있습니다. 이들은 컨볼루션 신경망의 feature maps 뿐만 아니라, 다른 high-dimensional(고차원) 데이터의 시각화를 위해서도 범용적으로 적용할 수 있는 Dimensionality Reduction 방법에 해당합니다. 예를 들어 &lt;a href=&quot;http://github.sualab.io/introduction/2017/11/29/image-recognition-overview-1.html#인간의-인식-성능을-좇기-위한-도전&quot; target=&quot;_blank&quot;&gt;MNIST 데이터셋&lt;/a&gt;으로 학습한 컨볼루션 신경망에 대하여 t-SNE를 적용할 경우, 아래 그림과 같은 형태의 Dataset Visualization 결과를 도출할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/tsne-visualization-example.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/tsne-visualization-example.png&quot; alt=&quot;MNIST 데이터셋으로 학습한 컨볼루션 신경망의 t-SNE Visualization (2D) 결과를 원본 이미지로 표현한 예시 &amp;lt;small&amp;gt;(Laurens van der Maaten and Geoffrey Hinton)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;MNIST 데이터셋으로 학습한 컨볼루션 신경망의 t-SNE Visualization (2D) 결과를 원본 이미지로 표현한 예시 &lt;small&gt;(Laurens van der Maaten and Geoffrey Hinton)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;혹은, &lt;a href=&quot;http://www.image-net.org/challenges/LSVRC/2014/&quot; target=&quot;_blank&quot;&gt;ILSVRC&lt;/a&gt;와 같은 데이터셋 상의 자연계 이미지들로 학습한 컨볼루션 신경망에 대해서도 동일한 방법으로 t-SNE를 적용하고, 이를 통해 얻어진 2D feature space 상의 공간적 위치에 의거하여, 이미지들을 이에 맞게 균일한 간격으로 배치한 아래와 같은 Dataset Visualization 결과도 도출할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/cnn-embed-full-1k.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-2/cnn-embed-full-1k.jpg&quot; alt=&quot;ILSVRC 데이터셋으로 학습한 컨볼루션 신경망의 t-SNE Visualization (2D) 결과를 원본 이미지로 표현한 예시 &amp;lt;small&amp;gt;(Andrej Karpathy)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;ILSVRC 데이터셋으로 학습한 컨볼루션 신경망의 t-SNE Visualization (2D) 결과를 원본 이미지로 표현한 예시 &lt;small&gt;(Andrej Karpathy)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;MNIST 데이터셋과 ILSVRC 데이터셋에 대한 Dataset Visualization 결과 모두에서, 대체로 동일하거나 유사한 클래스의 이미지들이 공간적으로 서로 모여 군집(cluster)을 이루고 있는 경향을 확인할 수 있습니다. 이와 같이 Dataset Visualization을 통해, 데이터셋 상에 포함된 전체 이미지들에 대한 컨볼루션 신경망의 예측 결과의 전반적인 경향성 및 각 예측 결과들 간의 거리 관계 등을 한눈에 확인할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;결론&quot;&gt;결론&lt;/h2&gt;

&lt;p&gt;지금까지 이미지 인식 문제에 적용된 딥러닝 모델, 즉 컨볼루션 신경망에 대한 대표적인 IML 방법론들을 확인해 보았습니다. 컨볼루션 신경망의 예측 결과에 대한 해석을 위한 가장 단순하고 직관적인 방법으로 feature map을 직접 이미지 형태로 시각화하는 Activation Visualization이 있는데, 늘 한 번에 많은 수의 feature map들을 동시에 관찰하면서 각각이 커버하는 시각적 특징이 무엇인지 추정해야 한다는 단점이 존재함을 확인하였습니다. 한편 학습된 컨볼루션 신경망의 weights 자체를 시각화하는 Weight Visualization의 경우, 예시 이미지들을 입력해 줄 필요가 없다는 장점이 있으나 그 결과에 대한 해석이 다소 난해하다는 문제가 있었습니다.&lt;/p&gt;

&lt;p&gt;Activation Maximization은 컨볼루션 신경망의 다양한 중간 출력값들이 커버하는 시각적 특징을 좀 더 효과적으로 확인할 수 있도록 하는 방법입니다. 컨볼루션 신경망 상의 특정 타겟 출력값을 최대로 활성화하는 입력 이미지들을 현재 가지고 있는 데이터셋 상에서 ‘탐색’하여 Maximally Activating Images를 얻거나, 혹은 gradient ascent에 기반한 optimization을 통해 이를 직접 ‘생성’하는 Maximization by Optimization을 시도할 수 있음을 확인하였습니다. 관심의 대상이 되는 타겟 출력값으로는 neuron, feature map(=channel), layer 혹은 prediction layer의 logit 등이 될 수 있으며, 이를 어떻게 설정하느냐에 따라 Activation Maximization 수행 결과가 크게 달라짐을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;반면 컨볼루션 신경망의 중간 출력값보다는 예측 결과 자체에 집중하여 여기에 대한 ‘설명’을 제공하기 위한 방법으로 Attribution이 있습니다. Saliency Map은 예측 클래스 logit의 입력 이미지에 대한 gradient를 계산하여 생성해 낸 Attribution 수단으로, 이를 관찰함으로써 컨볼루션 신경망의 특정 예측 결과가 이미지 상의 어느 부분에 기인하였는지 가시적으로 확인할 수 있습니다. 한편 Class Activation Map은 컨볼루션 layer의 feature map들에 대한 가중합을 계산하는 방식을 통해 Attribution 결과를 생성해 낸 것으로, 좀 더 부드러운 Attribution 결과를 보여준다는 특징이 있습니다.&lt;/p&gt;

&lt;p&gt;만일 어느 데이터셋 상의 전체 이미지들에 대한 컨볼루션 신경망의 예측 결과들을 조망하고 이들의 일반적 경향성을 확인하고 싶은 경우, PCA, t-SNE, UMAP과 같은 Dimensionality Reduction 방법을 적용하여 Dataset Visualization을 시도해볼 수 있습니다. 이를 통해 컨볼루션 신경망이 주어진 데이터셋에 대하여 좀 더 거시적인 맥락에서 어떻게 동작하도록 학습되었는지 간접적으로 파악할 수 있음을 확인하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jason Yosinski et al. Understanding neural networks through deep visualization.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1506.06579&quot; target=&quot;_blank&quot;&gt;Yosinski, Jason, et al. “Understanding neural networks through deep visualization.” arXiv preprint arXiv:1506.06579 (2015).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jost Tobias Springenberg et al. Striving for Simplicity: The All Convolutional Net.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1412.6806.pdf%20(http://arxiv.org/pdf/1412.6806.pdf)&quot; target=&quot;_blank&quot;&gt;Springenberg, Jost Tobias, et al. “Striving for simplicity: The all convolutional net.” arXiv preprint arXiv:1412.6806 (2014).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Karen Simonyan et al. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1312.6034.pdf&quot; target=&quot;_blank&quot;&gt;Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. “Deep inside convolutional networks: Visualising image classification models and saliency maps.” arXiv preprint arXiv:1312.6034 (2013).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Anh Nguyen et al. Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1602.03616&quot; target=&quot;_blank&quot;&gt;Nguyen, Anh, Jason Yosinski, and Jeff Clune. “Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks.” arXiv preprint arXiv:1602.03616 (2016).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bolei Zhou et al. Learning Deep Features for Discriminative Localization.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&quot; target=&quot;_blank&quot;&gt;Zhou, Bolei, et al. “Learning deep features for discriminative localization.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;David Bau et al. Network Dissection: Quantifying Interpretability of Deep Visual Representations.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2017/papers/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.pdf&quot; target=&quot;_blank&quot;&gt;Bau, David, et al. “Network dissection: Quantifying interpretability of deep visual representations.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Google Brain 팀에서 포스팅한 블로그 글 (1), (2)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://distill.pub/2017/feature-visualization&quot; target=&quot;_blank&quot;&gt;Christopher Olah, et al., “Feature Visualization”, Distill, 2017.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://distill.pub/2018/building-blocks&quot; target=&quot;_blank&quot;&gt;Christopher Olah, et al., “The Building Blocks of Interpretability”, Distill, 2018.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;컨볼루션 신경망의 각 layer에서의 activation visualization 결과 예시, ILSVRC 데이터셋에 대한 컨볼루션 신경망의 t-SNE Visualization (2D) 결과를 원본 이미지로 표현한 예시
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/convolutional-networks&quot; target=&quot;_blank&quot;&gt;Andrej Karpathy, “Convolutional Neural Networks for Visual Recognition.” Stanford University, http://cs231n.github.io/convolutional-networks. Accessed 5 October 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;타겟 출력값을 서로 다른 layer로 설정했을 시의 서로 다른 Maximization by Optimization(DeepDream) 수행 결과 예시
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/r1/deepdream.ipynb&quot; target=&quot;_blank&quot;&gt;DeepDreaming with TensorFlow&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PCA(principal component analysis)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot; target=&quot;_blank&quot;&gt;Wikipedia contributors. “Principal component analysis.” Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 9 Oct. 2019. Web. 22 Oct. 2019. &lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;t-SNE(t-distributed stochastic neighbor embedding)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&quot; target=&quot;_blank&quot;&gt;Maaten, Laurens van der, and Geoffrey Hinton. “Visualizing data using t-SNE.” Journal of machine learning research 9.Nov (2008): 2579-2605.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;UMAP(uniform manifold approximation and projection)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.03426&quot; target=&quot;_blank&quot;&gt;McInnes, Leland, John Healy, and James Melville. “Umap: Uniform manifold approximation and projection for dimension reduction.” arXiv preprint arXiv:1802.03426 (2018).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>김길호</name><email>Kyle.Kim@cognex.com</email></author><category term="Introduction" /><category term="interpretable machine learning" /><category term="interpretability" /><category term="explainable artificial intelligence" /><summary type="html">앞선 글에서 머신러닝 모델에 대한 해석력 확보를 위한 Interpretable Machine Learning(이하 IML)의 개요를 다뤘습니다. 이번 글에서는 IML에 대한 지금까지의 이해를 바탕으로, 많은 분들이 관심을 가지고 계실 딥러닝 모델에 대한 주요 IML 방법론들에 대해 알아보고자 합니다.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/saliency-map-examples.png" /><media:content medium="image" url="http://github.sualab.io/saliency-map-examples.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Interpretable Machine Learning 개요: (1) 머신러닝 모델에 대한 해석력 확보를 위한 방법</title><link href="http://github.sualab.io/introduction/2019/08/30/interpretable-machine-learning-overview-1.html" rel="alternate" type="text/html" title="Interpretable Machine Learning 개요: (1) 머신러닝 모델에 대한 해석력 확보를 위한 방법" /><published>2019-08-30T09:00:00+09:00</published><updated>2019-08-30T09:00:00+09:00</updated><id>http://github.sualab.io/introduction/2019/08/30/interpretable-machine-learning-overview-1</id><content type="html" xml:base="http://github.sualab.io/introduction/2019/08/30/interpretable-machine-learning-overview-1.html">&lt;p&gt;지금까지의 포스팅을 통해, 수아랩 블로그에서는 다양한 문제 상황에 대하여 동작하는 딥러닝 모델을 직접 제작하고 학습해 왔습니다. 다만 대부분 맨 마지막 과정에서 학습이 완료된 모델을 테스트하는데, 일정 크기의 테스트 데이터셋에 대한 모델의 예측 결과를 바탕으로 정확도(accuracy)와 같이 하나의 숫자로 표현되는 정량적 지표의 값을 계산하고, 그 크기를 보아 ‘딥러닝 모델이 예측을 얼마나 잘 했는지’를 판단하였습니다. 여기에 덧붙여 보완적으로, 올바르게 예측하였거나 잘못 예측한 예시 결과들을 몇 개 샘플링하여 관찰하고, 모델의 예측 결과에 대한 근거를 ‘추측’한 바 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 단순한 정량적 지표 및 샘플링된 예측 결과들 일부를 관찰하는 것만으로, 딥러닝 모델이 예측을 수행하는 자세한 과정에 대하여 완전히 ‘이해’하였다고 할 수 있을까요? 또, 테스트 데이터셋에서 다뤄지지 않은 보다 특수한 상황에서도 딥러닝 모델이 늘 예측을 올바로 수행할 것이라고 충분히 ‘신뢰’할 수 있을까요? 딥러닝의 폭발적인 성능에 매료되어 다양한 문제에 대하여 성능 향상을 끊임없이 추구해 오던 딥러닝 연구자들은, 수 년 전부터 서서히 ‘이해’와 ‘신뢰’ 확보를 위한 연구로 관심을 돌리기 시작했습니다.&lt;/p&gt;

&lt;p&gt;이러한 목표를 보통 &lt;strong&gt;해석력(interpretability)&lt;/strong&gt;이라는 한 단어로 표현하며, 사람의 해석이 가능하도록 하여 이해와 신뢰를 만들어 내기 위한 머신러닝 연구 분야를 &lt;strong&gt;interpretable machine learning&lt;/strong&gt;(이하 IML)이라고 부릅니다. 본 글의 1편에서는 현재까지 머신러닝 관련 학계 전반에서 고찰하고 정립해 온 IML의 필요성, 분류 기준 및 요건 등에 대하여 언급하고자 하며, 2편에서는 딥러닝 모델인 신경망의 해석력 확보를 위한 대표적인 방법론들을 안내해 드릴 예정입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;주의: 본 글은 아래와 같은 분들을 대상으로 합니다.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;머신러닝 알고리즘의 기본 구동 원리 및 주요 머신러닝 모델(e.g. 의사 결정 나무)의 기본적인 특징을 알고 계신 분들&lt;/li&gt;
      &lt;li&gt;딥러닝 알고리즘의 기본 구동 원리 및 딥러닝 모델에 대한 기초적인 내용들을 이해하고 계신 분들&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;서론&quot;&gt;서론&lt;/h2&gt;

&lt;p&gt;이 글을 읽고 계시는 독자 여러분들 중 대부분은, 한 번쯤 점(占)을 본 적이 있으실 거라고 생각합니다. 머신러닝 모델에 대한 이해와 신뢰의 문제를 이해하기 위해, 예전에 여러분들이 점을 봤던 기억을 잠시 떠올려 보도록 하겠습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/tarot-cards.jpeg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/tarot-cards.jpeg&quot; alt=&quot;점술의 예시: 타로 카드(Tarot card)를 통한 예측&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;점술의 예시: 타로 카드(Tarot card)를 통한 예측&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;보통 점술가는 그 장르(?)에 따라 특정한 매개물(동양은 관상이나 사주, 서양은 수정구 또는 타로 카드 등)을 적절히 활용, 이를 통해 관찰된 결과에 기반하여 여러분의 현재 또는 미래 상태에 대한 예측을 수행합니다. 이 과정에서 점술가는 매개물을 통해 도출한 예측의 근거를 화려한 언변을 섞어 설명하며, 이를 듣는 여러분들은 “오, 그럴싸한데?” 라고 생각하면서 점술가의 말을 계속 들어 나갔을 것입니다.&lt;/p&gt;

&lt;p&gt;점을 보는 과정에서 여러분이 은연 중에 중요하게 생각했던 것은 무엇이었을까요? 당연히 ‘예측의 정확성’을 일차적으로 중요하게 생각했을 것이나, 그 배경에는 ‘그럴싸한 설명’이 반드시 수반되었음을 간과하기 어려울 것입니다. 보통 활용하는 매개물이 기본적으로 상징하는 바에서 출발하여, 뭔가 이성적인 것처럼 보이는 근거를 덧붙이고, 여기에 일부 감성을 자극하는 스토리텔링까지 추가할 수록, 점술가의 예측에 대한 여러분들의 이해와 신뢰의 정도는 일반적으로 높아집니다. 심지어는 예측 결과가 틀린 것으로 밝혀졌더라도, 관대한 누군가는 (설명 과정이 너무나도 그럴싸했으므로) “다음 예측 때는 맞추겠지..” 하는 식의 반응을 보일 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;단적인 반대 예시로, 온 눈과 귀를 가린 점술가가 아무런 설명 없이 여러분의 현재 신상을 하나도 틀림 없이 예측했다면, 여러분은 그 점술가를 신뢰하기보다는 “나를 뒤에서 몰래 스토킹했나?” 하는 생각에 도리어 의심할 가능성이 높습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/tarot-card-reading.gif&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/tarot-card-reading.gif&quot; alt=&quot;타로 카드의 기본적인 상징성에 기반한 설명&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;(실제 점술가들은 바로 이 지점부터 본격적으로 썰(?)을 풀기 시작하면서, 고객에게 이해와 신뢰를 심어주게 됩니다..)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;타로 카드의 기본적인 상징성에 기반한 설명&lt;br /&gt;&lt;small&gt;(실제 점술가들은 바로 이 지점부터 본격적으로 썰(?)을 풀기 시작하면서, 고객에게 이해와 신뢰를 심어주게 됩니다..)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;정리하자면, 점술가의 예측에 대한 여러분의 이해와 신뢰를 만들어내기 위해서는, 예측 결과에 대하여 여러분이 납득할 수 있는 적절한 &lt;em&gt;설명&lt;/em&gt;이 필요하다고 할 수 있겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;머신러닝-모델에-대한-설명을-통한-해석력-확보의-필요성&quot;&gt;머신러닝 모델에 대한 설명을 통한 해석력 확보의 필요성&lt;/h3&gt;

&lt;p&gt;그런데 흥미롭게도, 어느 머신러닝 모델의 예측 결과를 이해하고 신뢰할 수 있는가에 대한 이슈도, 점술가의 예측 결과에 대한 이해와 신뢰 가능성의 이슈와 연관지어 생각해 보면 크게 다르지 않습니다. 예를 들어, 일반화 성능을 담보하기 위해 따로 정해놓은 테스트 데이터셋에 대하여, 학습된 머신러닝 모델의 예측 정확도가 거의 100%를 달성하였다고 하더라도, ‘이 모델이 일반적인 상황에서도 지금과 같이 잘 작동할 것이다’는 신뢰를 사용자가 가지기 위해서는, 보통의 사용자가 납득할 만한 근거를 가지고 예측을 수행했다는 ‘&lt;strong&gt;설명(explanation)&lt;/strong&gt;‘을 사용자에게 적절한 형태로 제공해 줘야 합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/explaining-prediction-of-model.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/explaining-prediction-of-model.png&quot; alt=&quot;머신러닝 모델의 예측 결과에 대한 적절한 설명의 예시 &amp;lt;small&amp;gt;(Marco T. Ribeiro et al.)&amp;lt;/small&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;('Explanation'의 초록색으로 표시된 요인들은 '독감' 예측을 지지하는 근거,&amp;lt;br&amp;gt;빨간색으로 표시된 요인들은 '독감' 예측과는 반대되는 근거를 나타냄)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;머신러닝 모델의 예측 결과에 대한 적절한 설명의 예시 &lt;small&gt;(Marco T. Ribeiro et al.)&lt;/small&gt;&lt;br /&gt;&lt;small&gt;(‘Explanation’의 초록색으로 표시된 요인들은 ‘독감’ 예측을 지지하는 근거,&lt;br /&gt;빨간색으로 표시된 요인들은 ‘독감’ 예측과는 반대되는 근거를 나타냄)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;예측 결과에 대한 적절한 설명이라고 한다면, 이를테면 위 그림에서 나온 사례를 들 수 있습니다. 환자의 여러 기본 정보 및 각종 증상 발생 여부 등을 포함하는 데이터를 기반으로 하여 해당 환자의 독감 발병 여부를 예측하는 모델이 있다고 합시다. 이 모델은 지도 학습(supervised learning)에 기반하여 학습되었기 때문에 {‘독감’, ‘독감X’} 따위의 예측 결과를 출력할 뿐, 그 과정에서 어떠한 요인들(features)을 근거로 삼았는지를 보통 직접적으로 보여 주지는 않습니다.&lt;/p&gt;

&lt;p&gt;모델의 예측 결과를 참조하여 진단을 해야 하는 의사의 입장에서는, 이 모델의 예측 정확도가 수치적으로 높다고는 하더라도, 예측에 대한 근거를 함께 받지 않은 상황에서 이를 마냥 신뢰하기가 어려울 것이라고 짐작할 수 있습니다. 그러나 만약 머신러닝 모델의 예측 결과의 주요한 근거가 무엇이었는지 설명을 받을 수 있다면, 의사 입장에서는 예측 결과가 납득할 만한 과정을 거쳐 도출되었다는 것을 확인함으로써 해당 예측 결과에 대한 신뢰를 가질 수 있습니다. 물론 이는 진단 결과를 전달받게 되는 환자의 입장에서도 마찬가지로 적용됩니다. 위 사례와 같이, 상용화 및 대중화를 염두에 두고 있으면서 머신러닝 기술이 핵심이 되는 제품을 출시하고자 할 수록, 이러한 적절한 설명의 중요성은 더 높아질 것이라고 짐작할 수 있습니다.&lt;/p&gt;

&lt;p&gt;한편, 머신러닝 모델의 예측 결과에 대한 설명이 제공될 경우, 단순히 제품 사용자에게 신뢰를 주는 것 외에도 다양한 상황에서 효용을 가져올 수 있습니다. 한 가지 예로, 머신러닝 모델을 개발하는 과정에서, 해결하고자 하는 문제의 핵심과 무관한 부분을 모델이 집중적으로 관찰하여 예측하는지에 대한 ‘&lt;em&gt;디버깅(debugging)&lt;/em&gt;‘을 수행함으로써, 모델의 일반화 성능 향상을 유도할 수 있습니다. 이러한 상황은 생각보다 빈번하게 발생하는데, 학습 데이터셋 상에 사람이 인지하기 어려운 편향(bias)이 존재할 경우 여기에 기인하여 발생합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/explaining-email-classification.gif&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;full-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/explaining-email-classification.gif&quot; alt=&quot;이메일 분류 문제에서의 예측 결과에 대한 설명 제시 사례 &amp;lt;small&amp;gt;(Marco T. Ribeiro et al.)&amp;lt;/small&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;(좌측: Algorithm 1, 이메일의 제목 및 내용을 보고 분류;&amp;lt;br&amp;gt;우측: Algorithm 2, 이메일의 제목/내용과 무관한 헤더를 보고 분류)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;이메일 분류 문제에서의 예측 결과에 대한 설명 제시 사례 &lt;small&gt;(Marco T. Ribeiro et al.)&lt;/small&gt;&lt;br /&gt;&lt;small&gt;(좌측: Algorithm 1, 이메일의 제목 및 내용을 보고 분류;&lt;br /&gt;우측: Algorithm 2, 이메일의 제목/내용과 무관한 헤더를 보고 분류)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 어느 주어진 이메일에 대하여, 그 주제가 ‘Christianity(기독교)’ 또는 ‘Atheism(무신론)’ 중 무엇에 대한 것인지 분류하도록, 서로 다른 2개의 알고리즘을 적용한 머신러닝 모델을 학습한 뒤 테스트한 결과를 하나 보여주고 있습니다. 실제 클래스가 ‘Athiesm’인 어느 이메일에 대하여, Algorithm 1을 적용한 모델과 Algorithm 2를 적용한 모델은 둘 모두 ‘Athiesm’으로 예측하였고, 정/오답 측면에서 보면 둘 모두 정답을 맞혔다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;그러나 이 때 각 모델이 이메일 상의 어느 부분에 집중하여 예측을 수행했는지에 대한 설명이 제시되면, 두 모델 간의 우열이 명확하게 갈립니다. 학습된 모델로 하여금 사용자가 기대했던 것은 이메일의 제목 또는 내용을 보고 이메일의 주제를 분류하는 것이었을 거고, (비록 예측 결과는 동일하게 나왔더라도) 이를 제대로 수행한 것은 Algorithm 1을 적용한 모델임을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;게다가, 머신러닝 모델을 적절하게 설명하는 것은 ‘&lt;em&gt;인류의 새로운 발견과 지식 축적&lt;/em&gt;‘을 위해서도 도움을 줄 수 있습니다. 사람의 어느 특정 작업에 대한 입력값 및 기대 출력값만을 주입하여 학습시킨 모델에 대하여, 그 행동 방식에 대한 설명을 함께 받을 수 있다면, 해당 작업을 해결하는 데 있어 사람이 그 전까지 알지 못했던 새로운 방법 또는 사실을 발견하게 될 것이고, 그것이 거듭될 수록 다양한 방면에서의 인류의 지식을 점진적으로 증진시키는 결과를 기대할 수 있습니다. 이건 좀 너무 나간 주장이 아닌가 싶으실텐데, 이를 입증할 만한 사례는 가까운 곳에서 찾을 수 있습니다. 지난 2016년 등장한 바둑 두는 기계인 ‘알파고(AlphaGo)’와 한국의 이세돌 9단의 대국 이벤트를 생각해 보면 됩니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/alphago-vs-lee-sedol.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/alphago-vs-lee-sedol.jpg&quot; alt=&quot;알파고의 수를 복기하면서 힘겨워하는 이세돌 9단&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;알파고의 수를 복기하면서 힘겨워하는 이세돌 9단&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;알파고는 단지 딥러닝(정확히는 딥 강화 학습)에 기반하여, 매 차례 현재의 바둑판의 상태를 읽어들인 뒤 최적의 다음 착수 위치만을 출력하도록 학습되었기 때문에, 단순히 그 출력 결과들만을 보고 사람이 그 속에 담긴 ‘전략’ 내지는 ‘의도’를 파악해 내는 것은 매우 어려운 일입니다. 이러한 이유 때문에, 이세돌 9단도 거듭되는 패배 속 지난 대국에 대한 복기를 하는 과정에서 깊은 어려움을 토로한 바 있습니다.&lt;/p&gt;

&lt;p&gt;그러나 착수 결과에 대한 적절한 설명이 부재하였음에도 불구하고, 이후 바둑계에서는 알파고의 수들에 대한 나름의 분석들을 통해 기존과는 다른 새로운 패러다임을 서서히 정립해 나가기 시작하였고, 심지어는 바둑 대회의 규칙 및 프로 바둑 기사의 역할 등에 있어서도 변화를 유발하는 계기가 되었다고 합니다(&lt;a href=&quot;https://news.joins.com/article/21352123&quot; target=&quot;_blank&quot;&gt;관련 기사&lt;/a&gt;). 만약 알파고의 착수 결과에 대하여 바둑 기사들이 이해할 수 있는 설명이 제공되었다면(이를테면, 무슨 의도로 해당 수를 두었는지), 바둑계의 이러한 변화는 더 크고 빠르게 찾아왔을지도 모릅니다.&lt;/p&gt;

&lt;h3 id=&quot;머신러닝-모델에-대한-설명의-어려움&quot;&gt;머신러닝 모델에 대한 설명의 어려움&lt;/h3&gt;

&lt;p&gt;그러나 문제는, &lt;a href=&quot;http://github.sualab.io/introduction/2017/09/04/what-is-machine-learning.html#러닝-모델&quot; target=&quot;_blank&quot;&gt;머신러닝 모델의 함수적 특성&lt;/a&gt; 상, 어느 입력값이 주어지면 그 입력에 부합하는 최적의 출력값을 산출하는 데에만 집중하여 내부 수식이 변화할 뿐, 그 과정에서 일반 사용자의 눈에 그럴싸한 설명을 제공하기 위한 그 어떠한 노력(?)도 거치지 않는다는 것입니다(함수는 단지 숫자를 받아 숫자를 출력할 뿐입니다). 특히, 보다 ‘깊은’ 딥러닝 모델 구조로 갈 수록 함수 자체는 점점 더 복잡해지게 되고, 그것의 예측 결과에 대한 그럴싸한 설명은 더욱 어려워지는 경향을 보입니다. 머신러닝 모델의 이러한 특징을 흔히 ‘black box’라고 표현하며, 이러한 맥락에서 머신러닝 모델을 ‘black box model’로 부르기도 합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/deep-neural-network-as-black-box.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/deep-neural-network-as-black-box.png&quot; alt=&quot;딥러닝 모델의 black box적 속성&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;딥러닝 모델의 black box적 속성&lt;/span&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;다시금 점(占)에 비유하자면, 딥러닝 모델은 ‘예측 능력은 아주 우수한데, 설명 능력이 최악인 점술가’라고 할 수 있겠습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이러한 black box적인 속성을 지니는 머신러닝 모델을 잘 설명하기 위한 방법이 다양한 각도에서 연구되어 왔으며, 몇 년 전부터는 IML이라는 이름의 분야로 서서히 정립되기 시작했습니다. 지금부터는 이를 자세히 들여다 보고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;iml의-접근-방법-분류&quot;&gt;IML의 접근 방법 분류&lt;/h2&gt;

&lt;p&gt;불과 몇 년 전까지만 하더라도 머신러닝 모델의 해석력에 대한 정의가 하나로 정립되지 않았었고 IML에 대한 연구 또한 파편화되어 진행되어 온 경향이 있었습니다. 그러나 2010년대 중반에 접어들면서 IML 분야에 대하여 학술적으로 정립해 나가려는 시도가 하나둘씩 등장하기 시작하였고, 일정한 기준에 따라 한 단계 상위 레벨에서 IML 방법론들을 분류하려는 시도를 하게 되었습니다.&lt;/p&gt;

&lt;p&gt;IML 방법론들에 대한 분류 기준들에 대해 이해하게 되면, 머신러닝 모델에 대한 해석력 확보를 위해 어떠한 맥락에서 고민이 이루어졌으며, 어떠한 방법으로 이를 달성하고자 하였는지에 대하여 이해하는 데 도움을 얻을 수 있습니다. 그리고 이를 기반으로 새로운 IML 방법론이 등장했을 때 그것을 어떻게 활용할 수 있을지 효과적으로 계획할 수 있습니다. 본 글에서는 근 1-2년 내에 발표된 IML 관련 서적 및 survey 논문(e.g. Amina Adadi et al.) 등에서 가장 지배적으로 채택되는 기준들을 소개하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;intrinsic-vs-post-hoc&quot;&gt;Intrinsic vs. Post-hoc&lt;/h3&gt;

&lt;p&gt;머신러닝 모델의 복잡성(complexity)은 해석력과 깊은 연관이 있습니다. 좀 더 정확하게는, 둘 간에 일종의 tradeoff가 존재합니다. 머신러닝 모델의 복잡성이 낮아지면서 단순한 구조를 가질수록 그 자체에 대한 사람의 해석이 용이해지는 경향이 있으며, 반대로 복잡성이 높아지면서 점점 복잡한 구조를 가지게 될 수록 이에 대한 사람의 해석은 난해해집니다.&lt;/p&gt;

&lt;p&gt;낮은 복잡성을 보여주는 대표적인 머신러닝 모델이 바로 &lt;strong&gt;의사 결정 나무(decision tree)&lt;/strong&gt;입니다. 의사 결정 나무에서는 몇몇 요인들의 값을 기준으로 하여 둘 이상의 가지들로 분기하면서 뻗어 나가는 형태를 지니는데, 예측 결과를 도출하게 된 과정에 대한 해석이 그 자체로 매우 직관적이고 용이하다는 장점이 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/decision-tree-example.jpg&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;medium-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/decision-tree-example.jpg&quot; alt=&quot;가장 간단한 의사 결정 나무 예시&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;(어느 예측 결과에 해당하는 마디를 따라 올라가면서, 그 근거를 손쉽게 확인할 수 있음)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;가장 간단한 의사 결정 나무 예시&lt;br /&gt;&lt;small&gt;(어느 예측 결과에 해당하는 마디를 따라 올라가면서, 그 근거를 손쉽게 확인할 수 있음)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이런 의사 결정 나무의 경우 그 자체적으로 해석력을 이미 확보하고 있다고 볼 수 있으며, 이를 두고 ‘&lt;strong&gt;투명성(transparency)&lt;/strong&gt;‘을 확보하고 있다고도 합니다. 이렇게 내재적으로 투명성을 확보하고 있는 머신러닝 모델을 ‘&lt;strong&gt;intrinsic&lt;/strong&gt;(본래 갖추어진)’하다고 지칭합니다. 그 외에도 기존의 &lt;a href=&quot;http://github.sualab.io/introduction/2017/09/04/what-is-machine-learning.html#선형-모델&quot; target=&quot;_blank&quot;&gt;선형 모델(linear model)&lt;/a&gt;에 &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot; target=&quot;_blank&quot;&gt;Lasso&lt;/a&gt; 등의 요인 선택(feature selection) 기법을 적용하여 얻어진 &lt;strong&gt;희소 선형 모델(sparse linear model; &lt;small&gt;취사 선택된 일부 요인들에 대해서만 0이 아닌 가중치가 존재&lt;/small&gt;)&lt;/strong&gt;, 각 요인들에 대한 일련의 if-else 규칙들로 정의된 &lt;strong&gt;규칙 리스트(rule lists)&lt;/strong&gt; 또한 해석에 있어서의 투명성을 내재적으로 확보하였다는 측면에서 intrinsic 모델로 분류할 수 있습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/rule-lists-example.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/rule-lists-example.png&quot; alt=&quot;규칙 리스트 예시: Falling rule lists &amp;lt;small&amp;gt;(Fulton Wang and Cynthia Rudin)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;규칙 리스트 예시: Falling rule lists &lt;small&gt;(Fulton Wang and Cynthia Rudin)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;반면, 복잡성이 극도로 높은 전형적인 머신러닝 모델로는 &lt;strong&gt;신경망(neural network)&lt;/strong&gt;, 즉 딥러닝 모델이 있습니다. 신경망은 내부적으로 복잡한 연결 관계를 지니고 있으며, 이로 인해 예측 결과를 도출하는 과정에서 하나의 입력 성분값(이미지 데이터의 경우 픽셀 1개의 값)이 어떻게 기여했는지 의미적으로 해석하는 것이 대단히 어렵습니다. 당연하게도, 신경망의 은닉층(hidden layer)의 수가 많아질수록 그 복잡성은 높아지며, 반대로 투명성은 낮아지고 해석력 확보는 어려워집니다.&lt;/p&gt;

&lt;p&gt;이렇게 복잡성이 높은 머신러닝 모델에 대하여 적용할 수 있는 대안으로, 비교적 간단한 형태를 지니는 별도의 ‘설명용 모델(interpretable model)’을 셋팅하고(e.g. 의사 결정 나무, 희소 선형 모델, 규칙 리스트 등) 이를 설명 대상이 되는 머신러닝 모델에 갖다 붙여 적용하는 방법을 시도할 수 있습니다. 설명용 모델의 예측 결과는 원본 머신러닝 모델을 모방하도록 하되, 복잡성은 가능한 한 낮추어 투명성을 확보하기 위한 전략으로, 이를 ‘&lt;strong&gt;post-hoc&lt;/strong&gt;(사후적 설명)’ 방법이라고 지칭합니다. Post-hoc 방법의 경우 곧 설명할 model-agnostic 방법과 밀접한 연관이 있기 때문에, 뒤에서 좀 더 자세히 알아보도록 하겠습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/post-hoc-methods.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/post-hoc-methods.png&quot; alt=&quot;Post-hoc 방식으로 '부착된' 설명용 모델과 원본 머신러닝 모델 간의 관계&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;(파란색으로 표시된 'A-&amp;gt;B' 관계의 경우, B가 A로부터 학습을 수행한다는 것을 표현함)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Post-hoc 방식으로 ‘부착된’ 설명용 모델과 원본 머신러닝 모델 간의 관계&lt;br /&gt;&lt;small&gt;(파란색으로 표시된 ‘A-&amp;gt;B’ 관계의 경우, B가 A로부터 학습을 수행한다는 것을 표현함)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;머신러닝 모델의 복잡성과 투명성 간에는 tradeoff가 존재한다고 하였는데, 오로지 투명성 향상을 위해 마냥 복잡성이 낮은 intrinsic 모델을 사용하는 것은 실제 예측 성능의 측면을 고려하면 좋은 선택이 아닙니다. 이미지 인식 등과 같이 복잡성이 낮은 머신러닝 모델로는 해결이 어려운 문제가 존재하며, 이러한 경우에는 투명성을 일부 희생하면서라도 복잡성이 높은 딥러닝 모델을 사용하여 예측 성능 자체를 높이는 데 집중하는 것이 더 나은 대안일 수 있습니다. 다시 말해, 해결하고자 하는 문제에 따라서, 그것에서 요구되는 예측 성능의 최소 기대 수준과 더불어, 예측 결과에 대해 필요한 해석력의 정도 등을 종합적으로 고려하여 적절한 머신러닝 모델을 선택해야 합니다.&lt;/p&gt;

&lt;h3 id=&quot;model-specific-vs-model-agnostic&quot;&gt;Model-specific vs. Model-agnostic&lt;/h3&gt;

&lt;p&gt;IML 방법론이 어느 특정한 종류의 머신러닝 모델에 특화되어 작동하는지, 혹은 모든 종류의 머신러닝 모델에 범용적으로 작동하는지에 따라 이들을 분류할 수도 있습니다. 전자의 경우 ‘&lt;strong&gt;‘model-specific&lt;/strong&gt;(모델 특정적)’, 후자의 경우 ‘&lt;strong&gt;model-agnostic&lt;/strong&gt;(모델 불가지론적)’ 방법이라고 지칭합니다.&lt;/p&gt;

&lt;p&gt;앞서 언급했던 의사 결정 나무 등과 같이 &lt;em&gt;intrinsic한 속성을 지니는 머신러닝 모델은, 그 자체가 본질적으로 model-specific한 속성을 동시에 지닙니다&lt;/em&gt;. 다시 말해, 이러한 머신러닝 모델들은 그 자체를 통해 자연스럽게 제공될 수 있는 특수한 형태의 설명을 본질적으로 갖추고 있다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;반면 신경망과 같이 복잡성이 높아 자체적인 투명성을 확보하기 어려운 머신러닝 모델의 경우, 별도의 post-hoc 방법을 통해 설명용 모델을 생성하고 이를 활용한다고 하였습니다. 현재 나와 있는 post-hoc 방법들의 경우, 그 적용 대상 머신러닝 모델의 종류와 무관하게 범용적으로 작동하도록 디자인되어 있는 경우가 대부분에 해당합니다. 다시 말해, &lt;em&gt;post-hoc 특성을 지닌 IML 방법론은 곧 model-agnostic하다&lt;/em&gt;고 해도 크게 무리가 없습니다.&lt;/p&gt;

&lt;p&gt;정리하면, intrinsic과 model-specific, post-hoc과 model-agnostic은 서로 간의 관점에 차이가 있을 뿐, 실질적으로는 동시적으로 적용될 수 있는 특성이라고 봐도 크게 무리가 없다고 할 수 있겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;local-vs-global&quot;&gt;Local vs. Global&lt;/h3&gt;

&lt;p&gt;어느 머신러닝 모델의 모든 예측 결과에 대하여, IML 방법론이 그럴싸한 설명을 빠짐 없이 제시할 수 있는 경우, 해당 모델을 해석하는 데 있어 가장 이상적일 것입니다. 이렇게 어느 IML 방법론이 머신러닝 모델의 예측 결과들에 대하여 ‘전역적으로(globally)’ 완벽하게 설명을 수행할 수 있는 경우, 이를 ‘&lt;strong&gt;global&lt;/strong&gt;(전역적)’ 방법이라고 지칭합니다.&lt;/p&gt;

&lt;p&gt;반면 설명 대상 머신러닝 모델의 복잡성이 증가할수록, 단일 IML 방법론이 모든 예측 결과에 대하여 그럴싸한 설명을 제시하는 것이 점점 어려워집니다. 이 때문에 몇몇 IML 방법론들은 완벽하게 ‘전역적인’ 설명을 포기하는 대신, 모델의 어느 예측 결과에 대하여 적어도 그와 유사한 양상을 나타내는 ‘주변’  예측 결과들에 한해서는 ‘국소적으로(locally)’ 그럴싸한 설명을 제시할 수 있도록 디자인되었습니다. 이를 ‘&lt;strong&gt;local&lt;/strong&gt;(국소적)’ 방법이라고 지칭합니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/local-explanations.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/local-explanations.png&quot; alt=&quot;IML 방법의 local한 설명을 표현하는 그림 &amp;lt;small&amp;gt;(Marco T. Ribeiro et al.)&amp;lt;/small&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;(위: 이진 분류(binary classification) 문제에서 원본 머신러닝 모델의 전체 예측 결과를 나타낸 공간, &amp;lt;br&amp;gt;아래: Local한 설명용 모델(갈색 점선)들이 원본 머신러닝 모델의 전체 예측 결과들 중 각각 일부분만을 커버하도록 셋팅된 결과)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;IML 방법의 local한 설명을 표현하는 그림 &lt;small&gt;(Marco T. Ribeiro et al.)&lt;/small&gt;&lt;br /&gt;&lt;small&gt;(위: 이진 분류(binary classification) 문제에서 원본 머신러닝 모델의 전체 예측 결과를 나타낸 공간, &lt;br /&gt;아래: Local한 설명용 모델(갈색 점선)들이 원본 머신러닝 모델의 전체 예측 결과들 중 각각 일부분만을 커버하도록 셋팅된 결과)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 볼 수 있듯이, local한 IML 방법은 원본 머신러닝 모델의 전체 예측 결과 중 일부 영역들만을 커버할 수 있도록 작동합니다. 좀 더 구체적으로, 어느 하나의 테스트 이미지에 대하여 원본 머신러닝 모델이 예측한 하나의 결과를 설명하고자 할 때, (원본 머신러닝 모델의 입장에서) 해당 이미지와 유사하게 인식한 다른 몇몇 테스트 이미지의 예측 결과들에 대해서도 그럴싸한 설명을 제공할 수 있는 설명용 모델을 즉석에서(ad-hoc) 제시합니다.&lt;/p&gt;

&lt;p&gt;오늘날 신경망과 같이 복잡성이 높은 머신러닝 모델을 사용하는 일반적인 상황에서, 예측 결과에 대하여 전역적으로 완벽한 설명을 제시하는 것은 현실적으로 매우 어려운 일입니다. 비록 머신러닝 모델의 전체 예측 결과에 대하여 완벽한 설명을 한 번에 제시하는 것은 불가능하더라도, 적어도 사용자가 관심을 가지는 몇 개의 예측 결과에 한하여 즉각적으로 그럴싸한 설명을 제시해 줄 수 있다는 측면에서, local한 IML 방법들은 설명의 실용성 측면에서 각광을 받고 있습니다.&lt;/p&gt;

&lt;p&gt;마지막으로, intrinsic(model-specific) vs. post-hoc(model-agnostic), local vs. global 기준에 의거하여, 현재까지 보고된 주요 IML 방법론들을 플롯팅해 본 결과를 아래와 같이 제시하였습니다.&lt;/p&gt;

&lt;!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ --&gt;
&lt;p&gt;&lt;a href=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/iml-methods-plot.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img class=&quot;large-image&quot; src=&quot;http://github.sualab.io/assets/images/interpretable-machine-learning-overview-1/iml-methods-plot.png&quot; alt=&quot;Intrinsic(Model-specific) vs. Post-hoc(Model-agnostic),&amp;lt;br&amp;gt;Local vs. Global 기준에 의거한 IML 방법론의 플롯팅 및 그룹화 결과 &amp;lt;small&amp;gt;(Amina Adadi et al.)&amp;lt;/small&amp;gt;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption&quot;&gt;Intrinsic(Model-specific) vs. Post-hoc(Model-agnostic),&lt;br /&gt;Local vs. Global 기준에 의거한 IML 방법론의 플롯팅 및 그룹화 결과 &lt;small&gt;(Amina Adadi et al.)&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;보통 intrinsic(model-specific)한 IML 방법은 그 자체가 예측 및 설명 모두를 위해 직접적으로 사용되는 경우는 드물며, 그 대신 신경망과 같이 복잡성이 높은 머신러닝 모델을 예측을 위해 먼저 사용한 뒤, 여기에 post-hoc(model-agnostic)한 IML 방법을 부가적으로 적용하는 방식이 많이 채택됩니다. 또, post-hoc한 IML 방법들 중 global한 것보다는 local에 가까운 방법들이 더 많이 보고되었는데, 이는 복잡성이 높은 머신러닝 모델에 대한 효과적인 설명을 위해 국소적으로나마 그럴싸한 설명을 제시할 수 있도록 하는 데 집중한 결과로 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;지금까지 과거부터 현재 시점까지 등장한 주요 IML 방법론들을, 좀 더 일반론적인 머신러닝 영역 전반에서 커버할 수 있도록 몇몇 기준들에 의거하여 서술하였습니다. 그러나 오늘날 실상을 보면, 딥러닝이 등장하고 몇몇 어려운 문제에서 드라마틱한 성능 향상을 거두게 되면서, 실제 산업 현장 등에서는 (해석력 등을 따지기 이전에) 딥러닝 모델의 사용이 반 강제화되는 상황이 점차적으로 늘어나기 시작했습니다.&lt;/p&gt;

&lt;p&gt;이에 따라 최근에는 기 학습된 신경망에 대한 해석력 확보를 위한 IML 방법론이 급격하게 늘어나고 있는데, 이들 중 거의 모두가 post-hoc이면서, 많은 수가 local한 경향을 보이고 있습니다. 딥러닝 모델을 위한 주요 IML 방법론에 대해서는 다음 편 글에서 집중적으로 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;iml-실현과-관련된-현실적-이슈&quot;&gt;IML 실현과 관련된 현실적 이슈&lt;/h2&gt;

&lt;p&gt;지금까지 IML과 관련된 기술적인 내용에 대해 알아보았다면, 지금부터는 IML 방법론이 제시한 설명을 받아들이는 ‘사람’의 특징을 고려한 몇 가지 이슈에 주목하고자 합니다.&lt;/p&gt;

&lt;p&gt;어느 머신러닝 모델에 대한 적절한 설명이 이루어지려면, 당연하게도, 그 설명 결과가 사람이 이해할 수 있을 만큼 ‘좋은’ 것이어야 합니다. 그런데, 사실 ‘좋은’ 설명이라는 것은 애초에 주관적 성격이 강하기 때문에, 그 설명을 받아들이는 사람이 누구인지에 따라 다르게 인식될 가능성이 있습니다. 사람에 따라 설명을 다르게 받아들일 수 있다는 문제를 극복하고자, IML 연구자들은 다양한 방면에서 문제 해결의 실마리를 찾고자 하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;인간-친화적인-설명의-특징&quot;&gt;인간 친화적인 설명의 특징&lt;/h3&gt;

&lt;p&gt;먼저, 비록 IML의 설명에 대한 반응이 주관적인 측면이 있다고 하더라도, 최소한 일반적인 대중들의 기저에서는 공유될 수 있는 ‘이해할 수 있을 만한’ 설명의 기본적인 특징에 대하여 고찰하고자 하였습니다. 그 예로 철학, 심리학, 인지 과학 등을 망라하는 관점에서 인간 친화적인 설명이 어떤 특징을 지니는지 연구한 내용(e.g. Tim Miller)에 대해 소개해 드리도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;첫째로, 좋은 설명은 ‘&lt;strong&gt;대조적(contrastive)&lt;/strong&gt;‘인 특징을 지닙니다. 보통의 사람들은 어느 하나의 결과 ‘A’ 자체에 집중하여 왜 그러한 결과가 도출되었는지 궁금해 하기보다는, 왜 다른 결과 ‘B’ 대신 ‘A’가 도출되었는지를 은연 중에 더 궁금해합니다. 즉, 어느 결과에 대하여 다른 잠재적 결과의 대조를 통해 그 결과에 대한 이해를 더 잘 가져가는 특성이 있습니다. 대조 대상은 실제 도출된 다른 결과일 수록 좋으며, 가상의 결과도 충분히 효과적입니다.&lt;/p&gt;

&lt;p&gt;따라서 다양한 상황에서, 머신러닝 모델의 어느 예측 결과 ‘A’가 도출된 근거를 다양한 요인들을 기반으로 주저리주저리 설명하는 것보다는, 그것과 확실히 비교되는 다른 예측 결과 ‘B’를 보여주면서, ‘A’와 ‘B’의 결과 차이를 만들어 낸 핵심적인 요인들을 중심으로 설명하는 것이 보통 더 효과적으로 먹힙니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;무언가를 잘못 먹고 배탈이 났을 때, 나와 식사를 같이 했던 다른 사람들이 먹지 않았지만 나만 먹었던 음식이 무엇이었는지를 먼저 떠올리는 것이 ‘대조적’ 사고 방식을 보여주는 대표적인 케이스입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;둘째로, 좋은 설명은 ‘&lt;strong&gt;선택적(selective)&lt;/strong&gt;‘인 특징을 지닙니다. 보통의 사람들은 어느 결과를 발생시킨 요인을 모든 곳에서 찾으려고 하지 않으며, 보통 자신에게 친숙하거나 혹은 자신이 잘 알고 있는 영역에 국한하여 한두 개의 주요한 요인을 집중적으로 찾으려고 하는 경향이 있습니다. 의사의 진단과 같이 특수한 상황이 아닌 이상, 보통의 사람들은 모든 영역을 망라한 다수의 요인들을 받아들이는 것에 대하여 인지적 부담을 느낍니다.&lt;/p&gt;

&lt;p&gt;머신러닝 모델의 예측 결과에 대한 설명을 제시할 시에도, 받아들이는 사람이 부담을 느끼지 않을 정도의 적당한 양(2~3개)을 제시하는 것이 효과적입니다. 예를 들어 의사 결정 나무를 통해 설명을 제시하고자 한다면, 깊이가 2 또는 3 정도인 것을 채택하는 것이 가장 효과적이라고 할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;오늘 주가가 폭락한 경우, 실제로는 여기에 영향을 미친 매우 다양한 요인이 존재하였을 것이나, 그 대신 세간에 잘 알려진 한두 가지 경제/사회적 이슈들에 집중하는 것이 ‘선택적’ 사고 방식을 보여주는 대표적인 케이스입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;또한, 좋은 설명은 ‘&lt;strong&gt;사회적(social)&lt;/strong&gt;‘인 특징을 지닙니다. 우리가 어떤 대상에 대한 설명을 할 때, 보통 청자가 처한 상황 및 청자의 배경 지식 등을 고려하여, 적절한 수준의 어휘를 사용하여 설명을 하는 게 일반적입니다. 머신러닝 모델에 대한 설명의 경우에도, 해당 모델이 실제로 어떠한 맥락에서 사용되는지(e.g. 적용 분야, 제품, 산업 등), 주요 타겟 사용자들의 속성이 어떠한지(e.g. 해당 분야에 대한 전문성 등) 등을 종합적으로 고려하여 효과적인 설명 방법을 설계하는 것이 이상적일 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‘딥러닝’에 대하여 직장에 있는 동료 엔지니어들에게 설명할 때와, 집에 계신 어머니에게 설명할 때 사용하는 어휘, 표현 및 보조 수단 등은 아마도 서로 명백하게 다를 것입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;설명에-대한-정량적-평가-방법&quot;&gt;설명에 대한 정량적 평가 방법&lt;/h3&gt;

&lt;p&gt;인간 친화적인 설명 방법에 대한 연구와 더불어, 제시된 설명에 대한 품질을 주요 사용자들로 하여금 정량적으로 평가하도록 하기 위한 연구가 병행되고 있습니다. 이는 근본적으로 주관적 성격을 지니는 설명에 대한 인식을, 주요 사용자들에 한해 가능한 한 객관화하고자 하는 시도로 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;설명에 대한 정량적 평가를 위한 초기 연구에서는(Finale Doshi-Velez and Been Kim), 설명 대상 머신러닝 모델이 해결하고자 하는 문제의 속성에 따라 다음의 세 가지 평가 방법을 제안하였습니다.&lt;/p&gt;

&lt;p&gt;(1) &lt;em&gt;Application-grounded&lt;/em&gt;: 실제 문제에 특화된 사람으로 하여금 설명에 대한 평가를 수행하도록 하는 방법입니다. 의료 분야에서의 머신러닝 모델의 설명 결과에 대하여 그 분야의 의사가 본인의 진단 결과와 면밀한 비교를 수행하는 경우와 같이, 해결하고자 하는 문제와 유관한 분야에 정통한 전문가를 활용할 수 있는 경우에 채택할 수 있는 평가 방법입니다. 다른 평가 방법보다 비용은 많이 들 것이나, 잘못된 설명 하나가 치명적인 결과를 초래할 수 있는 실제 산업 현장에는 적용해 볼 가치가 충분하다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;(2) &lt;em&gt;Human-grounded&lt;/em&gt;: 일반 대중들로 하여금 설명에 대한 평가를 수행하도록 하는 방법입니다. Application-grounded 방법과는 달리 평가 담당자가 해당 분야에 전문성을 보유하고 있어야 할 필요는 없으며, 비교적 단순한 과정(e.g. ‘설명A’ vs. ‘설명B’에 대한 선호도 조사 등)을 거쳐 설명에 대한 평가를 수행하도록 설계됩니다. 주로 일반 대중을 타겟으로 한 상용화 제품 등에 적용할 수 있는 방법으로, 비록 심층적이고 엄밀한 평가를 얻기는 어려우나, 타겟 사용자 집단에 대하여 많은 수의 응답을 효과적으로 얻을 수 있다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;(3) &lt;em&gt;Functionally-grounded&lt;/em&gt;: 설명에 대한 품질을 반영하는 ‘대리 척도(proxy metric)’를 정의, 이를 계산함으로써 사람의 개입 없이 자동화된 평가를 수행하도록 하는 방법입니다. 예를 들어 의사 결정 나무의 경우, 그 깊이가 깊어질수록 투명성이 낮아지고 해석력 확보가 어려워지는 경향이 있으므로, ‘의사 결정 나무의 깊이’를 그 자체의 설명에 대한 품질의 대리 척도로 활용하는 것이 나름대로 합리적이라고 할 수 있습니다. 물론, 해결하고자 하는 문제에 부합하는 적절한 대리 척도를 잘 정의하는 것이 필수적으로 선행되어야 하며, 이것이 결코 쉽지 않은 일이라는 점을 감안해야 합니다.&lt;/p&gt;

&lt;p&gt;타겟 사용자에게 충분히 ‘좋게’ 받아들여지는 설명 방법을 단번에 찾아내는 것은 매우 어려운 일이므로, 위와 같은 방법을 적절히 사용하여 실제 타겟 사용자에 대한 피드백을 얻고, 이를 반영함으로써 설명 방법을 개선해 나가는 과정을 계속 반복해야 합니다. 좀 더 거시적인 관점에서는, IML 실현을 위해 기존 머신러닝 시스템에 사람의 피드백이 끊임 없이 반영되도록 하는, ‘human-in-the-loop’을 시도하도록 지속적인 개선을 시도해야 할 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;결론&quot;&gt;결론&lt;/h2&gt;

&lt;p&gt;지금까지 머신러닝 모델에 대한 설명을 통한 해석력 확보의 필요성을 사용자, 모델 개발자, 일반 대중 등의 다양한 관점에서 확인해 보았고, 이를 실현하기 위한 IML 방법론들에 대한 개요와 더불어 IML 실현과 관련하여 고민해야 하는 현실적인 이슈들에 대하여 알아보았습니다.&lt;/p&gt;

&lt;p&gt;일반 사용자들의 이해와 신뢰 확보, 머신러닝 모델 개발자의 효과적인 디버깅, 인류의 새로운 발견과 지식 축적 등 다양한 상황에서 해석력 확보가 필요함을 확인하였으나, 머신러닝 모델이 근본적으로 가지고 있는 black box적 속성 때문에, 예측 성능 향상을 위해 모델의 복잡성을 높일수록 해석력 확보가 점점 어려워진다는 문제를 같이 확인하였습니다.&lt;/p&gt;

&lt;p&gt;이러한 문제를 해결하고자 다양한 IML(interpretable machine learning) 방법론들이 등장하였고, 이들을 거시적인 관점에서 분류하는 데 참조할 수 있는 기준들 - Intrinsic/Post-hoc, Model-specific/Model-agnostic, Local/Global - 에 대해 알아보았습니다. 이를 통해 IML에 대한 현재까지의 연구들이 머신러닝 모델에 대한 해석력 확보를 위해 어떤 맥락에서 접근하였는지에 대해 이해하고자 하였습니다. 예를 들어, 그 자체로 해석력을 확보하고 있는 의사 결정 나무, 희소 선형 모델, 규칙 리스트 등은 ‘설명용 모델’로써, 신경망과 같이 복잡성이 높은 모델에 적용될 수 있다고 하였습니다. 다만 설명용 모델의 복잡성 측면에서의 제약 조건 상, 설명 대상 머신러닝 모델의 모든 예측 결과를 커버하는 대신, 관심의 대상이 되는 일부 예측 결과의 주변 영역들에 대해서만 국소적으로 커버할 수 있도록 작용할 수 있음을 확인하였습니다.&lt;/p&gt;

&lt;p&gt;다른 한 편에서는, 일반적인 사람이 이해할 수 있을 만한 인간 친화적인 설명은 어떤 특징을 지니는지 인문학 및 인지 과학적 관점에서 연구하였고, 대조적, 선택적, 사회적 특징을 지니는 설명이 효과적임을 확인하였습니다. 또한 실제 제품 및 산업 현장 등에 적용된 머신러닝 모델에 대한 설명의 품질을 평가하고자, 해당 분야의 전문가, 일반 대중 또는 자동적으로 계산되는 대리 척도에 의거한 정량적인 평가 방법을 제안하였습니다.&lt;/p&gt;

&lt;p&gt;*위에서 소개한 분류 기준들에 부합하는 대표적인 IML 방법론들을 다루기에는 글의 분량 관계 상 어려울 것으로 판단하여, 이번 글에서는 IML에 대한 개요에 집중하였습니다. 다음 편에서는 IML에 대한 지금까지의 이해를 바탕으로, 여러분들이 많은 관심을 가지고 계실 딥러닝 모델에 대한 주요 IML 방법론들에 초점을 맞추어 좀 더 구체적으로 소개하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Amina Adadi and Mohammed Berrada, Peeking inside the black-box: a survey on explainable artificial intelligence (XAI).
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/iel7/6287639/6514899/08466590.pdf&quot; target=&quot;_blank&quot;&gt;Adadi, Amina, and Mohammed Berrada. “Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI).” IEEE Access 6 (2018): 52138-52160.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Riccardo Guidotti et al., A survey of methods for explaining black box models.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/ft_gateway.cfm?ftid=1997533&amp;amp;id=3236009&quot; target=&quot;_blank&quot;&gt;Guidotti, Riccardo, et al. “A survey of methods for explaining black box models.” ACM computing surveys (CSUR) 51.5 (2018): 93.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Finale Doshi-Velez and Been Kim, Towards a rigorous science of interpretable machine learning.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1702.08608&quot; target=&quot;_blank&quot;&gt;Doshi-Velez, Finale, and Been Kim. “Towards a rigorous science of interpretable machine learning.” arXiv preprint arXiv:1702.08608 (2017).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Zachary C. Lipton, The mythos of model interpretability.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.03490&quot; target=&quot;_blank&quot;&gt;Lipton, Zachary C. “The mythos of model interpretability.” arXiv preprint arXiv:1606.03490 (2016).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Been Kim et al., Examples are not enough, learn to criticize! criticism for interpretability.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6300-examples-are-not-enough-learn-to-criticize-criticism-for-interpretability.pdf&quot; target=&quot;_blank&quot;&gt;Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! criticism for interpretability.” Advances in Neural Information Processing Systems. 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Marco T. Ribeiro et al., Why should I trust you? Explaining the predictions of any classifier.
    &lt;ul&gt;
      &lt;li&gt;이메일 분류 문제에서의 예측 결과에 대한 ‘설명’ 제시 사례&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1602.04938.pdf?mod=article_inline&quot; target=&quot;_blank&quot;&gt;Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why should i trust you?: Explaining the predictions of any classifier.” Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM, 2016.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;점술의 예시: 타로 카드(Tarot card)를 통한 예측
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://medium.com/@aritrin/all-about-tarot-everything-you-need-to-know-about-tarot-card-reading-60f1d03b675b&quot; target=&quot;_blank&quot;&gt;Ari Tri Nugroho. “All About Tarot: Everything You Need to Know about Tarot Card Reading.” Medium, https://medium.com/@aritrin/all-about-tarot-everything-you-need-to-know-about-tarot-card-reading-60f1d03b675b. Accessed 6 August 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;타로 카드의 기본적인 상징성에 기반한 설명
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://giphygifs.s3.amazonaws.com/media/5fBH6ztSjOdRgr46IWk/giphy.gif&quot; target=&quot;_blank&quot;&gt;Geek &amp;amp; Sundry, “Web series death gif” GIPHY, https://giphy.com/gifs/geekandsundry-lol-ashley-johnson-spooked-5fBH6ztSjOdRgr46IWk. Accessed 6 August 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;알파고의 수를 복기하면서 힘겨워하는 이세돌 9단
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://news.chosun.com/site/data/html_dir/2016/03/11/2016031102984.html&quot; target=&quot;_blank&quot;&gt;차정승 기자, “이세돌 2연패 후 ‘밤샘 복기’…호텔방서 칩거하며 ‘알파고 파기 비법’ 연구.” 조선닷컴, http://news.chosun.com/site/data/html_dir/2016/03/11/2016031102984.html. Accessed 8 August 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;알파고 등장 이후의 바둑 패러다임의 변화 기사
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://news.joins.com/article/21352123&quot; target=&quot;_blank&quot;&gt;정아람 기자, “알파고 쇼크 1년 … 바둑 패러다임이 달라졌다.” 중앙일보, https://news.joins.com/article/21352123. Accessed 8 August 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;가장 간단한 의사 결정 나무 예시
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://becominghuman.ai/understanding-decision-trees-43032111380f&quot; target=&quot;_blank&quot;&gt;Egor Dezhic, “Understanding Decision Trees.” Medium, https://becominghuman.ai/understanding-decision-trees-43032111380f, Accessed 20 August 2019.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Lasso
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot; target=&quot;_blank&quot;&gt;Wikipedia contributors. “Lasso (statistics).” Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 13 Aug. 2019. Web. 20 Aug. 2019. &lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rule lists
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v38/wang15a.pdf&quot; target=&quot;_blank&quot;&gt;Wang, Fulton, and Cynthia Rudin. “Falling rule lists.” Artificial Intelligence and Statistics. 2015.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tim Miller, Explanation in artificial intelligence: Insights from the social sciences.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.07269&quot; target=&quot;_blank&quot;&gt;Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” Artificial Intelligence (2018).&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>김길호</name><email>Kyle.Kim@cognex.com</email></author><category term="Introduction" /><category term="interpretable machine learning" /><category term="interpretability" /><category term="explainable artificial intelligence" /><summary type="html">지금까지의 포스팅을 통해, 수아랩 블로그에서는 다양한 문제 상황에 대하여 동작하는 딥러닝 모델을 직접 제작하고 학습해 왔습니다. 다만 대부분 맨 마지막 과정에서 학습이 완료된 모델을 테스트하는데, 일정 크기의 테스트 데이터셋에 대한 모델의 예측 결과를 바탕으로 정확도(accuracy)와 같이 하나의 숫자로 표현되는 정량적 지표의 값을 계산하고, 그 크기를 보아 ‘딥러닝 모델이 예측을 얼마나 잘 했는지’를 판단하였습니다. 여기에 덧붙여 보완적으로, 올바르게 예측하였거나 잘못 예측한 예시 결과들을 몇 개 샘플링하여 관찰하고, 모델의 예측 결과에 대한 근거를 ‘추측’한 바 있습니다.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://github.sualab.io/post-hoc-methods.png" /><media:content medium="image" url="http://github.sualab.io/post-hoc-methods.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>