<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Page metadata --> 
  <!-- Reference: http://jovandeginste.github.io/2016/05/18/add-metadata-tags-to-jekyll-blog-posts.html -->
  <meta name="description" content="(이전 포스팅 보기)">

  <meta property="og:site_name" content="Cognex Deep Learning Lab-KOR Research Blog">
  
  <meta property="og:title" content="Bayesian Optimization 개요: 딥러닝 모델의 효과적인 hyperparameter 탐색 방법론 (2)">
  <meta property="og:type" content="article">
  <meta property="og:description" content="(이전 포스팅 보기)

"/>
  
  
  <meta property="article:published_time" content="2019-04-01T09:00:00+09:00">
  <meta property="article:author" content="http://research.sualab.com/about/">
  
  <meta property="og:url" content="http://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html" />
  
  <meta itemprop="keywords" content="bayesian optimization,hyperparameter optimization,gaussian process" />
  
  <meta property="article:tag" content="bayesian optimization">
  
  <meta property="article:tag" content="hyperparameter optimization">
  
  <meta property="article:tag" content="gaussian process">
  
  
  
  <!-- end of Page metadata -->

  <title>Bayesian Optimization 개요: 딥러닝 모델의 효과적인 hyperparameter 탐색 방법론 (2)</title>
  <meta name="description" content="(이전 포스팅 보기)">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html">  <link rel="alternate" type="application/rss+xml" title="Cognex Deep Learning Lab-KOR Research Blog" href="/feed.xml">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  <!-- Enabling line-breaking for MathJax equations -->
  <!-- @reference: https://stackoverflow.com/questions/29893923/how-to-make-formula-with-mathjax-responsive/29904718 -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
                SVG: { linebreaks: { automatic: true } }
                });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>
<body><header class="site-header" role="banner">

<div class="wrapper">
  
  
	<div class="cognex-logo-div">
		<img class="cognex-logo-img" src="/assets/images/Cognex_logo.png" />
	</div>
	<div>
  	<a class="site-title" href="/">Cognex Deep Learning Lab-KOR Research Blog</a>
	</div>

  
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
        </svg></span>
      </label>

      <div class="trigger">
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
	<a class="page-link" href="/Introduction.html"> Introduction </a>
	<a class="page-link" href="/Practice.html"> Practice </a>
	<a class="page-link" href="/Development.html"> Development </a>
	<a class="page-link" href="/Review.html"> Review </a>
	<a class="page-link" href="/etc..html"> etc. </a>
  <a class="page-link" href="https://jobs.cognex.com/" target="_blank"> Jobs </a>
      </div>
    </nav>
  
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!-- Look the author details up from the site config. -->

<!-- Post metadata -->
<!-- Reference: http://jovandeginste.github.io/2016/05/18/add-metadata-tags-to-jekyll-blog-posts.html -->

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Bayesian Optimization 개요: 딥러닝 모델의 효과적인 hyperparameter 탐색 방법론 (2)</h1>
    <p class="post-meta">
      <time datetime="2019-04-01T09:00:00+09:00" itemprop="datePublished">
        
        Apr 1, 2019
      </time>
       • 
        
          <span itemprop="category" itemscope itemtype="http://schema.org/Category"><a href="/Introduction.html">Introduction</a>, </span>
        
          <span itemprop="category" itemscope itemtype="http://schema.org/Category"><a href="/Practice.html">Practice</a></span>
        
      
      
        • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name"><a href="https://github.com/kimkilho" target="_blank">김길호</a></span></span>
        <!-- Author metadata -->
        <meta itemprop="email" content="Kyle.Kim@cognex.com" />
        <meta itemprop="web" content="https://github.com/kimkilho" />
        <!-- end of Author metadata -->
      
      
         <br>Tags: bayesian optimization, hyperparameter optimization, gaussian process
      
    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p><a href="http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html">(이전 포스팅 보기)</a></p>

<p>지난 글에서 딥러닝 모델의 Hyperparamter Optimization을 위한 Bayesian Optimization 방법론의 대략적인 원리 및 행동 방식에 대한 설명을 드렸습니다. 이번 글에서는 실제 Bayesian Optimization을 위한 Python 라이브러리인 <em>bayesian-optimization</em>을 사용하여, 간단한 예시 목적 함수의 최적해를 탐색하는 과정을 먼저 소개하고, 실제 딥러닝 모델의 최적 hyperparameter를 탐색하는 과정을 안내해 드리도록 하겠습니다.</p>

<ul>
  <li><strong>주의: 본 글은 아래와 같은 분들을 대상으로 합니다.</strong>
    <ul>
      <li>딥러닝 알고리즘의 기본 구동 원리 및 정규화(regularization) 등의 테크닉에 대한 기초적인 내용들을 이해하고 계신 분들</li>
      <li>Python 언어 및 TensorFlow의 기본적인 사용법을 알고 계신 분들</li>
    </ul>
  </li>
  <li>본격적인 시작에 앞서, 여러분의 Python 환경 상에 <a href="https://pypi.org/project/bayesian-optimization/" target="_blank">bayesian-optimization</a> 라이브러리를 먼저 설치해 주시길 바랍니다. 이는 PyPI에서 <em>bayesian-optimization</em>이라는 이름의 패키지로 제공되며, pip로 설치하실 수 있습니다.</li>
  <li>본 글의 중반부에 소개된, 예시 목적 함수에 대한 최적해를 탐색하는 과정은 <a href="https://github.com/sualab/sualab.github.io/blob/master/assets/notebooks/bayesian-optimization-overview-2.ipynb" target="_blank">여기</a>에서 확인하실 수 있습니다.</li>
  <li>본 글의 후반부에서는 지난 <a href="http://research.sualab.com/machine-learning/computer-vision/2018/01/17/image-classification-deep-learning.html" target="_blank">&lt;이미지 Classification 문제와 딥러닝: AlexNet으로 개vs고양이 분류하기&gt;</a> 글에서 사용했던 AlexNet 구현체를 그대로 가져와서, 딥러닝 모델 학습과 관련된 최적의 hyperparameter를 탐색하는 과정에 대해서만 <em>bayesian-optimization</em> 라이브러리를 사용하는 방법을 중심으로 설명합니다. 본문을 따라 구현체를 작성하고 시험적으로 구동해 보고자 하시는 분들은, 아래 사항들을 참조해 주십시오.
    <ul>
      <li>만일 &lt;이미지 Classification 문제와 딥러닝: AlexNet으로 개vs고양이 분류하기&gt; 글을 읽어보지 않으셨다면, 먼저 해당 글을 읽어보시면서 AlexNet 구현체 및 개vs고양이 분류 데이터셋에 대한 준비를 미리 완료해 주시길 바랍니다.</li>
      <li>본 글에서 최적 hyperparameter 탐색을 수행하는 전체 구현체 코드는 <a href="https://github.com/sualab/asirra-dogs-cats-classification" target="_blank">수아랩의 GitHub 저장소</a>에서 자유롭게 확인하실 수 있습니다.
        <ul>
          <li>전체 구현체 코드 원본에는 모든 주석이 (일반적인 관습에 맞춰) 영문으로 작성되어 있으나, 본 글에서는 원활한 설명을 위해 이들을 한국어로 번역하였습니다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="bayesian-optimization-라이브러리-소개">bayesian-optimization 라이브러리 소개</h2>

<p>오늘날 Bayesian Optimization을 다양한 문제에 원활하게 적용할 수 있도록 하는 여러 Python 라이브러리가 공개되어 있습니다. 이들 중에서 Fernando Nogueira 씨가 제작한 <a href="https://github.com/fmfn/BayesianOptimization" target="_blank"><i>bayesian-optimization</i></a> 라이브러리를 간략하게 소개해 드리고, 간단한 예시 목적 함수의 최적해를 탐색하는 방법에 대해 안내해 드린 후, 실제 딥러닝 모델의 hyperparameter 탐색을 위해서 어떻게 사용할 수 있는지를 집중적으로 설명하도록 하겠습니다.</p>

<!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ -->
<p><a href="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/bayesian-optimization-library.png" target="_blank">
  <img class="full-image" src="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/bayesian-optimization-library.png" alt="bayesian-optimization 라이브러리" />
</a>
<span class="caption">bayesian-optimization 라이브러리</span></p>

<p>우선 <em>bayesian-optimization</em> 라이브러리는 Surrogate Model로 Gaussian Process(이하 GP)를 채택하는데, 이와 관련된 세부 계산 과정들을 한두 개의 단순한 함수들로 wrapping해 놓았기 때문에, GP의 계산과 관련된 자세한 과정을 모르고 있더라도 아주 쉽고 간결하게 사용할 수 있다는 게 최대 장점입니다. 지난 글에서 보여 드렸던, Bayesian Optimization의 필수 요소인 Surrogate Model과 Acquisition Function의 행동적 특징만 제대로 이해하고 있으면, 해당 라이브러리를 사용하는 데 아무런 문제가 없다고 할 수 있습니다.</p>

<p>또한 <em>bayesian-optimization</em> 라이브러리는 다른 Bayesian Optimization 라이브러리에 비해 dependency가 적기 때문에, 본격적인 사용에 앞서 추가로 설치해야 하는 라이브러리가 <em>numpy</em>, <em>scipy</em>, <em>scikit-learn</em> 등에 불과하다는 점도 장점이라고 할 수 있습니다. 여기에 덧붙여 다양한 사용 시나리오에 대한 문서화 및 시각화 등이 상세하게 잘 되어 있기 때문에, 라이브러리의 사용법을 빠르게 터득할 수 있다는 점 또한 장점으로 꼽을 만합니다.</p>

<p>다른 Bayesian Optimization 라이브러리와 비교하여 <em>bayesian-optimization</em> 라이브러리의 단점이 있다면 Surrogate Model로 GP 외의 것은 지원하지 않는다는 점일 것입니다. 다만 지난 글에서 언급했던 학습률(learning rate), L2 정규화 계수(L2 regularization coefficient) 등과 같이 연속형(continuous) hyperparameter를 탐색하는 데에 한해서는, GP만으로도 준수한 결과를 기대해 볼 수 있습니다.</p>

<p>(그러나 아키텍처(architecture) 또는 데이터 증강(data augmentation) 등과 같은 요소들과 연관되어 있는 이산형(discrete) hyperparameter를 탐색고자 할 경우에는 단순 GP만으로는 부족한 측면이 있습니다. 만일 탐색 대상 hyperparameter가 순서형(ordinal)인 경우에는 이를 연속형으로 간주하여 GP를 통해 탐색할 수는 있습니다만, 이것이 완벽한 해법은 아님을 감안해야 합니다.)</p>

<h2 id="간단한-예시-목적-함수의-최적해-탐색">간단한 예시 목적 함수의 최적해 탐색</h2>

<p><em>bayesian-optimization</em> 라이브러리의 기본적인 사용 방법을 파악하기 위해, 입력값이 <script type="math/tex">x</script> 하나인 간단한 예시 목적 함수 <script type="math/tex">f(x)</script>의 최적해 <script type="math/tex">x^*</script>를 Bayesian Optimization으로 탐색하는 방법을 먼저 소개해 드리도록 하겠습니다. 통상적으로 Bayesian Optimization의 공략 대상이 되는 함수는 그 표현식을 명시적으로 알지 못하는 black-box function인 경우가 보통이라고 하였으나, 이 경우에는 이해를 돕기 위한 목적으로 표현식을 알고 있는 함수를 대신 사용한다고 봐 주시면 됩니다.</p>

<p>참고로, 본 과정은 <em>bayesian-optimization</em> 라이브러리의 GitHub 저장소에서 제공되는 <a href="https://github.com/fmfn/BayesianOptimization/blob/master/examples/visualization.ipynb" target="_blank">step-by-step visualization 문서</a>의 내용을 상당 부분 참조하여 재구성한 것입니다. 실제 Bayesian Optimization 과정에 대한 시각화 방법이 궁금하시다면, 해당 페이지를 참조해 주시길 바랍니다.</p>

<script type="math/tex; mode=display">f(x) = e^{-(x-3)^2} + e^{-(3x-2)^2} + \frac{1}{x^2+1}</script>

<p>여기에서는 위와 같은 예시 목적 함수의 <em>최댓값</em>을 탐색하는 상황을 가정하겠습니다. 상기 표현식을 알고 있다는 가정 하에서 실제 최적화 알고리즘을 통해 계산해 보면, <script type="math/tex">x=0.631</script> 부근에서 최댓값 <script type="math/tex">f(0.631)=1.707</script>이 발생합니다.</p>

<h4 id="0-import-import-import">0. import, import, import…</h4>

<p>맨 먼저 <em>bayesian-optimization</em> 라이브러리의 핵심 요소라고 할 수 있는 <code class="highlighter-rouge">BayesianOptimization</code> 클래스를 import하고, <em>numpy</em> 패키지를 추가로 import합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">bayes_opt</span> <span class="kn">import</span> <span class="n">BayesianOptimization</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<h4 id="1-입력값-및-목적-함수-정의">1. 입력값 및 목적 함수 정의</h4>

<p>다음으로 입력값 <code class="highlighter-rouge">x</code>를 인자로 하는 목적 함수 <code class="highlighter-rouge">target</code>을 아래와 같이 정의합니다. 해당 목적 함수를 따로 플롯팅해 보면 그 아래 그림과 같이 나타납니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">target</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ -->
<p><a href="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/target-function-plot.png" target="_blank">
  <img class="full-image" src="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/target-function-plot.png" alt="예시 목적 함수의 플롯팅 결과" />
</a>
<span class="caption">예시 목적 함수의 플롯팅 결과</span></p>

<h4 id="2-bayesianoptimization-객체-생성">2. BayesianOptimization 객체 생성</h4>

<p>다음으로 <code class="highlighter-rouge">BayesianOptimization</code> 객체를 하나 생성합니다. 코드 상에는 드러나 있지 않으나, 이 <code class="highlighter-rouge">BayesianOptimization</code> 객체에는 Surrogate Model인 GP가 기본적으로 내장되어 있으며, 이는 실제로는 해당 객체 내부의 멤버 변수 <code class="highlighter-rouge">_gp</code>로 표현됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bayes_optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span> 
                                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">BayesianOptimization</code> 객체를 생성할 시, 앞쪽 두 개의 인자는 필수적으로 입력해 줘야 하는 것들에 해당합니다. 첫 번째 인자(<code class="highlighter-rouge">target</code>)는 최적해를 탐색하고자 하는 목적 함수 <script type="math/tex">f(x)</script>를, 두 번째 인자(<code class="highlighter-rouge">{'x': (-2, 6)}</code>)는 입력값 <script type="math/tex">x</script>의 탐색 대상 구간 <script type="math/tex">(a, b)</script>를 dictionary 형태로 받습니다. 입력값 <script type="math/tex">x</script>의 탐색 대상 구간은, 미지의 목적 함수에 대하여 현재까지 여러분들이 인지하고 있는 사전 지식에 기반하여 적절히 설정하되, 추후 필요한 경우 범위를 좁히거나 넓혀 재설정할 수도 있습니다.</p>

<p>한편 <code class="highlighter-rouge">random_state</code> 인자는 Bayesian Optimization 상의 랜덤성이 존재하는 부분(e.g. 다음 입력값 후보 추출 등)을 통제할 수 있도록 random seed를 입력해 주기 위한 목적으로 입력됩니다. 랜덤성을  통제할 필요가 없을 경우 입력하지 않아도 무방합니다.</p>

<h4 id="3-bayesian-optimization-실행">3. Bayesian Optimization 실행</h4>

<p>마지막으로, 생성한 <code class="highlighter-rouge">BayesianOptimization</code> 객체의 <code class="highlighter-rouge">maximize</code> 함수를 호출하여, 내부 멤버 변수 <code class="highlighter-rouge">_gp</code>를 반복적으로 업데이트하면서 실제 Bayesian Optimization 과정을 수행합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bayes_optimizer</span><span class="p">.</span><span class="n">maximize</span><span class="p">(</span><span class="n">init_points</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">acq</span><span class="o">=</span><span class="s">'ei'</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">maximize</code> 함수의 <code class="highlighter-rouge">init_points</code> 인자는, 맨 처음에 일부 Random Search 방법으로 조사할 입력값-함숫값 점들의 갯수(<script type="math/tex">n</script>)을 나타냅니다. 앞서 설정한 구간 내에서 해당 <code class="highlighter-rouge">init_points</code> 개의 입력값들을 랜덤하게 샘플링한 뒤, 앞서 정의한 <code class="highlighter-rouge">target</code> 함수에 대입하여 이들에 대한 함숫값들을 각각 계산하여 저장합니다. 이 때, <code class="highlighter-rouge">init_points</code>를 큰 값으로 설정할수록, 전체 구간에 대한 충분한 사전 탐색을 수행해 놓을 수 있으나, 그만큼 시간이 많이 소요됩니다.</p>

<p>다음으로 <code class="highlighter-rouge">n_iter</code> 인자는, 처음 <script type="math/tex">n</script>개의 입력값-함숫값 점들을 조사한 후, 조사된 입력값-함숫값 점들의 총 갯수가 <script type="math/tex">N</script>개에 도달할 때까지, Bayesian Optimization 방법을 통해 추가로 조사할 입력값-함숫값 점들의 총 갯수(<script type="math/tex">N-n</script>)를 나타냅니다. 예를 들어, 총 16개의 입력값-함숫값 점들을 확보하여 최적값을 탐색하고자 할 시, <code class="highlighter-rouge">init_points=2</code>로 설정하여 처음 2개의 입력값-함숫값 점들을 조사하고자 한다면, <code class="highlighter-rouge">n_iter=14</code>로 설정하면 나머지 14개의 입력값-함숫값 점들은 Bayesian Optimization 방법으로 조사하게 됩니다.</p>

<p>이어지는 <code class="highlighter-rouge">acq</code> 인자는, 현재 <em>bayesian-optimization</em> 라이브러리에서 제공하는 Acquisition Function들 중 어느 것을 사용할지를 명시하는 부분입니다. 만일 Expected Improvement(EI)를 Acquisition Function으로 사용하고자 한다면, <code class="highlighter-rouge">acq='ei'</code>로 설정하면 됩니다. 이에 결부되어 나오는 마지막 인자가 <code class="highlighter-rouge">xi</code>인데, 이는 지난 글에서 수식으로 보여드린 바 있는, exploration-explotation 간의 상대적 강도를 조절해 주는 파라미터 <script type="math/tex">\xi</script>입니다. <em>bayesian-optimization</em>에서 <code class="highlighter-rouge">xi</code>의 기본값은 0.0이나, 그보다는 exploration의 강도를 좀 더 높이기 위해 0.01 정도로 설정해 주는 것이 무난합니다.</p>

<h4 id="4-bayesian-optimization-최종-결과">4. Bayesian Optimization 최종 결과</h4>

<p>처음 랜덤한 방식의 2회, 이후 14회의 반복 회차에 걸쳐 Bayesian Optimization을 수행한 결과는 아래와 같이 출력됩니다.</p>

<!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ -->
<p><a href="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/bayesian-optimization-result-example.png" target="_blank">
  <img class="full-image" src="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/bayesian-optimization-result-example.png" alt="" />
</a>
<span class="caption"></span></p>

<p>‘iter’는 반복 회차, ‘target’은 목적 함수의 값, ‘x’는 입력값을 나타냅니다. 현재 회차 이전까지 조사된 함숫값들과 비교하여, 현재 회차에 최댓값이 얻어진 경우, <em>bayesian-optimization</em> 라이브러리는 이를 자동으로 다른 색 글자로 표시하는 것을 확인할 수 있습니다. 최종적으로 업데이트가 완료된 <code class="highlighter-rouge">BayesianOptimization</code> 객체 내부의 멤버 변수 <code class="highlighter-rouge">_gp</code>를 실제 목적 함수와 함께 도시하면 아래와 같습니다.</p>

<!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ -->
<p><a href="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/estimated-function-plot.png" target="_blank">
  <img class="full-image" src="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/estimated-function-plot.png" alt="예시 목적 함수에 대한 추정의 플롯팅 결과&lt;br&gt;&lt;small&gt;(파란색 실선: 목적 함수(Objective Function), 검은색 점선: GP를 통해 추정한 평균 함수(Prediction),&lt;br&gt;붉은색 점: 조사된 입력값-함숫값 점(Observations), 하늘색 영역: GP를 통해 추정한 표준편차(불확실성; Uncertain area))&lt;/small&gt;" />
</a>
<span class="caption">예시 목적 함수에 대한 추정의 플롯팅 결과<br /><small>(파란색 실선: 목적 함수(Objective Function), 검은색 점선: GP를 통해 추정한 평균 함수(Prediction),<br />붉은색 점: 조사된 입력값-함숫값 점(Observations), 하늘색 영역: GP를 통해 추정한 표준편차(불확실성; Uncertain area))</small></span></p>

<p>총 16회의 입력값-함숫값 조사를 거쳐, GP를 통해 추정한 평균 함수 <script type="math/tex">\mu(x)</script>의 결과가 실제 목적 함수 <script type="math/tex">f(x)</script>와 거의 일치하는 것을 확인할 수 있습니다. 추정된 평균 함수에 대하여 최적화 알고리즘을 적용해 보면, <script type="math/tex">x=0.629</script> 부근에서 최댓값 <script type="math/tex">\mu(0.629)=1.707</script>이 발생합니다. 앞서 계산했던 실제 최적 입력값인 <script type="math/tex">x=0.631</script>과 약 <script type="math/tex">0.002</script> 정도의 차이만 존재하는 것을 확인할 수 있습니다.</p>

<h2 id="bayesian-optimization을-사용한-딥러닝-모델의-주요-hyperparameter-탐색">Bayesian Optimization을 사용한 딥러닝 모델의 주요 hyperparameter 탐색</h2>

<p><em>bayesian-optimization</em> 라이브러리를 사용하면, 위와 같이 불과 몇 줄의 코드만으로도 목적 함수의 최적해에 대한 탐색이 가능합니다. 위에서 소개한 과정을 그대로 따라하되, ‘<strong>특정 hyperparameter를 입력값으로 받아, 이를 딥러닝 모델 학습에 적용하였을 시의 검증 데이터셋에 대한 성능 결과 수치를 출력값으로 제시하는 함수</strong>‘를 목적 함수로 설정하면, <em>bayesian-optimization</em> 라이브러리를 그대로 딥러닝 모델의 Hyperparameter Optimization에 적용할 수 있습니다.</p>

<h3 id="개vs고양이-분류-문제-해결을-위한-alexnet-구현체">‘개vs고양이 분류’ 문제 해결을 위한 AlexNet 구현체</h3>

<p>실제 딥러닝 모델 학습 예시로, 지난 <a href="http://research.sualab.com/machine-learning/computer-vision/2018/01/17/image-classification-deep-learning.html" target="_blank">&lt;이미지 Classification 문제와 딥러닝: AlexNet으로 개vs고양이 분류하기&gt;</a> 글에서 사용했던 AlexNet 구현체를 그대로 사용, Asirra Dogs vs. Cats 데이터셋으로 해당 모델을 학습하고 이를 ‘개vs고양이 분류’ 문제에 적용하는 과정을 다시 한 번 채택하기로 하였습니다. AlexNet으로 개vs고양이를 분류하는 과정에 대한 자세한 설명은, 해당 글을 다시 참조해 주시길 바랍니다.</p>

<p>지난 글에서 개vs고양이 분류를 위해 AlexNet을 학습할 당시에는, 사전에 지정한 하나의 hyperparameter들의 조합만을 사용하여 모델을 1회만 학습한 바 있습니다. 당시 설정했던 hyperparameter들 중 모델의 학습 성패 및 일반화 성능에 지대한 영향을 미칠 만한 것은 (decay 적용 전의) <strong>초기 학습률(initial learning rate)</strong>과 <strong>L2 정규화 계수(L2 weight decay)</strong>라고 할 수 있습니다.</p>

<ul>
  <li>초기 학습률(initial learning rate): 0.01</li>
  <li>L2 정규화 계수(L2 weight decay): 0.0005</li>
</ul>

<p>지난 글에서는 초기 학습률 및 L2 정규화 계수를 위와 같이 설정한 바 있습니다. 과연 위의 설정값들이 AlexNet의 일반화 성능을 극대화시키기 위한 ‘최적’의 hyperparameter 값들에 해당하는지 확인해 보기 위해, Bayesian Optimization을 통해 초기 학습률 및 L2 정규화 계수에 대한 탐색을 수행해 보도록 하겠습니다.</p>

<h3 id="train-with-bopy-스크립트">train-with-bo.py 스크립트</h3>

<p>기존 AlexNet 구현체에서 실제 학습을 수행하는 과정을 담은 것이 <code class="highlighter-rouge">train.py</code> 스크립트였는데, 여기에 <em>bayesian-optimization</em> 라이브러리를 적용한 Bayesian Optimization 과정을 추가하여 새로운 <code class="highlighter-rouge">train-with-bo.py</code> 스크립트를 구현하였습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" 1. 원본 데이터셋을 메모리에 로드하고 분리함 """</span>
<span class="n">root_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">'/'</span><span class="p">,</span> <span class="s">'mnt'</span><span class="p">,</span> <span class="s">'sdb2'</span><span class="p">,</span> <span class="s">'Datasets'</span><span class="p">,</span> <span class="s">'asirra'</span><span class="p">)</span>    <span class="c1"># FIXME
</span><span class="n">trainval_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_dir</span><span class="p">,</span> <span class="s">'train'</span><span class="p">)</span>

<span class="c1"># 원본 학습+검증 데이터셋을 로드하고, 이를 학습 데이터셋과 검증 데이터셋으로 나눔
</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">read_asirra_subset</span><span class="p">(</span><span class="n">trainval_dir</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">trainval_size</span> <span class="o">=</span> <span class="n">X_trainval</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">val_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">trainval_size</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>    <span class="c1"># FIXME
</span><span class="n">val_set</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">DataSet</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">[:</span><span class="n">val_size</span><span class="p">],</span> <span class="n">y_trainval</span><span class="p">[:</span><span class="n">val_size</span><span class="p">])</span>
<span class="n">train_set</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">DataSet</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">[</span><span class="n">val_size</span><span class="p">:],</span> <span class="n">y_trainval</span><span class="p">[</span><span class="n">val_size</span><span class="p">:])</span>

<span class="c1"># 중간 점검
</span><span class="k">print</span><span class="p">(</span><span class="s">'Training set stats:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_set</span><span class="p">.</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_set</span><span class="p">.</span><span class="n">images</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">train_set</span><span class="p">.</span><span class="n">images</span><span class="p">.</span><span class="nb">max</span><span class="p">())</span>
<span class="k">print</span><span class="p">((</span><span class="n">train_set</span><span class="p">.</span><span class="n">labels</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">).</span><span class="nb">sum</span><span class="p">(),</span> <span class="p">(</span><span class="n">train_set</span><span class="p">.</span><span class="n">labels</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Validation set stats:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">val_set</span><span class="p">.</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">val_set</span><span class="p">.</span><span class="n">images</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">val_set</span><span class="p">.</span><span class="n">images</span><span class="p">.</span><span class="nb">max</span><span class="p">())</span>
<span class="k">print</span><span class="p">((</span><span class="n">val_set</span><span class="p">.</span><span class="n">labels</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">).</span><span class="nb">sum</span><span class="p">(),</span> <span class="p">(</span><span class="n">val_set</span><span class="p">.</span><span class="n">labels</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">())</span>


<span class="s">""" 2. 학습 수행 및 성능 평가를 위한 기본 하이퍼파라미터 설정 """</span>
<span class="n">hp_d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">image_mean</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">.</span><span class="n">images</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>    <span class="c1"># 평균 이미지
</span><span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'/tmp/asirra_mean.npy'</span><span class="p">,</span> <span class="n">image_mean</span><span class="p">)</span>    <span class="c1"># 평균 이미지를 저장
</span><span class="n">hp_d</span><span class="p">[</span><span class="s">'image_mean'</span><span class="p">]</span> <span class="o">=</span> <span class="n">image_mean</span>

<span class="c1"># FIXME: 학습 관련 하이퍼파라미터
</span><span class="n">hp_d</span><span class="p">[</span><span class="s">'batch_size'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">hp_d</span><span class="p">[</span><span class="s">'num_epochs'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">hp_d</span><span class="p">[</span><span class="s">'augment_train'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">hp_d</span><span class="p">[</span><span class="s">'augment_pred'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">hp_d</span><span class="p">[</span><span class="s">'init_learning_rate'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">hp_d</span><span class="p">[</span><span class="s">'momentum'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">hp_d</span><span class="p">[</span><span class="s">'learning_rate_patience'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">hp_d</span><span class="p">[</span><span class="s">'learning_rate_decay'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">hp_d</span><span class="p">[</span><span class="s">'eps'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-8</span>

<span class="c1"># FIXME: 정규화 관련 하이퍼파라미터
</span><span class="n">hp_d</span><span class="p">[</span><span class="s">'weight_decay'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0005</span>
<span class="n">hp_d</span><span class="p">[</span><span class="s">'dropout_prob'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># FIXME: 성능 평가 관련 하이퍼파라미터
</span><span class="n">hp_d</span><span class="p">[</span><span class="s">'score_threshold'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-4</span>


<span class="s">""" 3. 특정한 초기 학습률 및 L2 정규화 계수 하에서 학습을 수행한 후, 검증 성능을 출력하는 목적 함수 정의 """</span>
<span class="k">def</span> <span class="nf">train_and_validate</span><span class="p">(</span><span class="n">init_learning_rate_log</span><span class="p">,</span> <span class="n">weight_decay_log</span><span class="p">):</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ConfigProto</span><span class="p">()</span>
    <span class="n">config</span><span class="p">.</span><span class="n">gpu_options</span><span class="p">.</span><span class="n">allow_growth</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="n">hp_d</span><span class="p">[</span><span class="s">'init_learning_rate'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">init_learning_rate_log</span>
    <span class="n">hp_d</span><span class="p">[</span><span class="s">'weight_decay'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">weight_decay_log</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">ConvNet</span><span class="p">([</span><span class="mi">227</span><span class="p">,</span> <span class="mi">227</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="o">**</span><span class="n">hp_d</span><span class="p">)</span>
    <span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">evaluator</span><span class="p">,</span> <span class="n">val_set</span><span class="o">=</span><span class="n">val_set</span><span class="p">,</span> <span class="o">**</span><span class="n">hp_d</span><span class="p">)</span>

    <span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
    <span class="n">train_results</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">hp_d</span><span class="p">)</span>

    <span class="c1"># 검증 정확도의 최댓값을 목적 함수의 출력값으로 반환
</span>    <span class="n">best_val_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">train_results</span><span class="p">[</span><span class="s">'eval_scores'</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">best_val_score</span>


<span class="s">""" 4. BayesianOptimization 객체 생성, 실행 및 최종 결과 출력 """</span>
<span class="n">bayes_optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span>
    <span class="n">f</span><span class="o">=</span><span class="n">train_and_validate</span><span class="p">,</span>
    <span class="n">pbounds</span><span class="o">=</span><span class="p">{</span>
        <span class="s">'init_learning_rate_log'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>    <span class="c1"># FIXME
</span>        <span class="s">'weight_decay_log'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>            <span class="c1"># FIXME
</span>    <span class="p">},</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="n">bayes_optimizer</span><span class="p">.</span><span class="n">maximize</span><span class="p">(</span><span class="n">init_points</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">27</span><span class="p">,</span> <span class="n">acq</span><span class="o">=</span><span class="s">'ei'</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>    <span class="c1"># FIXME
</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bayes_optimizer</span><span class="p">.</span><span class="n">res</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Iteration {}: </span><span class="se">\n\t</span><span class="s">{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">res</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Final result: '</span><span class="p">,</span> <span class="n">bayes_optimizer</span><span class="p">.</span><span class="nb">max</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">train-with-bo.py</code> 스크립트에서는 다음의 4단계 과정을 거칩니다.</p>

<ol>
  <li>원본 학습 데이터셋을 메모리에 로드하고, 이를 학습 데이터셋(80%)과 검증 데이터셋(20%)으로 나눈 뒤 각각을 사용하여 <code class="highlighter-rouge">DataSet</code> 객체를 생성함</li>
  <li>학습 수행 및 성능 평가를 위한 (조사 대상 외의) 기본 hyperparameter들을 설정함</li>
  <li>특정한 초기 학습률 및 L2 정규화 계수 하에서 학습을 수행한 후, 검증 성능을 출력하는 목적 함수 <code class="highlighter-rouge">train_and_validate</code>을 정의함
    <ul>
      <li>함수 내부에서 <code class="highlighter-rouge">ConvNet</code> 객체, <code class="highlighter-rouge">Evaluator</code> 객체 및 <code class="highlighter-rouge">Optimizer</code> 객체를 생성하고, TensorFlow Graph와 Session을 초기화한 뒤, <code class="highlighter-rouge">Optimizer.train</code> 함수를 호출하여 모델 학습을 수행함</li>
      <li>학습 진행 과정에서, 검증 데이터셋에 대하여 매 epoch마다 얻어진 모델의 예측 정확도(검증 정확도) 중 최댓값을 <code class="highlighter-rouge">train_and_validate</code> 함수가 반환하도록 함</li>
    </ul>
  </li>
  <li>앞서 정의한 목적 함수 및 입력값들의 탐색 대상 구간을 인자로 입력하여 <code class="highlighter-rouge">BayesianOptimization</code> 객체를 생성한 후, 초기 Random Search 및 이후 Bayesian Optimization을 통해 조사할 입력값-함숫값 점들의 갯수들을 각각 인자로 입력하여 <code class="highlighter-rouge">maximize</code> 함수를 호출하고, 모든 과정이 끝나면 최종 결과를 출력함</li>
</ol>

<p>이 때, <code class="highlighter-rouge">train_and_evaluate</code> 함수의 인자로 받는 <code class="highlighter-rouge">init_learning_rate_log</code>와 <code class="highlighter-rouge">weight_decay_log</code>는, 각각 <strong>초기 학습률과 L2 정규화 계수를 <a href="https://en.wikipedia.org/wiki/Logarithmic_scale" target="_blank">base-10 log scale</a>로 표현한 것</strong>임을 유의하셔야 합니다. 예를 들어 <code class="highlighter-rouge">init_learning_rate_log</code>의 값이 <script type="math/tex">\alpha</script>인 경우 실제 학습 시의 초기 학습률은 <script type="math/tex">10^{\alpha}</script>로 설정되며, <code class="highlighter-rouge">weight_decay_log</code>의 값이 <script type="math/tex">\lambda</script>인 경우 실제 학습 시의 L2 정규화 계수는 <script type="math/tex">10^{\lambda}</script>로 설정됩니다.</p>

<p>그리고, <code class="highlighter-rouge">BayesianOptimization</code> 객체 생성 및 <code class="highlighter-rouge">maximize</code> 함수 호출 시 인자들과 관련하여, <code class="highlighter-rouge">FIXME</code>로 표시된 부분은 여러분의 상황과 기호에 맞춰 수정하실 수 있습니다. 본 글에서 Bayesian Optimization을 수행할 당시의 해당 인자 값들을 아래와 같이 설정하였습니다.</p>

<ul>
  <li>입력값들의 탐색 대상 구간 <script type="math/tex">(a, b)</script> (<code class="highlighter-rouge">pbounds</code>)
    <ul>
      <li>초기 학습률(initial learning rate; base-10 log scale): (-5, -1)</li>
      <li>L2 정규화 계수(L2 weight decay; base-10 log scale): (-5, -1)</li>
    </ul>
  </li>
  <li>처음 랜덤하게 조사할 입력값-함숫값 점들의 갯수 <script type="math/tex">n</script> (<code class="highlighter-rouge">init_points</code>): 3</li>
  <li>Bayesian Optimization 방법을 통해 추가로 조사할 입력값-함숫값 점들의 갯수 <script type="math/tex">N-n</script> (<code class="highlighter-rouge">n_iter</code>): 27</li>
</ul>

<h3 id="bayesian-optimization을-통한-hyperparameter-탐색-결과">Bayesian Optimization을 통한 hyperparameter 탐색 결과</h3>

<h4 id="초기-학습률-및-l2-정규화-계수에-대한-bayesian-optimization-수행-과정">초기 학습률 및 L2 정규화 계수에 대한 Bayesian Optimization 수행 과정</h4>

<p><code class="highlighter-rouge">train-with-bo.py</code> 스크립트를 실행하여, 최적의 초기 학습률 및 L2 정규화 계수를 탐색하는 과정을 아래 그림과 같이 시각화하였습니다. 입력값이 두 가지이므로 이를 2차원 평면 상의 히트맵(heatmap) 형태로 나타냈으며, 빨간색에 가까울수록 그 값이 크고, 파란색에 가까울수록 그 값이 작음을 의미합니다. 실제 조사된 입력값-함숫값 점의 경우, 검은색 점으로 표시했습니다.</p>

<!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ -->
<p><a href="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/2d-bayesian-optimization-process.gif" target="_blank">
  <img class="full-image" src="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/2d-bayesian-optimization-process.gif" alt="(초기 학습률, L2 정규화 계수)에 대한 2차원 Bayesian Optimization(GP, EI) 수행 과정&lt;br&gt;&lt;small&gt;(각각 구간 $$(-5, -1)$$, $$(-5, -1)$$에서 최초 3개($$n=3$$), 총 30개($$N=30$$)의 점에 대한 반복 조사 결과&lt;br&gt;좌측: GP를 통해 추정한 평균 함수 $$\mu(x,y)$$, 중앙: GP를 통해 추정한 표준편차 $$\sigma(x,y)$$,&lt;br&gt;우측: GP의 확률적 추정 결과에 대한 EI 함수 계산 결과(십자 표시: 다음 조사 대상 점의 위치); random_seed=0)&lt;/small&gt;" />
</a>
<span class="caption">(초기 학습률, L2 정규화 계수)에 대한 2차원 Bayesian Optimization(GP, EI) 수행 과정<br /><small>(각각 구간 <script type="math/tex">(-5, -1)</script>, <script type="math/tex">(-5, -1)</script>에서 최초 3개(<script type="math/tex">n=3</script>), 총 30개(<script type="math/tex">N=30</script>)의 점에 대한 반복 조사 결과<br />좌측: GP를 통해 추정한 평균 함수 <script type="math/tex">\mu(x,y)</script>, 중앙: GP를 통해 추정한 표준편차 <script type="math/tex">\sigma(x,y)</script>,<br />우측: GP의 확률적 추정 결과에 대한 EI 함수 계산 결과(십자 표시: 다음 조사 대상 점의 위치); random_seed=0)</small></span></p>

<p>위 과정을 거쳐 초기 학습률과 L2 정규화 계수에 대한 탐색을 수행한 결과, 최적의 값들이 아래와 같이 확인되었습니다.</p>

<ul>
  <li>최적 초기 학습률(initial learning rate): <script type="math/tex">10^{-1.7927} \approx 0.016118</script></li>
  <li>최적 L2 정규화 계수(L2 weight decay): <script type="math/tex">10^{-4.5997} \approx 0.000025</script></li>
</ul>

<h4 id="최적의-hyperparameter-값들을-채택한-학습-곡선">최적의 hyperparameter 값들을 채택한 학습 곡선</h4>

<p>위와 같이 찾은 최적 hyperparameter 값들을 그대로 채택, 기존 <code class="highlighter-rouge">train.py</code> 스크립트를 실행하여 실제 학습을 수행한 결과, 아래의 정보들을 담은 학습 곡선(learning curve)을 얻을 수 있었습니다.</p>

<ul>
  <li>매 반복 회차에서의 손실 함수의 값</li>
  <li>매 epoch에 대하여 (1) 학습 데이터셋으로부터 추출한 미니배치(minibatch)에 대한 모델의 예측 정확도(학습 정확도)와 (2) 검증 데이터셋에 대한 모델의 예측 정확도(검증 정확도)</li>
</ul>

<!-- @reference: https://www.kevinmcgillivray.net/captions-for-images-with-jekyll/ -->
<p><a href="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/optimal-learning-curve-result.svg" target="_blank">
  <img class="large-image" src="http://research.sualab.com/assets/images/bayesian-optimization-overview-2/optimal-learning-curve-result.svg" alt="학습 곡선 플롯팅 결과&lt;br&gt;&lt;small&gt;(파란색: 학습 데이터셋 정확도, 빨간색: 검증 데이터셋 정확도)&lt;/small&gt;" />
</a>
<span class="caption">학습 곡선 플롯팅 결과<br /><small>(파란색: 학습 데이터셋 정확도, 빨간색: 검증 데이터셋 정확도)</small></span></p>

<p>학습 과정 말미에서, 검증 정확도가 0.9384일 때의 모델 파라미터들을 최종적으로 채택하여, 테스트를 위해 저장하였습니다.</p>

<h4 id="테스트-결과">테스트 결과</h4>

<p>테스트 결과 측정된 정확도는 <strong>0.93688</strong>로 확인되었습니다. 이전 글에서 측정된 정확도가 <strong>0.92768</strong>이었는데, 이를 약 1% 가량 상회하는 결과가 얻어졌습니다. 지금까지의 과정을 다시금 곱씹어 보면, 제한된 양의 학습 데이터셋만을 활용해서, 두 가지 hyperparameter에 대하여 단 30회의 자동화된 반복적 조사를 통해 최적의 hyperparameter 값을 찾아낸 결과라고 할 수 있습니다.</p>

<p>매 회 조사 결과를 사람이 직접 관찰한 뒤 그 다음 번 조사 대상을 설정하여 진행하는 Manual Search나, 매 회 새로운 조사 수행 시 ‘사전 지식’이 반영되지 않아 다소 불필요한 조사를 반복하게 되는 Grid Search 및 Random Search에 비해, 매 회 조사 대상 선정을 자동화하였으면서 동시에 확률적 추정을 통해 ‘사전 지식’을 충분히 반영하였다는 측면에서 이들보다 효율적으로 얻어진 결과라고 생각됩니다.</p>

<h2 id="결론">결론</h2>

<p>지난 글에서 딥러닝 모델의 Hyperparamter Optimization을 위한 Bayesian Optimization 방법론의 대략적인 원리 및 행동 방식에 대하여 알아보았고, 본 글에서는 이를 구현하기 위한 Python 라이브러리인 <em>bayesian-optimization</em>에 대한 소개 및 기본적인 사용법을 안내해 드렸습니다. Surrogate Model로 GP, Acquision Function으로 EI를 채택하여, 간단한 예시 목적 함수의 최적해를 탐색하는 과정을 진행하였으며, 그 다음 실제 AlexNet 모델의 초기 학습률 및 L2 정규화 계수의 최적값을 탐색하는 과정을 진행하였습니다.</p>

<p>더욱 직접적인 비교를 위해, 여러분께서 초기 학습률 및 L2 정규화 계수에 대한 Grid Search와 Random Search 또한 직접 구현하고 실행하시어, 이들을 통해 최적의 hyperparameter를 탐색하는 과정을 Bayesian Optimization 과정과 직접 비교해 보시길 권장드립니다. 보다 드라마틱한 차이를 확인하실 수 있을 것이라고 생각합니다.</p>

<h2 id="references">References</h2>

<ul>
  <li>Fernando Nogueira, bayesian-optimization: A Python implementation of global optimization with gaussian processes.
    <ul>
      <li><a href="https://github.com/fmfn/BayesianOptimization" target="_blank">https://github.com/fmfn/BayesianOptimization</a></li>
    </ul>
  </li>
  <li>Logarithmic scale
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Logarithmic_scale" target="_blank">Wikipedia contributors. “Logarithmic scale.” Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 13 Mar. 2019. Web. 1 Apr. 2019.</a></li>
    </ul>
  </li>
</ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Cognex Deep Learning Lab-KOR Research Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Cognex Deep Learning Lab-KOR Research Blog
            
            </li>
            
            <li><a href="https://www.cognex.co.kr/" target="_blank">https://www.cognex.co.kr/</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
          
          <li>
            <a href="https://facebook.com/cognexcorp" target="_blank"><i class="fa fa-facebook"></i> <span class="username">cognexcorp</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Cognex Deep Learning Lab-KOR research blog: covers subjects regarding machine learning, computer vision, high-performance computing, and so on.
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
